{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gina-Kuo/NTHU_2025_DLIA_HW/blob/main/lab2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c",
      "metadata": {
        "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "###### Lab 2\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2025\n",
        "\n",
        "#### 11320IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Lab 2: Predicting Heart Disease with Deep Learning\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
      "metadata": {
        "tags": [],
        "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
        "\n",
        "Throughout this lab, you'll engage with the following key activities:\n",
        "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
        "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
        "- Evaluate the performance of the trained model to understand its accuracy.\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "1. age: Age of the patient in years\n",
        "2. sex: (Male/Female)\n",
        "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
        "4. trestbps: Resting blood pressure\n",
        "5. chol: Serum cholesterol in mg/dl\n",
        "6. fbs: Fasting blood sugar > 120 mg/dl\n",
        "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
        "8. thalach: Maximum heart rate achieved\n",
        "9. exang: Exercise induced angina\n",
        "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
        "11. slope: The slope of the peak exercise ST segment\n",
        "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
        "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
        "14. target: target have disease or not (1=yes, 0=no)\n",
        "\n",
        "### References\n",
        "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad594fc8-4989-40f3-b124-4550fe7df386",
      "metadata": {
        "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
      },
      "source": [
        "## A. Checking and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfITSFq7skol",
        "outputId": "058ad356-44df-4e4c-e87b-a703a6769271"
      },
      "id": "pfITSFq7skol",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
        "outputId": "2caf6323-daf7-483f-fdd0-8cdc366c395c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age     sex      cp  trestbps   chol  fbs  restecg  thalach  exang  \\\n",
              "0     41    Male  medium     105.0  198.0    0      1.0    168.0      0   \n",
              "1     65  Female     low     120.0  177.0    0      1.0    140.0      0   \n",
              "2     44  Female  medium     130.0  219.0    0      0.0    188.0      0   \n",
              "3     54  Female    high     125.0  273.0    0      0.0    152.0      0   \n",
              "4     51  Female  severe     125.0  213.0    0      0.0    125.0      1   \n",
              "..   ...     ...     ...       ...    ...  ...      ...      ...    ...   \n",
              "268   40  Female     low     110.0  167.0    0      0.0    114.0      1   \n",
              "269   60  Female     low     117.0  230.0    1      1.0    160.0      1   \n",
              "270   64  Female    high     140.0  335.0    0      1.0    158.0      0   \n",
              "271   43  Female     low     120.0  177.0    0      0.0    120.0      1   \n",
              "272   57  Female     low     150.0  276.0    0      0.0    112.0      1   \n",
              "\n",
              "     oldpeak  slope  ca  thal  target  \n",
              "0        0.0    2.0   1   2.0     1.0  \n",
              "1        0.4    2.0   0   3.0     1.0  \n",
              "2        0.0    2.0   0   2.0     1.0  \n",
              "3        0.5    0.0   1   2.0     1.0  \n",
              "4        1.4    2.0   1   2.0     1.0  \n",
              "..       ...    ...  ..   ...     ...  \n",
              "268      2.0    1.0   0   3.0     0.0  \n",
              "269      1.4    2.0   2   3.0     0.0  \n",
              "270      0.0    2.0   0   2.0     0.0  \n",
              "271      2.5    1.0   0   3.0     0.0  \n",
              "272      0.6    1.0   1   1.0     0.0  \n",
              "\n",
              "[273 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1db4238-8c0d-41be-b3fa-a6f1269f04bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>Male</td>\n",
              "      <td>medium</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>Female</td>\n",
              "      <td>medium</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>Female</td>\n",
              "      <td>severe</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>273 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1db4238-8c0d-41be-b3fa-a6f1269f04bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1db4238-8c0d-41be-b3fa-a6f1269f04bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1db4238-8c0d-41be-b3fa-a6f1269f04bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82124f8b-a8b4-4bda-91ad-78dc940c9b4f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82124f8b-a8b4-4bda-91ad-78dc940c9b4f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82124f8b-a8b4-4bda-91ad-78dc940c9b4f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d96b627f-3c9e-45b1-a728-9df9e01ed951\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d96b627f-3c9e-45b1-a728-9df9e01ed951 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 273,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"low\",\n          \"severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.852394760861472,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.44738664891147,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5291071887540655,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.20476792843544,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.184296490376038,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6175201588975051,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6220982126950155,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4986279198706136,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning for Industrial Application/HW2/heart_dataset_train_all.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "34241797-60f0-4818-a44b-f5379948d621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34241797-60f0-4818-a44b-f5379948d621",
        "outputId": "d1775c9b-140e-40c5-b3af-211f73a90c9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
              "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
        "outputId": "5ef5b4e7-b576-47be-c0c8-34ac257d0b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 273 entries, 0 to 272\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       273 non-null    int64  \n",
            " 1   sex       272 non-null    object \n",
            " 2   cp        272 non-null    object \n",
            " 3   trestbps  272 non-null    float64\n",
            " 4   chol      271 non-null    float64\n",
            " 5   fbs       273 non-null    int64  \n",
            " 6   restecg   272 non-null    float64\n",
            " 7   thalach   272 non-null    float64\n",
            " 8   exang     273 non-null    int64  \n",
            " 9   oldpeak   273 non-null    float64\n",
            " 10  slope     271 non-null    float64\n",
            " 11  ca        273 non-null    int64  \n",
            " 12  thal      272 non-null    float64\n",
            " 13  target    272 non-null    float64\n",
            "dtypes: float64(8), int64(4), object(2)\n",
            "memory usage: 30.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
        "outputId": "992c3a5d-249d-4d3e-d2f8-eb664631efac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         1\n",
              "cp          1\n",
              "trestbps    1\n",
              "chol        2\n",
              "fbs         0\n",
              "restecg     1\n",
              "thalach     1\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       2\n",
              "ca          0\n",
              "thal        1\n",
              "target      1\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# checking for null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932",
      "metadata": {
        "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
        "outputId": "3bdf129d-8f60-4b15-d389-80370b279149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
      "metadata": {
        "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "3745d161-0854-41fa-c326-cc7df36ad1c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age sex cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     41   0  1     105.0  198.0    0      1.0    168.0      0      0.0   \n",
              "1     65   1  0     120.0  177.0    0      1.0    140.0      0      0.4   \n",
              "2     44   1  1     130.0  219.0    0      0.0    188.0      0      0.0   \n",
              "3     54   1  2     125.0  273.0    0      0.0    152.0      0      0.5   \n",
              "4     51   1  3     125.0  213.0    0      0.0    125.0      1      1.4   \n",
              "..   ...  .. ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "268   40   1  0     110.0  167.0    0      0.0    114.0      1      2.0   \n",
              "269   60   1  0     117.0  230.0    1      1.0    160.0      1      1.4   \n",
              "270   64   1  2     140.0  335.0    0      1.0    158.0      0      0.0   \n",
              "271   43   1  0     120.0  177.0    0      0.0    120.0      1      2.5   \n",
              "272   57   1  0     150.0  276.0    0      0.0    112.0      1      0.6   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0      2.0   1   2.0     1.0  \n",
              "1      2.0   0   3.0     1.0  \n",
              "2      2.0   0   2.0     1.0  \n",
              "3      0.0   1   2.0     1.0  \n",
              "4      2.0   1   2.0     1.0  \n",
              "..     ...  ..   ...     ...  \n",
              "268    1.0   0   3.0     0.0  \n",
              "269    2.0   2   3.0     0.0  \n",
              "270    2.0   0   2.0     0.0  \n",
              "271    1.0   0   3.0     0.0  \n",
              "272    1.0   1   1.0     0.0  \n",
              "\n",
              "[270 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8f41d00-eb3b-47af-8c2b-3bf41f3ce09a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8f41d00-eb3b-47af-8c2b-3bf41f3ce09a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8f41d00-eb3b-47af-8c2b-3bf41f3ce09a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8f41d00-eb3b-47af-8c2b-3bf41f3ce09a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11f13cd4-e453-409c-b105-860bff0eaa88\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11f13cd4-e453-409c-b105-860bff0eaa88')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11f13cd4-e453-409c-b105-860bff0eaa88 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2ca31107-82a7-41d5-9cff-7f739e2a96b0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2ca31107-82a7-41d5-9cff-7f739e2a96b0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 270,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.90467515098084,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.5294114300184,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5293141619418645,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.21725300546228,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.188378543758624,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6181877820120636,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6238744511959269,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49894560448305486,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Mapping 'sex' descriptions to numbers\n",
        "sex_description = {\n",
        "    'Male': 0,\n",
        "    'Female': 1,\n",
        "}\n",
        "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
        "\n",
        "# Mapping 'cp' (chest pain) descriptions to numbers\n",
        "pain_description = {\n",
        "    'low': 0,\n",
        "    'medium': 1,\n",
        "    'high': 2,\n",
        "    'severe': 3\n",
        "}\n",
        "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "051108c6-7011-4187-9e36-bd2944a019ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "051108c6-7011-4187-9e36-bd2944a019ca",
        "outputId": "3ac3c912-f48d-481d-91ee-baaeac303b04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              age    trestbps        chol         fbs     restecg     thalach  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean    54.385185  131.525926  245.607407    0.151852    0.522222  149.807407   \n",
              "std      9.149713   17.904675   51.529411    0.359544    0.529314   23.217253   \n",
              "min     29.000000   94.000000  126.000000    0.000000    0.000000   71.000000   \n",
              "25%     47.250000  120.000000  210.250000    0.000000    0.000000  134.500000   \n",
              "50%     56.000000  130.000000  240.500000    0.000000    1.000000  152.500000   \n",
              "75%     61.000000  140.000000  274.000000    0.000000    1.000000  167.750000   \n",
              "max     77.000000  200.000000  564.000000    1.000000    2.000000  202.000000   \n",
              "\n",
              "            exang     oldpeak       slope          ca        thal      target  \n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000  \n",
              "mean     0.333333    1.024074    1.400000    0.744444    2.300000    0.544444  \n",
              "std      0.472280    1.188379    0.618188    1.037166    0.623874    0.498946  \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              "25%      0.000000    0.000000    1.000000    0.000000    2.000000    0.000000  \n",
              "50%      0.000000    0.600000    1.000000    0.000000    2.000000    1.000000  \n",
              "75%      1.000000    1.600000    2.000000    1.000000    3.000000    1.000000  \n",
              "max      1.000000    6.200000    2.000000    4.000000    3.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e71aa07d-e0ef-4db6-94cc-5974db9b1e4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.385185</td>\n",
              "      <td>131.525926</td>\n",
              "      <td>245.607407</td>\n",
              "      <td>0.151852</td>\n",
              "      <td>0.522222</td>\n",
              "      <td>149.807407</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.024074</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>0.544444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.149713</td>\n",
              "      <td>17.904675</td>\n",
              "      <td>51.529411</td>\n",
              "      <td>0.359544</td>\n",
              "      <td>0.529314</td>\n",
              "      <td>23.217253</td>\n",
              "      <td>0.472280</td>\n",
              "      <td>1.188379</td>\n",
              "      <td>0.618188</td>\n",
              "      <td>1.037166</td>\n",
              "      <td>0.623874</td>\n",
              "      <td>0.498946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.250000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>210.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>134.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>152.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>167.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e71aa07d-e0ef-4db6-94cc-5974db9b1e4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e71aa07d-e0ef-4db6-94cc-5974db9b1e4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e71aa07d-e0ef-4db6-94cc-5974db9b1e4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-91a105c4-6928-4af5-bd16-1bde0220585a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91a105c4-6928-4af5-bd16-1bde0220585a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-91a105c4-6928-4af5-bd16-1bde0220585a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.26807726653665,\n        \"min\": 9.149713209948986,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          54.385185185185186,\n          56.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.73772068120547,\n        \"min\": 17.90467515098084,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          131.52592592592592,\n          130.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 149.27786697513412,\n        \"min\": 51.5294114300184,\n        \"max\": 564.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          245.6074074074074,\n          240.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.38369701469601,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.15185185185185185,\n          1.0,\n          0.35954367027245654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.2064556868165,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.5222222222222223,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.47345746130112,\n        \"min\": 23.21725300546228,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          149.8074074074074,\n          152.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.31861707775643,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.47227992455486234\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.94427117355892,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          270.0,\n          1.0240740740740741,\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.05680862661214,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          1.4,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.1259484289674,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.7444444444444445,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.81255144269917,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          2.3,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.25610060161931,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5444444444444444,\n          1.0,\n          0.49894560448305486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
        "outputId": "f012f933-16ed-463a-cdc4-493f21887839"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age       sex        cp  trestbps      chol       fbs  \\\n",
              "age       1.000000 -0.062222 -0.103697  0.261782  0.210520  0.109847   \n",
              "sex      -0.062222  1.000000 -0.040197 -0.055463 -0.166885  0.042384   \n",
              "cp       -0.103697 -0.040197  1.000000  0.035563 -0.063592  0.065869   \n",
              "trestbps  0.261782 -0.055463  0.035563  1.000000  0.128444  0.170606   \n",
              "chol      0.210520 -0.166885 -0.063592  0.128444  1.000000  0.003430   \n",
              "fbs       0.109847  0.042384  0.065869  0.170606  0.003430  1.000000   \n",
              "restecg  -0.124588 -0.069599  0.008389 -0.145195 -0.162687 -0.086165   \n",
              "thalach  -0.412624 -0.058626  0.300307 -0.056631 -0.023753 -0.014297   \n",
              "exang     0.111263  0.124054 -0.428233  0.067116  0.063902  0.029190   \n",
              "oldpeak   0.200243  0.089726 -0.183616  0.184896  0.084355  0.007943   \n",
              "slope    -0.165360 -0.038771  0.135174 -0.126553 -0.031929 -0.056866   \n",
              "ca        0.254462  0.140795 -0.180598  0.093545  0.068647  0.164266   \n",
              "thal      0.077368  0.198493 -0.139765  0.068690  0.121280 -0.004972   \n",
              "target   -0.244798 -0.283776  0.425574 -0.173239 -0.096773 -0.068845   \n",
              "\n",
              "           restecg   thalach     exang   oldpeak     slope        ca  \\\n",
              "age      -0.124588 -0.412624  0.111263  0.200243 -0.165360  0.254462   \n",
              "sex      -0.069599 -0.058626  0.124054  0.089726 -0.038771  0.140795   \n",
              "cp        0.008389  0.300307 -0.428233 -0.183616  0.135174 -0.180598   \n",
              "trestbps -0.145195 -0.056631  0.067116  0.184896 -0.126553  0.093545   \n",
              "chol     -0.162687 -0.023753  0.063902  0.084355 -0.031929  0.068647   \n",
              "fbs      -0.086165 -0.014297  0.029190  0.007943 -0.056866  0.164266   \n",
              "restecg   1.000000  0.025457 -0.089225 -0.047837  0.074982 -0.053946   \n",
              "thalach   0.025457  1.000000 -0.404349 -0.340564  0.370073 -0.205060   \n",
              "exang    -0.089225 -0.404349  1.000000  0.294308 -0.280124  0.106250   \n",
              "oldpeak  -0.047837 -0.340564  0.294308  1.000000 -0.585472  0.223375   \n",
              "slope     0.074982  0.370073 -0.280124 -0.585472  1.000000 -0.083491   \n",
              "ca       -0.053946 -0.205060  0.106250  0.223375 -0.083491  1.000000   \n",
              "thal     -0.003377 -0.078637  0.189253  0.200315 -0.090606  0.136160   \n",
              "target    0.101817  0.432687 -0.457502 -0.443504  0.363983 -0.391031   \n",
              "\n",
              "              thal    target  \n",
              "age       0.077368 -0.244798  \n",
              "sex       0.198493 -0.283776  \n",
              "cp       -0.139765  0.425574  \n",
              "trestbps  0.068690 -0.173239  \n",
              "chol      0.121280 -0.096773  \n",
              "fbs      -0.004972 -0.068845  \n",
              "restecg  -0.003377  0.101817  \n",
              "thalach  -0.078637  0.432687  \n",
              "exang     0.189253 -0.457502  \n",
              "oldpeak   0.200315 -0.443504  \n",
              "slope    -0.090606  0.363983  \n",
              "ca        0.136160 -0.391031  \n",
              "thal      1.000000 -0.311701  \n",
              "target   -0.311701  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1798786e-3204-4c71-a9cf-956cb821c50c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062222</td>\n",
              "      <td>-0.103697</td>\n",
              "      <td>0.261782</td>\n",
              "      <td>0.210520</td>\n",
              "      <td>0.109847</td>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.412624</td>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.200243</td>\n",
              "      <td>-0.165360</td>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.077368</td>\n",
              "      <td>-0.244798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>-0.062222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.283776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>-0.103697</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.425574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0.261782</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>-0.173239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0.210520</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.096773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0.109847</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.068845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>0.101817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>-0.412624</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.432687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>-0.457502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0.200243</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.443504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>-0.165360</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.363983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>-0.391031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0.077368</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.311701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>-0.244798</td>\n",
              "      <td>-0.283776</td>\n",
              "      <td>0.425574</td>\n",
              "      <td>-0.173239</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.068845</td>\n",
              "      <td>0.101817</td>\n",
              "      <td>0.432687</td>\n",
              "      <td>-0.457502</td>\n",
              "      <td>-0.443504</td>\n",
              "      <td>0.363983</td>\n",
              "      <td>-0.391031</td>\n",
              "      <td>-0.311701</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1798786e-3204-4c71-a9cf-956cb821c50c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1798786e-3204-4c71-a9cf-956cb821c50c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1798786e-3204-4c71-a9cf-956cb821c50c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0fad504-4f4b-4451-a992-f2184561b341\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0fad504-4f4b-4451-a992-f2184561b341')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0fad504-4f4b-4451-a992-f2184561b341 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3336796953563416,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2002432632272073,\n          0.25446220532709146,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2991228194776611,\n        \"min\": -0.28377582305309207,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08972560084685732,\n          0.14079450235695648,\n          -0.062222038735579396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.343237644388663,\n        \"min\": -0.42823328385023884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.18361571896503148,\n          -0.18059763067401924,\n          -0.10369651400029238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28956654914616836,\n        \"min\": -0.1732391623681546,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.18489606509122675,\n          0.09354457752219755,\n          0.2617824169771793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2867254997156597,\n        \"min\": -0.16688487098702884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08435532313293413,\n          0.06864714277304984,\n          0.21052008975387562\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27150092525557695,\n        \"min\": -0.08616493327966852,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.00794318047234304,\n          0.16426559207767857,\n          0.10984693693665605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28984501434053883,\n        \"min\": -0.16268743250369436,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.04783727701622021,\n          -0.05394643396214631,\n          -0.12458764457926447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38230319141316005,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.34056397282727374,\n          -0.20506017090677786,\n          -0.4126237683266394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3727203587084935,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2943081762485106,\n          0.10624980942135133,\n          0.11126311741056943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38472554244127527,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          1.0,\n          0.22337523920060878,\n          0.2002432632272073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3681628540625692,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.5854716356239958,\n          -0.08349138857404798,\n          -0.16535999776800558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3194198076353616,\n        \"min\": -0.3910311347568131,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.22337523920060878,\n          1.0,\n          0.25446220532709146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29843375004744976,\n        \"min\": -0.3117007343731907,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2003145523377015,\n          0.13616037988626117,\n          0.07736766291885036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4251943991536586,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.4435044185350298,\n          -0.3910311347568131,\n          -0.2447981425022257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a",
      "metadata": {
        "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
      },
      "source": [
        "#### Converting the DataFrame to a NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
        "outputId": "6d8a875c-9b89-49ab-9b48-88ca3a16505d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np_data = df.values\n",
        "np_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "29b8e189-7f39-435a-8038-39098b147325",
      "metadata": {
        "id": "29b8e189-7f39-435a-8038-39098b147325"
      },
      "outputs": [],
      "source": [
        "split_point = int(np_data.shape[0]*0.7)\n",
        "\n",
        "np.random.shuffle(np_data)\n",
        "\n",
        "x_train = np_data[:split_point, :13]\n",
        "y_train = np_data[:split_point, 13]\n",
        "x_val = np_data[split_point:, :13]\n",
        "y_val = np_data[split_point:, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
        "outputId": "7b3ea37d-90ef-4e96-aef8-e17590a38912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in train and validation are 189 and 40.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to tensors\n",
        "x_train = torch.from_numpy(np.array(x_train, dtype=np.float32)).float()\n",
        "y_train = torch.from_numpy(np.array(y_train, dtype=np.int64)).long()\n",
        "x_val = torch.from_numpy(np.array(x_val, dtype=np.float32)).float()\n",
        "y_val = torch.from_numpy(np.array(y_val, dtype=np.int64)).long()\n",
        "\n",
        "# Split val into val/test\n",
        "val_split = int(x_val.shape[0] * 0.5)\n",
        "x_test = x_val[val_split:]\n",
        "y_test = y_val[val_split:]\n",
        "x_val = x_val[:val_split]\n",
        "y_val = y_val[:val_split]\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27",
      "metadata": {
        "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
      },
      "source": [
        "## B. Defining Neural Networks\n",
        "\n",
        "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
        "\n",
        "- Neural networks are defined by subclassing `nn.Module`.\n",
        "- The layers of the neural network are initialized in the `__init__` method.\n",
        "- The forward pass operations on input data are defined in the `forward` method.\n",
        "\n",
        "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "77975746-a7a7-4676-9527-57674cd98c0f",
      "metadata": {
        "id": "77975746-a7a7-4676-9527-57674cd98c0f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa",
      "metadata": {
        "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
      },
      "source": [
        "## C. Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
        "outputId": "0858c5a8-f49e-4748-8e33-c44648a4d4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 10:02:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0             28W /   70W |     160MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check your GPU status.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
        "outputId": "99dcbbbf-47a3-490a-88fa-eb9b00081572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with LR=0.01, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 3.5606, Train acc: 50.2646%, Val loss: 3.0178, Val acc: 37.5000%, Best Val loss: 3.0178 Best Val acc: 37.50%\n",
            "Epoch 2/100, Train loss: 1.5100, Train acc: 48.1481%, Val loss: 0.8139, Val acc: 62.5000%, Best Val loss: 0.8139 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 1.0510, Train acc: 52.3810%, Val loss: 1.0264, Val acc: 45.0000%, Best Val loss: 0.8139 Best Val acc: 62.50%\n",
            "Epoch 4/100, Train loss: 0.8527, Train acc: 58.7302%, Val loss: 0.4194, Val acc: 75.0000%, Best Val loss: 0.4194 Best Val acc: 75.00%\n",
            "Epoch 5/100, Train loss: 0.7621, Train acc: 59.2593%, Val loss: 0.6772, Val acc: 62.5000%, Best Val loss: 0.4194 Best Val acc: 75.00%\n",
            "Epoch 6/100, Train loss: 0.6182, Train acc: 64.0212%, Val loss: 0.4004, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 7/100, Train loss: 0.6151, Train acc: 65.6085%, Val loss: 0.5293, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 8/100, Train loss: 0.5849, Train acc: 71.9577%, Val loss: 0.4481, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 9/100, Train loss: 0.5810, Train acc: 70.8995%, Val loss: 0.4733, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 10/100, Train loss: 0.5939, Train acc: 67.1958%, Val loss: 0.5422, Val acc: 72.5000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 11/100, Train loss: 0.5638, Train acc: 71.4286%, Val loss: 0.4088, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 12/100, Train loss: 0.5649, Train acc: 68.7831%, Val loss: 0.5792, Val acc: 70.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 13/100, Train loss: 0.5615, Train acc: 73.5450%, Val loss: 0.4371, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 14/100, Train loss: 0.5369, Train acc: 71.4286%, Val loss: 0.5369, Val acc: 72.5000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 15/100, Train loss: 0.5480, Train acc: 68.2540%, Val loss: 0.4835, Val acc: 75.0000%, Best Val loss: 0.4004 Best Val acc: 75.00%\n",
            "Epoch 16/100, Train loss: 0.5497, Train acc: 73.0159%, Val loss: 0.3996, Val acc: 75.0000%, Best Val loss: 0.3996 Best Val acc: 75.00%\n",
            "Epoch 17/100, Train loss: 0.5443, Train acc: 69.8413%, Val loss: 0.5417, Val acc: 75.0000%, Best Val loss: 0.3996 Best Val acc: 75.00%\n",
            "Epoch 18/100, Train loss: 0.5376, Train acc: 70.8995%, Val loss: 0.3885, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 75.00%\n",
            "Epoch 19/100, Train loss: 0.5709, Train acc: 68.7831%, Val loss: 0.4379, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 75.00%\n",
            "Epoch 20/100, Train loss: 0.5255, Train acc: 71.4286%, Val loss: 0.5058, Val acc: 77.5000%, Best Val loss: 0.3885 Best Val acc: 77.50%\n",
            "Epoch 21/100, Train loss: 0.5291, Train acc: 72.4868%, Val loss: 0.3963, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 77.50%\n",
            "Epoch 22/100, Train loss: 0.5307, Train acc: 71.9577%, Val loss: 0.5668, Val acc: 67.5000%, Best Val loss: 0.3885 Best Val acc: 77.50%\n",
            "Epoch 23/100, Train loss: 0.5266, Train acc: 71.4286%, Val loss: 0.3928, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 77.50%\n",
            "Epoch 24/100, Train loss: 0.5578, Train acc: 70.3704%, Val loss: 0.4639, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 77.50%\n",
            "Epoch 25/100, Train loss: 0.5387, Train acc: 71.4286%, Val loss: 0.4899, Val acc: 80.0000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5124, Train acc: 73.0159%, Val loss: 0.4459, Val acc: 75.0000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5133, Train acc: 73.5450%, Val loss: 0.4796, Val acc: 80.0000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5298, Train acc: 72.4868%, Val loss: 0.4155, Val acc: 77.5000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.5264, Train acc: 68.2540%, Val loss: 0.4622, Val acc: 77.5000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5007, Train acc: 73.0159%, Val loss: 0.4501, Val acc: 77.5000%, Best Val loss: 0.3885 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5173, Train acc: 70.3704%, Val loss: 0.3692, Val acc: 72.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 32/100, Train loss: 0.5302, Train acc: 76.7196%, Val loss: 0.4770, Val acc: 80.0000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 33/100, Train loss: 0.4985, Train acc: 72.4868%, Val loss: 0.3855, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 34/100, Train loss: 0.5027, Train acc: 74.0741%, Val loss: 0.4539, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 35/100, Train loss: 0.4912, Train acc: 76.1905%, Val loss: 0.4078, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 36/100, Train loss: 0.5019, Train acc: 74.0741%, Val loss: 0.4072, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 37/100, Train loss: 0.4982, Train acc: 71.9577%, Val loss: 0.5505, Val acc: 67.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 38/100, Train loss: 0.5032, Train acc: 75.6614%, Val loss: 0.3888, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 39/100, Train loss: 0.5046, Train acc: 76.7196%, Val loss: 0.4007, Val acc: 77.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 40/100, Train loss: 0.5302, Train acc: 70.3704%, Val loss: 0.6195, Val acc: 62.5000%, Best Val loss: 0.3692 Best Val acc: 80.00%\n",
            "Epoch 41/100, Train loss: 0.5170, Train acc: 71.4286%, Val loss: 0.3548, Val acc: 75.0000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 42/100, Train loss: 0.4989, Train acc: 73.5450%, Val loss: 0.5425, Val acc: 67.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 43/100, Train loss: 0.4848, Train acc: 76.7196%, Val loss: 0.3692, Val acc: 77.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 44/100, Train loss: 0.5089, Train acc: 76.7196%, Val loss: 0.5741, Val acc: 67.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 45/100, Train loss: 0.4892, Train acc: 75.1323%, Val loss: 0.3698, Val acc: 77.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 46/100, Train loss: 0.4813, Train acc: 76.1905%, Val loss: 0.5026, Val acc: 67.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 47/100, Train loss: 0.4773, Train acc: 77.7778%, Val loss: 0.3842, Val acc: 77.5000%, Best Val loss: 0.3548 Best Val acc: 80.00%\n",
            "Epoch 48/100, Train loss: 0.4839, Train acc: 76.1905%, Val loss: 0.4110, Val acc: 82.5000%, Best Val loss: 0.3548 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5000, Train acc: 73.5450%, Val loss: 0.4839, Val acc: 70.0000%, Best Val loss: 0.3548 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4891, Train acc: 77.2487%, Val loss: 0.3451, Val acc: 77.5000%, Best Val loss: 0.3451 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4988, Train acc: 74.6032%, Val loss: 0.4616, Val acc: 75.0000%, Best Val loss: 0.3451 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4745, Train acc: 79.3651%, Val loss: 0.4321, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 53/100, Train loss: 0.4663, Train acc: 77.7778%, Val loss: 0.3877, Val acc: 80.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 54/100, Train loss: 0.4577, Train acc: 79.3651%, Val loss: 0.4575, Val acc: 80.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 55/100, Train loss: 0.4568, Train acc: 79.3651%, Val loss: 0.3465, Val acc: 80.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 56/100, Train loss: 0.4805, Train acc: 75.6614%, Val loss: 0.4671, Val acc: 75.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 57/100, Train loss: 0.4645, Train acc: 76.7196%, Val loss: 0.3919, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 58/100, Train loss: 0.4512, Train acc: 78.8360%, Val loss: 0.4042, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 59/100, Train loss: 0.4530, Train acc: 78.3069%, Val loss: 0.3984, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 60/100, Train loss: 0.4466, Train acc: 77.7778%, Val loss: 0.3876, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 61/100, Train loss: 0.4469, Train acc: 78.8360%, Val loss: 0.3912, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 62/100, Train loss: 0.4472, Train acc: 78.8360%, Val loss: 0.3841, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.4438, Train acc: 79.8942%, Val loss: 0.4116, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4556, Train acc: 76.7196%, Val loss: 0.3700, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4517, Train acc: 78.8360%, Val loss: 0.3940, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4440, Train acc: 79.3651%, Val loss: 0.3983, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4408, Train acc: 79.8942%, Val loss: 0.3644, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4431, Train acc: 78.3069%, Val loss: 0.3693, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4359, Train acc: 78.3069%, Val loss: 0.4202, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4444, Train acc: 79.8942%, Val loss: 0.3907, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4492, Train acc: 79.8942%, Val loss: 0.3461, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.4454, Train acc: 78.3069%, Val loss: 0.4344, Val acc: 80.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.4388, Train acc: 78.8360%, Val loss: 0.3599, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.4427, Train acc: 79.8942%, Val loss: 0.3861, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 75/100, Train loss: 0.4368, Train acc: 81.4815%, Val loss: 0.4103, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 76/100, Train loss: 0.4434, Train acc: 79.3651%, Val loss: 0.3584, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 77/100, Train loss: 0.4434, Train acc: 78.8360%, Val loss: 0.4095, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 78/100, Train loss: 0.4318, Train acc: 81.4815%, Val loss: 0.3697, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 79/100, Train loss: 0.4333, Train acc: 78.8360%, Val loss: 0.3673, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 80/100, Train loss: 0.4338, Train acc: 79.8942%, Val loss: 0.3934, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 81/100, Train loss: 0.4343, Train acc: 81.4815%, Val loss: 0.3711, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 82/100, Train loss: 0.4324, Train acc: 79.8942%, Val loss: 0.3678, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 83/100, Train loss: 0.4316, Train acc: 80.4233%, Val loss: 0.3870, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 84/100, Train loss: 0.4364, Train acc: 80.9524%, Val loss: 0.3996, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 85/100, Train loss: 0.4309, Train acc: 81.4815%, Val loss: 0.3727, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 86/100, Train loss: 0.4307, Train acc: 79.3651%, Val loss: 0.3645, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 87/100, Train loss: 0.4306, Train acc: 79.8942%, Val loss: 0.3753, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 88/100, Train loss: 0.4305, Train acc: 82.5397%, Val loss: 0.3825, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 89/100, Train loss: 0.4308, Train acc: 81.4815%, Val loss: 0.3874, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 90/100, Train loss: 0.4310, Train acc: 81.4815%, Val loss: 0.3800, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 91/100, Train loss: 0.4302, Train acc: 82.5397%, Val loss: 0.3745, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 92/100, Train loss: 0.4309, Train acc: 82.0106%, Val loss: 0.3709, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 93/100, Train loss: 0.4278, Train acc: 80.9524%, Val loss: 0.3720, Val acc: 85.0000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 94/100, Train loss: 0.4296, Train acc: 82.0106%, Val loss: 0.3763, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 95/100, Train loss: 0.4296, Train acc: 82.0106%, Val loss: 0.3787, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 96/100, Train loss: 0.4297, Train acc: 82.5397%, Val loss: 0.3786, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 97/100, Train loss: 0.4278, Train acc: 82.5397%, Val loss: 0.3794, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 98/100, Train loss: 0.4278, Train acc: 82.5397%, Val loss: 0.3792, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 99/100, Train loss: 0.4304, Train acc: 82.5397%, Val loss: 0.3792, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "Epoch 100/100, Train loss: 0.4287, Train acc: 82.5397%, Val loss: 0.3792, Val acc: 82.5000%, Best Val loss: 0.3451 Best Val acc: 85.00%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.3533, Train acc: 48.6772%, Val loss: 1.1073, Val acc: 62.5000%, Best Val loss: 1.1073 Best Val acc: 62.50%\n",
            "Epoch 2/100, Train loss: 1.3794, Train acc: 58.7302%, Val loss: 1.1750, Val acc: 52.5000%, Best Val loss: 1.1073 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 0.8827, Train acc: 64.5503%, Val loss: 0.4490, Val acc: 80.0000%, Best Val loss: 0.4490 Best Val acc: 80.00%\n",
            "Epoch 4/100, Train loss: 0.6741, Train acc: 68.2540%, Val loss: 0.3568, Val acc: 75.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 5/100, Train loss: 0.6850, Train acc: 65.0794%, Val loss: 0.6381, Val acc: 70.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 6/100, Train loss: 0.5594, Train acc: 72.4868%, Val loss: 0.4226, Val acc: 77.5000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 7/100, Train loss: 0.5360, Train acc: 71.4286%, Val loss: 0.5171, Val acc: 75.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 8/100, Train loss: 0.5414, Train acc: 71.4286%, Val loss: 0.4213, Val acc: 77.5000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.5501, Train acc: 73.0159%, Val loss: 0.3783, Val acc: 70.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.5984, Train acc: 68.7831%, Val loss: 0.3756, Val acc: 72.5000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.5545, Train acc: 72.4868%, Val loss: 0.7849, Val acc: 52.5000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6563, Train acc: 63.4921%, Val loss: 0.6634, Val acc: 65.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.6475, Train acc: 62.9630%, Val loss: 0.3752, Val acc: 70.0000%, Best Val loss: 0.3568 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.6228, Train acc: 69.3122%, Val loss: 0.3511, Val acc: 70.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.6261, Train acc: 63.4921%, Val loss: 0.3816, Val acc: 77.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5518, Train acc: 72.4868%, Val loss: 0.5525, Val acc: 72.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5531, Train acc: 68.2540%, Val loss: 0.7705, Val acc: 55.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5751, Train acc: 70.3704%, Val loss: 0.3673, Val acc: 75.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5864, Train acc: 69.3122%, Val loss: 0.3751, Val acc: 67.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.6345, Train acc: 65.6085%, Val loss: 0.3534, Val acc: 75.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5299, Train acc: 75.1323%, Val loss: 0.7073, Val acc: 62.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.6209, Train acc: 68.2540%, Val loss: 0.4145, Val acc: 80.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5808, Train acc: 69.3122%, Val loss: 0.3880, Val acc: 70.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5894, Train acc: 69.3122%, Val loss: 0.4984, Val acc: 77.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.6028, Train acc: 70.8995%, Val loss: 0.7586, Val acc: 57.5000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5417, Train acc: 70.8995%, Val loss: 0.3598, Val acc: 75.0000%, Best Val loss: 0.3511 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5617, Train acc: 69.8413%, Val loss: 0.3480, Val acc: 72.5000%, Best Val loss: 0.3480 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5113, Train acc: 73.0159%, Val loss: 0.3559, Val acc: 75.0000%, Best Val loss: 0.3480 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.4816, Train acc: 78.8360%, Val loss: 0.5434, Val acc: 72.5000%, Best Val loss: 0.3480 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5138, Train acc: 75.6614%, Val loss: 0.5231, Val acc: 75.0000%, Best Val loss: 0.3480 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5265, Train acc: 73.5450%, Val loss: 0.4059, Val acc: 82.5000%, Best Val loss: 0.3480 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5033, Train acc: 75.1323%, Val loss: 0.3504, Val acc: 75.0000%, Best Val loss: 0.3480 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5555, Train acc: 73.0159%, Val loss: 0.3587, Val acc: 77.5000%, Best Val loss: 0.3480 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.4824, Train acc: 77.2487%, Val loss: 0.4672, Val acc: 77.5000%, Best Val loss: 0.3480 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.4694, Train acc: 74.0741%, Val loss: 0.3458, Val acc: 77.5000%, Best Val loss: 0.3458 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.4784, Train acc: 74.0741%, Val loss: 0.3496, Val acc: 77.5000%, Best Val loss: 0.3458 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.4731, Train acc: 78.3069%, Val loss: 0.3803, Val acc: 80.0000%, Best Val loss: 0.3458 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.4724, Train acc: 78.3069%, Val loss: 0.3262, Val acc: 80.0000%, Best Val loss: 0.3262 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5324, Train acc: 70.3704%, Val loss: 0.3641, Val acc: 77.5000%, Best Val loss: 0.3262 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.4857, Train acc: 74.0741%, Val loss: 0.5559, Val acc: 70.0000%, Best Val loss: 0.3262 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.4793, Train acc: 77.2487%, Val loss: 0.3190, Val acc: 80.0000%, Best Val loss: 0.3190 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4715, Train acc: 76.1905%, Val loss: 0.3757, Val acc: 82.5000%, Best Val loss: 0.3190 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.4625, Train acc: 78.3069%, Val loss: 0.4843, Val acc: 75.0000%, Best Val loss: 0.3190 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5017, Train acc: 73.5450%, Val loss: 0.3429, Val acc: 80.0000%, Best Val loss: 0.3190 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.4846, Train acc: 75.6614%, Val loss: 0.3142, Val acc: 80.0000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.6207, Train acc: 66.6667%, Val loss: 0.5034, Val acc: 75.0000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4652, Train acc: 78.3069%, Val loss: 0.3539, Val acc: 77.5000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.4689, Train acc: 74.6032%, Val loss: 0.3162, Val acc: 80.0000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.4545, Train acc: 77.7778%, Val loss: 0.6270, Val acc: 67.5000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.5115, Train acc: 75.1323%, Val loss: 0.3355, Val acc: 75.0000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.5176, Train acc: 71.4286%, Val loss: 0.5388, Val acc: 75.0000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4870, Train acc: 77.2487%, Val loss: 0.3296, Val acc: 77.5000%, Best Val loss: 0.3142 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4905, Train acc: 74.0741%, Val loss: 0.3120, Val acc: 82.5000%, Best Val loss: 0.3120 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4467, Train acc: 75.6614%, Val loss: 0.4196, Val acc: 77.5000%, Best Val loss: 0.3120 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4430, Train acc: 79.8942%, Val loss: 0.3476, Val acc: 82.5000%, Best Val loss: 0.3120 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4452, Train acc: 77.7778%, Val loss: 0.3161, Val acc: 82.5000%, Best Val loss: 0.3120 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4346, Train acc: 79.8942%, Val loss: 0.4455, Val acc: 80.0000%, Best Val loss: 0.3120 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4577, Train acc: 77.2487%, Val loss: 0.3064, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 82.50%\n",
            "Epoch 59/100, Train loss: 0.4596, Train acc: 77.2487%, Val loss: 0.4712, Val acc: 80.0000%, Best Val loss: 0.3064 Best Val acc: 82.50%\n",
            "Epoch 60/100, Train loss: 0.4727, Train acc: 74.0741%, Val loss: 0.3182, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 82.50%\n",
            "Epoch 61/100, Train loss: 0.4471, Train acc: 74.6032%, Val loss: 0.3633, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 82.50%\n",
            "Epoch 62/100, Train loss: 0.4430, Train acc: 79.8942%, Val loss: 0.3302, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.4315, Train acc: 76.1905%, Val loss: 0.3517, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4159, Train acc: 81.4815%, Val loss: 0.3198, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4247, Train acc: 80.4233%, Val loss: 0.3661, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4164, Train acc: 82.5397%, Val loss: 0.3377, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4154, Train acc: 83.0688%, Val loss: 0.3342, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4194, Train acc: 82.5397%, Val loss: 0.3449, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4107, Train acc: 83.5979%, Val loss: 0.3388, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4218, Train acc: 80.9524%, Val loss: 0.3411, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4162, Train acc: 80.9524%, Val loss: 0.3748, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.4121, Train acc: 81.4815%, Val loss: 0.3127, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.4172, Train acc: 81.4815%, Val loss: 0.3807, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.4122, Train acc: 82.0106%, Val loss: 0.3298, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 75/100, Train loss: 0.4252, Train acc: 82.5397%, Val loss: 0.3383, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 76/100, Train loss: 0.4268, Train acc: 76.1905%, Val loss: 0.4087, Val acc: 77.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 77/100, Train loss: 0.4084, Train acc: 80.9524%, Val loss: 0.3081, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 78/100, Train loss: 0.4142, Train acc: 82.0106%, Val loss: 0.3277, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 79/100, Train loss: 0.4194, Train acc: 80.9524%, Val loss: 0.3756, Val acc: 82.5000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 80/100, Train loss: 0.4093, Train acc: 82.0106%, Val loss: 0.3147, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 81/100, Train loss: 0.4108, Train acc: 82.5397%, Val loss: 0.3241, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 85.00%\n",
            "Epoch 82/100, Train loss: 0.4137, Train acc: 84.1270%, Val loss: 0.3514, Val acc: 87.5000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4053, Train acc: 83.5979%, Val loss: 0.3361, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4107, Train acc: 83.0688%, Val loss: 0.3205, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4075, Train acc: 82.0106%, Val loss: 0.3547, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4064, Train acc: 83.5979%, Val loss: 0.3538, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4057, Train acc: 83.5979%, Val loss: 0.3268, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4065, Train acc: 82.5397%, Val loss: 0.3236, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4048, Train acc: 83.5979%, Val loss: 0.3417, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4032, Train acc: 84.6561%, Val loss: 0.3433, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4046, Train acc: 83.5979%, Val loss: 0.3381, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4034, Train acc: 83.5979%, Val loss: 0.3369, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4032, Train acc: 84.1270%, Val loss: 0.3392, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4028, Train acc: 83.5979%, Val loss: 0.3366, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4024, Train acc: 83.5979%, Val loss: 0.3380, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4037, Train acc: 84.1270%, Val loss: 0.3392, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4026, Train acc: 84.1270%, Val loss: 0.3391, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4025, Train acc: 84.1270%, Val loss: 0.3385, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4021, Train acc: 83.5979%, Val loss: 0.3386, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4008, Train acc: 83.5979%, Val loss: 0.3386, Val acc: 85.0000%, Best Val loss: 0.3064 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 8.6435, Train acc: 47.6190%, Val loss: 3.4227, Val acc: 62.5000%, Best Val loss: 3.4227 Best Val acc: 62.50%\n",
            "Epoch 2/100, Train loss: 3.5072, Train acc: 51.8519%, Val loss: 1.4434, Val acc: 52.5000%, Best Val loss: 1.4434 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 1.5323, Train acc: 59.2593%, Val loss: 1.4841, Val acc: 60.0000%, Best Val loss: 1.4434 Best Val acc: 62.50%\n",
            "Epoch 4/100, Train loss: 1.0119, Train acc: 67.1958%, Val loss: 0.3706, Val acc: 75.0000%, Best Val loss: 0.3706 Best Val acc: 75.00%\n",
            "Epoch 5/100, Train loss: 0.7601, Train acc: 68.7831%, Val loss: 0.3566, Val acc: 77.5000%, Best Val loss: 0.3566 Best Val acc: 77.50%\n",
            "Epoch 6/100, Train loss: 0.7712, Train acc: 65.6085%, Val loss: 0.3647, Val acc: 77.5000%, Best Val loss: 0.3566 Best Val acc: 77.50%\n",
            "Epoch 7/100, Train loss: 0.7548, Train acc: 61.9048%, Val loss: 0.4577, Val acc: 70.0000%, Best Val loss: 0.3566 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.6487, Train acc: 64.5503%, Val loss: 0.3808, Val acc: 75.0000%, Best Val loss: 0.3566 Best Val acc: 77.50%\n",
            "Epoch 9/100, Train loss: 0.5610, Train acc: 72.4868%, Val loss: 0.3496, Val acc: 80.0000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.6075, Train acc: 68.7831%, Val loss: 0.4256, Val acc: 75.0000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.6224, Train acc: 66.6667%, Val loss: 0.3740, Val acc: 75.0000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.5653, Train acc: 70.8995%, Val loss: 0.3777, Val acc: 77.5000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.5710, Train acc: 67.7249%, Val loss: 0.4823, Val acc: 65.0000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.6670, Train acc: 67.7249%, Val loss: 0.4269, Val acc: 75.0000%, Best Val loss: 0.3496 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.7213, Train acc: 67.1958%, Val loss: 0.3279, Val acc: 77.5000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5614, Train acc: 68.2540%, Val loss: 0.3295, Val acc: 77.5000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5396, Train acc: 72.4868%, Val loss: 0.4359, Val acc: 80.0000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5887, Train acc: 67.7249%, Val loss: 0.5928, Val acc: 62.5000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.8502, Train acc: 64.5503%, Val loss: 0.4570, Val acc: 67.5000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.8202, Train acc: 64.0212%, Val loss: 0.5563, Val acc: 67.5000%, Best Val loss: 0.3279 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.7984, Train acc: 67.7249%, Val loss: 0.3070, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.5726, Train acc: 69.8413%, Val loss: 0.5294, Val acc: 75.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5592, Train acc: 70.3704%, Val loss: 0.5334, Val acc: 75.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5239, Train acc: 73.0159%, Val loss: 0.4679, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5258, Train acc: 73.5450%, Val loss: 0.4436, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5398, Train acc: 70.8995%, Val loss: 0.3798, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.4895, Train acc: 75.6614%, Val loss: 0.3381, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5131, Train acc: 73.0159%, Val loss: 0.3789, Val acc: 75.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.6781, Train acc: 64.0212%, Val loss: 0.5761, Val acc: 65.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.8539, Train acc: 60.3175%, Val loss: 0.4706, Val acc: 75.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.7592, Train acc: 63.4921%, Val loss: 0.3718, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 32/100, Train loss: 0.5557, Train acc: 74.0741%, Val loss: 0.3344, Val acc: 75.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 33/100, Train loss: 0.5134, Train acc: 69.8413%, Val loss: 0.5618, Val acc: 72.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 34/100, Train loss: 0.4888, Train acc: 73.5450%, Val loss: 0.3761, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 35/100, Train loss: 0.4539, Train acc: 80.4233%, Val loss: 0.5552, Val acc: 72.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 36/100, Train loss: 0.4820, Train acc: 74.0741%, Val loss: 0.4730, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 37/100, Train loss: 0.5211, Train acc: 68.2540%, Val loss: 0.3490, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 38/100, Train loss: 0.6116, Train acc: 67.1958%, Val loss: 0.3225, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 39/100, Train loss: 0.5248, Train acc: 71.9577%, Val loss: 0.3312, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 40/100, Train loss: 0.4489, Train acc: 78.8360%, Val loss: 0.3216, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 41/100, Train loss: 0.4489, Train acc: 76.1905%, Val loss: 0.3251, Val acc: 80.0000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 42/100, Train loss: 0.5407, Train acc: 71.9577%, Val loss: 0.3823, Val acc: 77.5000%, Best Val loss: 0.3070 Best Val acc: 80.00%\n",
            "Epoch 43/100, Train loss: 0.6083, Train acc: 66.1376%, Val loss: 0.3001, Val acc: 82.5000%, Best Val loss: 0.3001 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5127, Train acc: 70.8995%, Val loss: 0.3118, Val acc: 80.0000%, Best Val loss: 0.3001 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.4761, Train acc: 73.5450%, Val loss: 0.3139, Val acc: 85.0000%, Best Val loss: 0.3001 Best Val acc: 85.00%\n",
            "Epoch 46/100, Train loss: 0.4363, Train acc: 80.4233%, Val loss: 0.3192, Val acc: 77.5000%, Best Val loss: 0.3001 Best Val acc: 85.00%\n",
            "Epoch 47/100, Train loss: 0.4476, Train acc: 79.8942%, Val loss: 0.4543, Val acc: 75.0000%, Best Val loss: 0.3001 Best Val acc: 85.00%\n",
            "Epoch 48/100, Train loss: 0.5509, Train acc: 68.2540%, Val loss: 0.7692, Val acc: 52.5000%, Best Val loss: 0.3001 Best Val acc: 85.00%\n",
            "Epoch 49/100, Train loss: 0.4629, Train acc: 76.1905%, Val loss: 0.5400, Val acc: 72.5000%, Best Val loss: 0.3001 Best Val acc: 85.00%\n",
            "Epoch 50/100, Train loss: 0.4662, Train acc: 77.2487%, Val loss: 0.2929, Val acc: 80.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 51/100, Train loss: 0.4477, Train acc: 75.1323%, Val loss: 0.3180, Val acc: 82.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 52/100, Train loss: 0.5547, Train acc: 71.9577%, Val loss: 0.3049, Val acc: 77.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 53/100, Train loss: 0.4270, Train acc: 78.3069%, Val loss: 0.7110, Val acc: 52.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 54/100, Train loss: 0.4871, Train acc: 75.1323%, Val loss: 0.3268, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 55/100, Train loss: 0.4132, Train acc: 77.2487%, Val loss: 0.3003, Val acc: 80.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 56/100, Train loss: 0.4222, Train acc: 77.7778%, Val loss: 0.3170, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 57/100, Train loss: 0.4170, Train acc: 80.4233%, Val loss: 0.5341, Val acc: 72.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 58/100, Train loss: 0.4256, Train acc: 79.8942%, Val loss: 0.3416, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 59/100, Train loss: 0.4151, Train acc: 82.0106%, Val loss: 0.3323, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 60/100, Train loss: 0.4014, Train acc: 82.5397%, Val loss: 0.3328, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 61/100, Train loss: 0.4141, Train acc: 80.4233%, Val loss: 0.3347, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 62/100, Train loss: 0.3977, Train acc: 83.0688%, Val loss: 0.3793, Val acc: 82.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.4132, Train acc: 79.3651%, Val loss: 0.3046, Val acc: 80.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4103, Train acc: 80.9524%, Val loss: 0.3948, Val acc: 80.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4215, Train acc: 80.4233%, Val loss: 0.5086, Val acc: 77.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4322, Train acc: 78.3069%, Val loss: 0.3027, Val acc: 82.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4541, Train acc: 75.6614%, Val loss: 0.3435, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4138, Train acc: 82.5397%, Val loss: 0.4439, Val acc: 77.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4214, Train acc: 80.4233%, Val loss: 0.2989, Val acc: 82.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4082, Train acc: 82.0106%, Val loss: 0.4978, Val acc: 77.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4117, Train acc: 80.9524%, Val loss: 0.3116, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.3940, Train acc: 82.0106%, Val loss: 0.3163, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.3869, Train acc: 82.5397%, Val loss: 0.3834, Val acc: 82.5000%, Best Val loss: 0.2929 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.3913, Train acc: 83.5979%, Val loss: 0.3424, Val acc: 87.5000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.3873, Train acc: 82.5397%, Val loss: 0.3342, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.3990, Train acc: 80.9524%, Val loss: 0.3606, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.3955, Train acc: 80.9524%, Val loss: 0.3328, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.3819, Train acc: 84.6561%, Val loss: 0.3360, Val acc: 87.5000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.3875, Train acc: 82.5397%, Val loss: 0.3364, Val acc: 87.5000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.3742, Train acc: 86.2434%, Val loss: 0.3175, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.3798, Train acc: 83.5979%, Val loss: 0.3314, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.3902, Train acc: 82.5397%, Val loss: 0.3508, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.3699, Train acc: 86.2434%, Val loss: 0.3109, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.3840, Train acc: 83.0688%, Val loss: 0.3357, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.3815, Train acc: 82.5397%, Val loss: 0.3789, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.3754, Train acc: 84.6561%, Val loss: 0.3186, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.3799, Train acc: 83.0688%, Val loss: 0.3215, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.3823, Train acc: 84.1270%, Val loss: 0.3627, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.3767, Train acc: 85.7143%, Val loss: 0.3329, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.3745, Train acc: 86.2434%, Val loss: 0.3349, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.3761, Train acc: 85.1852%, Val loss: 0.3241, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.3754, Train acc: 84.6561%, Val loss: 0.3374, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.3772, Train acc: 84.6561%, Val loss: 0.3463, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.3719, Train acc: 85.7143%, Val loss: 0.3388, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.3725, Train acc: 87.3016%, Val loss: 0.3345, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.3742, Train acc: 86.2434%, Val loss: 0.3308, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.3743, Train acc: 86.2434%, Val loss: 0.3321, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.3732, Train acc: 86.2434%, Val loss: 0.3332, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.3730, Train acc: 86.2434%, Val loss: 0.3337, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.3752, Train acc: 86.7725%, Val loss: 0.3337, Val acc: 85.0000%, Best Val loss: 0.2929 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 1.6269, Train acc: 56.6138%, Val loss: 0.7266, Val acc: 60.0000%, Best Val loss: 0.7266 Best Val acc: 60.00%\n",
            "Epoch 2/100, Train loss: 0.8283, Train acc: 62.9630%, Val loss: 0.4277, Val acc: 75.0000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 3/100, Train loss: 0.7141, Train acc: 65.6085%, Val loss: 0.4807, Val acc: 75.0000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 4/100, Train loss: 0.6700, Train acc: 66.1376%, Val loss: 0.4577, Val acc: 72.5000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 5/100, Train loss: 0.6086, Train acc: 67.1958%, Val loss: 0.4920, Val acc: 75.0000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 6/100, Train loss: 0.6019, Train acc: 67.7249%, Val loss: 0.4302, Val acc: 72.5000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 7/100, Train loss: 0.5995, Train acc: 70.8995%, Val loss: 0.4664, Val acc: 72.5000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 8/100, Train loss: 0.6394, Train acc: 66.1376%, Val loss: 0.5621, Val acc: 72.5000%, Best Val loss: 0.4277 Best Val acc: 75.00%\n",
            "Epoch 9/100, Train loss: 0.5714, Train acc: 68.7831%, Val loss: 0.3897, Val acc: 77.5000%, Best Val loss: 0.3897 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.5735, Train acc: 67.7249%, Val loss: 0.7024, Val acc: 60.0000%, Best Val loss: 0.3897 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5848, Train acc: 68.7831%, Val loss: 0.3740, Val acc: 77.5000%, Best Val loss: 0.3740 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5724, Train acc: 70.8995%, Val loss: 0.6018, Val acc: 70.0000%, Best Val loss: 0.3740 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.5659, Train acc: 66.1376%, Val loss: 0.3720, Val acc: 77.5000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.5473, Train acc: 70.8995%, Val loss: 0.5402, Val acc: 72.5000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 15/100, Train loss: 0.5586, Train acc: 73.5450%, Val loss: 0.4693, Val acc: 70.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 16/100, Train loss: 0.5303, Train acc: 71.9577%, Val loss: 0.4333, Val acc: 70.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 17/100, Train loss: 0.5271, Train acc: 73.5450%, Val loss: 0.4477, Val acc: 70.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 18/100, Train loss: 0.5236, Train acc: 71.9577%, Val loss: 0.5397, Val acc: 72.5000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 19/100, Train loss: 0.5338, Train acc: 73.0159%, Val loss: 0.4136, Val acc: 75.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 20/100, Train loss: 0.5219, Train acc: 73.0159%, Val loss: 0.4059, Val acc: 75.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 21/100, Train loss: 0.5289, Train acc: 74.0741%, Val loss: 0.5296, Val acc: 72.5000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 22/100, Train loss: 0.5158, Train acc: 75.6614%, Val loss: 0.3803, Val acc: 75.0000%, Best Val loss: 0.3720 Best Val acc: 77.50%\n",
            "Epoch 23/100, Train loss: 0.5103, Train acc: 74.6032%, Val loss: 0.4904, Val acc: 80.0000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5110, Train acc: 76.1905%, Val loss: 0.3737, Val acc: 75.0000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5181, Train acc: 71.4286%, Val loss: 0.4527, Val acc: 80.0000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5092, Train acc: 70.3704%, Val loss: 0.4303, Val acc: 72.5000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5043, Train acc: 74.6032%, Val loss: 0.3976, Val acc: 75.0000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.4974, Train acc: 76.7196%, Val loss: 0.4259, Val acc: 70.0000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.4894, Train acc: 75.6614%, Val loss: 0.3921, Val acc: 77.5000%, Best Val loss: 0.3720 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5221, Train acc: 72.4868%, Val loss: 0.3649, Val acc: 82.5000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5556, Train acc: 71.9577%, Val loss: 0.3741, Val acc: 80.0000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.4938, Train acc: 75.1323%, Val loss: 0.4971, Val acc: 72.5000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.4967, Train acc: 71.9577%, Val loss: 0.4070, Val acc: 75.0000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.4808, Train acc: 76.1905%, Val loss: 0.3827, Val acc: 77.5000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.4877, Train acc: 75.6614%, Val loss: 0.4782, Val acc: 72.5000%, Best Val loss: 0.3649 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5265, Train acc: 73.5450%, Val loss: 0.3615, Val acc: 82.5000%, Best Val loss: 0.3615 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5157, Train acc: 76.7196%, Val loss: 0.4678, Val acc: 72.5000%, Best Val loss: 0.3615 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.4811, Train acc: 77.7778%, Val loss: 0.3807, Val acc: 77.5000%, Best Val loss: 0.3615 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.4779, Train acc: 79.3651%, Val loss: 0.3943, Val acc: 80.0000%, Best Val loss: 0.3615 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.4695, Train acc: 75.6614%, Val loss: 0.4107, Val acc: 80.0000%, Best Val loss: 0.3615 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.4705, Train acc: 78.3069%, Val loss: 0.3606, Val acc: 80.0000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4665, Train acc: 78.8360%, Val loss: 0.4343, Val acc: 82.5000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.4565, Train acc: 78.3069%, Val loss: 0.3695, Val acc: 77.5000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.4614, Train acc: 77.7778%, Val loss: 0.3948, Val acc: 80.0000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.4521, Train acc: 79.3651%, Val loss: 0.3902, Val acc: 82.5000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.4483, Train acc: 78.8360%, Val loss: 0.3996, Val acc: 82.5000%, Best Val loss: 0.3606 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4519, Train acc: 79.3651%, Val loss: 0.3595, Val acc: 77.5000%, Best Val loss: 0.3595 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.4553, Train acc: 80.9524%, Val loss: 0.5197, Val acc: 67.5000%, Best Val loss: 0.3595 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5045, Train acc: 74.6032%, Val loss: 0.3452, Val acc: 80.0000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4878, Train acc: 77.2487%, Val loss: 0.5181, Val acc: 70.0000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4570, Train acc: 78.3069%, Val loss: 0.3320, Val acc: 77.5000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4624, Train acc: 79.3651%, Val loss: 0.3798, Val acc: 80.0000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4492, Train acc: 79.8942%, Val loss: 0.3782, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4518, Train acc: 77.2487%, Val loss: 0.3592, Val acc: 80.0000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4409, Train acc: 82.0106%, Val loss: 0.3759, Val acc: 80.0000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4366, Train acc: 79.3651%, Val loss: 0.3625, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4358, Train acc: 81.4815%, Val loss: 0.3812, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 58/100, Train loss: 0.4343, Train acc: 82.0106%, Val loss: 0.3659, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 59/100, Train loss: 0.4307, Train acc: 80.4233%, Val loss: 0.3673, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 60/100, Train loss: 0.4287, Train acc: 82.0106%, Val loss: 0.3885, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 61/100, Train loss: 0.4289, Train acc: 81.4815%, Val loss: 0.3702, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 62/100, Train loss: 0.4413, Train acc: 80.9524%, Val loss: 0.3998, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.4594, Train acc: 78.3069%, Val loss: 0.3738, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4357, Train acc: 80.9524%, Val loss: 0.3339, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4280, Train acc: 82.5397%, Val loss: 0.3925, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4297, Train acc: 81.4815%, Val loss: 0.3473, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4235, Train acc: 82.5397%, Val loss: 0.3722, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4272, Train acc: 83.0688%, Val loss: 0.3550, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4208, Train acc: 84.6561%, Val loss: 0.3738, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4208, Train acc: 80.9524%, Val loss: 0.3674, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4217, Train acc: 83.0688%, Val loss: 0.3543, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.4174, Train acc: 84.1270%, Val loss: 0.3783, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.4240, Train acc: 80.9524%, Val loss: 0.3717, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.4186, Train acc: 82.0106%, Val loss: 0.3450, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 75/100, Train loss: 0.4172, Train acc: 84.6561%, Val loss: 0.3593, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 76/100, Train loss: 0.4180, Train acc: 83.0688%, Val loss: 0.3847, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 77/100, Train loss: 0.4147, Train acc: 83.0688%, Val loss: 0.3494, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 78/100, Train loss: 0.4183, Train acc: 84.1270%, Val loss: 0.3527, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 79/100, Train loss: 0.4153, Train acc: 84.6561%, Val loss: 0.3659, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 80/100, Train loss: 0.4150, Train acc: 84.6561%, Val loss: 0.3566, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 81/100, Train loss: 0.4156, Train acc: 85.1852%, Val loss: 0.3484, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 82/100, Train loss: 0.4122, Train acc: 83.5979%, Val loss: 0.3668, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 83/100, Train loss: 0.4178, Train acc: 82.0106%, Val loss: 0.3589, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 84/100, Train loss: 0.4133, Train acc: 83.5979%, Val loss: 0.3625, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 85/100, Train loss: 0.4126, Train acc: 82.0106%, Val loss: 0.3648, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 86/100, Train loss: 0.4119, Train acc: 81.4815%, Val loss: 0.3623, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 87/100, Train loss: 0.4098, Train acc: 84.1270%, Val loss: 0.3585, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 88/100, Train loss: 0.4159, Train acc: 84.6561%, Val loss: 0.3475, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 89/100, Train loss: 0.4116, Train acc: 85.1852%, Val loss: 0.3547, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 90/100, Train loss: 0.4100, Train acc: 84.1270%, Val loss: 0.3595, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 91/100, Train loss: 0.4110, Train acc: 82.5397%, Val loss: 0.3660, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 92/100, Train loss: 0.4131, Train acc: 82.5397%, Val loss: 0.3641, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 93/100, Train loss: 0.4111, Train acc: 82.5397%, Val loss: 0.3617, Val acc: 85.0000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 94/100, Train loss: 0.4106, Train acc: 83.0688%, Val loss: 0.3607, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 95/100, Train loss: 0.4108, Train acc: 84.1270%, Val loss: 0.3601, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 96/100, Train loss: 0.4098, Train acc: 83.5979%, Val loss: 0.3593, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 97/100, Train loss: 0.4107, Train acc: 84.1270%, Val loss: 0.3587, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 98/100, Train loss: 0.4099, Train acc: 84.1270%, Val loss: 0.3587, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 99/100, Train loss: 0.4105, Train acc: 84.1270%, Val loss: 0.3586, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "Epoch 100/100, Train loss: 0.4105, Train acc: 84.1270%, Val loss: 0.3586, Val acc: 82.5000%, Best Val loss: 0.3320 Best Val acc: 85.00%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.3847, Train acc: 50.2646%, Val loss: 3.0500, Val acc: 37.5000%, Best Val loss: 3.0500 Best Val acc: 37.50%\n",
            "Epoch 2/100, Train loss: 1.5579, Train acc: 60.8466%, Val loss: 0.3994, Val acc: 70.0000%, Best Val loss: 0.3994 Best Val acc: 70.00%\n",
            "Epoch 3/100, Train loss: 0.8182, Train acc: 70.8995%, Val loss: 0.3291, Val acc: 75.0000%, Best Val loss: 0.3291 Best Val acc: 75.00%\n",
            "Epoch 4/100, Train loss: 0.7768, Train acc: 63.4921%, Val loss: 0.7285, Val acc: 67.5000%, Best Val loss: 0.3291 Best Val acc: 75.00%\n",
            "Epoch 5/100, Train loss: 0.6008, Train acc: 68.7831%, Val loss: 0.4617, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 6/100, Train loss: 0.5659, Train acc: 71.4286%, Val loss: 0.3823, Val acc: 75.0000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 7/100, Train loss: 0.5929, Train acc: 68.7831%, Val loss: 0.3672, Val acc: 72.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.6055, Train acc: 67.1958%, Val loss: 0.3658, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 9/100, Train loss: 0.6120, Train acc: 68.2540%, Val loss: 0.6085, Val acc: 67.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.6009, Train acc: 69.8413%, Val loss: 0.7056, Val acc: 60.0000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 11/100, Train loss: 0.5857, Train acc: 69.3122%, Val loss: 0.5298, Val acc: 75.0000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 12/100, Train loss: 0.5884, Train acc: 67.1958%, Val loss: 0.4075, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 13/100, Train loss: 0.5737, Train acc: 69.8413%, Val loss: 0.3630, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 77.50%\n",
            "Epoch 14/100, Train loss: 0.5394, Train acc: 70.8995%, Val loss: 0.4906, Val acc: 80.0000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.5538, Train acc: 71.4286%, Val loss: 0.3677, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.6150, Train acc: 63.4921%, Val loss: 0.3925, Val acc: 80.0000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5612, Train acc: 70.8995%, Val loss: 0.3482, Val acc: 75.0000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5478, Train acc: 71.9577%, Val loss: 0.3416, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5725, Train acc: 69.3122%, Val loss: 0.3684, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5929, Train acc: 68.7831%, Val loss: 0.3752, Val acc: 80.0000%, Best Val loss: 0.3291 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5572, Train acc: 70.8995%, Val loss: 0.4013, Val acc: 82.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 22/100, Train loss: 0.5975, Train acc: 67.1958%, Val loss: 0.4815, Val acc: 80.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 23/100, Train loss: 0.5273, Train acc: 72.4868%, Val loss: 0.8269, Val acc: 52.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 24/100, Train loss: 0.5854, Train acc: 68.7831%, Val loss: 0.7242, Val acc: 55.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 25/100, Train loss: 0.5939, Train acc: 68.2540%, Val loss: 0.5884, Val acc: 70.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 26/100, Train loss: 0.5531, Train acc: 71.4286%, Val loss: 0.7758, Val acc: 57.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 27/100, Train loss: 0.6995, Train acc: 64.5503%, Val loss: 0.3609, Val acc: 82.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 28/100, Train loss: 0.5763, Train acc: 73.0159%, Val loss: 0.3714, Val acc: 82.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.5201, Train acc: 73.5450%, Val loss: 0.3777, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5892, Train acc: 69.3122%, Val loss: 0.3636, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.5058, Train acc: 74.6032%, Val loss: 0.3867, Val acc: 80.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.4927, Train acc: 76.1905%, Val loss: 0.4338, Val acc: 82.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.5129, Train acc: 70.3704%, Val loss: 0.5941, Val acc: 72.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.4862, Train acc: 74.6032%, Val loss: 0.3982, Val acc: 82.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.4739, Train acc: 76.1905%, Val loss: 0.3543, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.4927, Train acc: 76.1905%, Val loss: 0.3526, Val acc: 77.5000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5289, Train acc: 71.4286%, Val loss: 0.5773, Val acc: 70.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5876, Train acc: 69.3122%, Val loss: 0.8805, Val acc: 50.0000%, Best Val loss: 0.3291 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.6235, Train acc: 69.3122%, Val loss: 0.3242, Val acc: 77.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5643, Train acc: 69.8413%, Val loss: 0.3440, Val acc: 77.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5968, Train acc: 68.7831%, Val loss: 0.3832, Val acc: 82.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4781, Train acc: 79.3651%, Val loss: 0.4868, Val acc: 75.0000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.4620, Train acc: 77.7778%, Val loss: 0.3397, Val acc: 80.0000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.5586, Train acc: 66.6667%, Val loss: 0.3476, Val acc: 77.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.5195, Train acc: 74.0741%, Val loss: 0.7107, Val acc: 57.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.5739, Train acc: 66.1376%, Val loss: 0.3894, Val acc: 82.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4837, Train acc: 74.6032%, Val loss: 0.3335, Val acc: 80.0000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.4786, Train acc: 74.6032%, Val loss: 0.3589, Val acc: 80.0000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5106, Train acc: 74.0741%, Val loss: 0.5963, Val acc: 67.5000%, Best Val loss: 0.3242 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4729, Train acc: 78.8360%, Val loss: 0.3234, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4617, Train acc: 78.3069%, Val loss: 0.3932, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4429, Train acc: 80.9524%, Val loss: 0.3535, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4430, Train acc: 79.8942%, Val loss: 0.4359, Val acc: 75.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4417, Train acc: 78.8360%, Val loss: 0.3580, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4418, Train acc: 79.3651%, Val loss: 0.4464, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4592, Train acc: 77.7778%, Val loss: 0.3329, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4610, Train acc: 78.8360%, Val loss: 0.3896, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4465, Train acc: 80.4233%, Val loss: 0.3608, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 59/100, Train loss: 0.4641, Train acc: 74.0741%, Val loss: 0.3316, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 60/100, Train loss: 0.4611, Train acc: 78.3069%, Val loss: 0.5575, Val acc: 70.0000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 61/100, Train loss: 0.4726, Train acc: 76.7196%, Val loss: 0.3334, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 62/100, Train loss: 0.4322, Train acc: 78.8360%, Val loss: 0.4340, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 82.50%\n",
            "Epoch 63/100, Train loss: 0.4271, Train acc: 80.9524%, Val loss: 0.3362, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4291, Train acc: 79.3651%, Val loss: 0.4470, Val acc: 72.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4533, Train acc: 77.2487%, Val loss: 0.3393, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4336, Train acc: 80.9524%, Val loss: 0.3801, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4266, Train acc: 80.4233%, Val loss: 0.3689, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4256, Train acc: 82.5397%, Val loss: 0.3915, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4257, Train acc: 82.0106%, Val loss: 0.3770, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 70/100, Train loss: 0.4232, Train acc: 82.5397%, Val loss: 0.3747, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 71/100, Train loss: 0.4228, Train acc: 82.5397%, Val loss: 0.3602, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 72/100, Train loss: 0.4322, Train acc: 80.9524%, Val loss: 0.3704, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 73/100, Train loss: 0.4284, Train acc: 80.9524%, Val loss: 0.3517, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 74/100, Train loss: 0.4156, Train acc: 82.0106%, Val loss: 0.3995, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 85.00%\n",
            "Epoch 75/100, Train loss: 0.4236, Train acc: 83.0688%, Val loss: 0.3591, Val acc: 87.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4292, Train acc: 80.4233%, Val loss: 0.3581, Val acc: 87.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4223, Train acc: 82.0106%, Val loss: 0.3982, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4199, Train acc: 84.1270%, Val loss: 0.3599, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4198, Train acc: 82.0106%, Val loss: 0.3840, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4164, Train acc: 82.0106%, Val loss: 0.3495, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4175, Train acc: 80.9524%, Val loss: 0.3817, Val acc: 80.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4142, Train acc: 83.0688%, Val loss: 0.3708, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4153, Train acc: 82.5397%, Val loss: 0.3468, Val acc: 87.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4141, Train acc: 82.0106%, Val loss: 0.3688, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4140, Train acc: 83.5979%, Val loss: 0.3840, Val acc: 77.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4149, Train acc: 82.5397%, Val loss: 0.3666, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4100, Train acc: 83.5979%, Val loss: 0.3602, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4147, Train acc: 82.5397%, Val loss: 0.3623, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4109, Train acc: 82.5397%, Val loss: 0.3615, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4128, Train acc: 83.5979%, Val loss: 0.3699, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4121, Train acc: 83.0688%, Val loss: 0.3731, Val acc: 82.5000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4115, Train acc: 83.5979%, Val loss: 0.3695, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4114, Train acc: 83.5979%, Val loss: 0.3629, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4132, Train acc: 82.5397%, Val loss: 0.3607, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4127, Train acc: 82.5397%, Val loss: 0.3603, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4116, Train acc: 83.0688%, Val loss: 0.3640, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4105, Train acc: 84.1270%, Val loss: 0.3642, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4091, Train acc: 84.1270%, Val loss: 0.3650, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4098, Train acc: 84.1270%, Val loss: 0.3652, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4103, Train acc: 84.1270%, Val loss: 0.3653, Val acc: 85.0000%, Best Val loss: 0.3234 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 5.9487, Train acc: 48.6772%, Val loss: 0.4336, Val acc: 80.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 2/100, Train loss: 2.1650, Train acc: 55.5556%, Val loss: 0.8474, Val acc: 65.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 3/100, Train loss: 1.5222, Train acc: 57.6720%, Val loss: 0.4696, Val acc: 75.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 4/100, Train loss: 1.5443, Train acc: 56.6138%, Val loss: 1.5573, Val acc: 50.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 5/100, Train loss: 1.1250, Train acc: 65.6085%, Val loss: 1.2244, Val acc: 55.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 6/100, Train loss: 0.9080, Train acc: 66.6667%, Val loss: 0.4613, Val acc: 70.0000%, Best Val loss: 0.4336 Best Val acc: 80.00%\n",
            "Epoch 7/100, Train loss: 0.8485, Train acc: 60.8466%, Val loss: 0.3462, Val acc: 75.0000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 8/100, Train loss: 0.6566, Train acc: 67.1958%, Val loss: 0.4017, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.7071, Train acc: 66.6667%, Val loss: 0.9748, Val acc: 47.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.7856, Train acc: 62.9630%, Val loss: 0.5854, Val acc: 72.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.6059, Train acc: 69.8413%, Val loss: 0.4658, Val acc: 80.0000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6444, Train acc: 68.7831%, Val loss: 0.9133, Val acc: 47.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.8080, Train acc: 64.0212%, Val loss: 1.0122, Val acc: 47.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.6040, Train acc: 70.8995%, Val loss: 0.4749, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.5311, Train acc: 72.4868%, Val loss: 0.5570, Val acc: 72.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5537, Train acc: 68.7831%, Val loss: 0.4508, Val acc: 80.0000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5209, Train acc: 71.4286%, Val loss: 0.4128, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5370, Train acc: 69.3122%, Val loss: 0.4633, Val acc: 80.0000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5332, Train acc: 70.3704%, Val loss: 0.5103, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5261, Train acc: 72.4868%, Val loss: 0.4661, Val acc: 82.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 21/100, Train loss: 0.5041, Train acc: 73.5450%, Val loss: 0.3801, Val acc: 72.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 22/100, Train loss: 0.5285, Train acc: 71.9577%, Val loss: 0.3745, Val acc: 72.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 23/100, Train loss: 0.5353, Train acc: 73.5450%, Val loss: 0.4917, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 24/100, Train loss: 0.5285, Train acc: 71.9577%, Val loss: 0.3659, Val acc: 70.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 25/100, Train loss: 0.5003, Train acc: 75.6614%, Val loss: 0.4626, Val acc: 82.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 26/100, Train loss: 0.5033, Train acc: 74.0741%, Val loss: 0.4938, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 27/100, Train loss: 0.4903, Train acc: 75.1323%, Val loss: 0.3826, Val acc: 75.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 28/100, Train loss: 0.4926, Train acc: 74.0741%, Val loss: 0.6050, Val acc: 70.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.6145, Train acc: 66.1376%, Val loss: 0.6102, Val acc: 70.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5576, Train acc: 70.8995%, Val loss: 0.6298, Val acc: 67.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.7755, Train acc: 63.4921%, Val loss: 0.6954, Val acc: 60.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.7762, Train acc: 66.1376%, Val loss: 1.2161, Val acc: 47.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.6494, Train acc: 66.1376%, Val loss: 0.4931, Val acc: 77.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.4981, Train acc: 74.6032%, Val loss: 0.4569, Val acc: 82.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.5098, Train acc: 73.0159%, Val loss: 0.3605, Val acc: 70.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.4936, Train acc: 74.6032%, Val loss: 0.6277, Val acc: 67.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5492, Train acc: 74.0741%, Val loss: 0.7942, Val acc: 52.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5192, Train acc: 70.8995%, Val loss: 0.5705, Val acc: 75.0000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5212, Train acc: 73.0159%, Val loss: 0.5717, Val acc: 62.5000%, Best Val loss: 0.3462 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5643, Train acc: 71.4286%, Val loss: 0.3440, Val acc: 80.0000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5681, Train acc: 67.7249%, Val loss: 0.7927, Val acc: 60.0000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.6445, Train acc: 68.7831%, Val loss: 1.3231, Val acc: 45.0000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.9137, Train acc: 63.4921%, Val loss: 0.4597, Val acc: 77.5000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.6443, Train acc: 67.7249%, Val loss: 0.3815, Val acc: 80.0000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.7040, Train acc: 65.0794%, Val loss: 0.5023, Val acc: 62.5000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.6836, Train acc: 67.7249%, Val loss: 0.3518, Val acc: 77.5000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.5643, Train acc: 74.0741%, Val loss: 0.5872, Val acc: 75.0000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 48/100, Train loss: 0.5263, Train acc: 73.0159%, Val loss: 0.4121, Val acc: 82.5000%, Best Val loss: 0.3440 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.5464, Train acc: 69.3122%, Val loss: 0.3273, Val acc: 75.0000%, Best Val loss: 0.3273 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4950, Train acc: 74.0741%, Val loss: 0.3252, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4641, Train acc: 76.1905%, Val loss: 0.3710, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4858, Train acc: 75.6614%, Val loss: 0.6603, Val acc: 62.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4937, Train acc: 74.0741%, Val loss: 0.4860, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4705, Train acc: 77.7778%, Val loss: 0.3533, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4492, Train acc: 76.7196%, Val loss: 0.3322, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4247, Train acc: 79.3651%, Val loss: 0.6232, Val acc: 67.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4882, Train acc: 76.7196%, Val loss: 0.3885, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4395, Train acc: 80.9524%, Val loss: 0.3279, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 59/100, Train loss: 0.4490, Train acc: 77.7778%, Val loss: 0.4353, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 60/100, Train loss: 0.4431, Train acc: 76.1905%, Val loss: 0.4675, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 61/100, Train loss: 0.4413, Train acc: 77.2487%, Val loss: 0.3526, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 62/100, Train loss: 0.4194, Train acc: 82.0106%, Val loss: 0.3848, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 63/100, Train loss: 0.4424, Train acc: 77.7778%, Val loss: 0.4815, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 64/100, Train loss: 0.4203, Train acc: 80.9524%, Val loss: 0.3382, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 65/100, Train loss: 0.4214, Train acc: 78.3069%, Val loss: 0.3896, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 66/100, Train loss: 0.4498, Train acc: 79.8942%, Val loss: 0.4977, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 67/100, Train loss: 0.4511, Train acc: 77.7778%, Val loss: 0.3271, Val acc: 80.0000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 68/100, Train loss: 0.4457, Train acc: 77.2487%, Val loss: 0.5227, Val acc: 77.5000%, Best Val loss: 0.3252 Best Val acc: 82.50%\n",
            "Epoch 69/100, Train loss: 0.4432, Train acc: 82.5397%, Val loss: 0.3249, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 70/100, Train loss: 0.4518, Train acc: 79.8942%, Val loss: 0.4115, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 71/100, Train loss: 0.4109, Train acc: 80.9524%, Val loss: 0.3401, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 72/100, Train loss: 0.4187, Train acc: 79.8942%, Val loss: 0.3527, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 73/100, Train loss: 0.4084, Train acc: 82.0106%, Val loss: 0.3827, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 74/100, Train loss: 0.4111, Train acc: 81.4815%, Val loss: 0.3607, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 75/100, Train loss: 0.4083, Train acc: 81.4815%, Val loss: 0.3759, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 76/100, Train loss: 0.4099, Train acc: 81.4815%, Val loss: 0.3630, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 77/100, Train loss: 0.4093, Train acc: 82.5397%, Val loss: 0.3532, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 78/100, Train loss: 0.3997, Train acc: 83.0688%, Val loss: 0.3939, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 79/100, Train loss: 0.4190, Train acc: 80.4233%, Val loss: 0.3498, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 80/100, Train loss: 0.3980, Train acc: 82.5397%, Val loss: 0.3956, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 81/100, Train loss: 0.4075, Train acc: 82.0106%, Val loss: 0.3694, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 82/100, Train loss: 0.4134, Train acc: 81.4815%, Val loss: 0.3471, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 83/100, Train loss: 0.3982, Train acc: 83.5979%, Val loss: 0.4047, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 84/100, Train loss: 0.4066, Train acc: 82.0106%, Val loss: 0.3751, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 85/100, Train loss: 0.3974, Train acc: 82.0106%, Val loss: 0.3496, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 86/100, Train loss: 0.3995, Train acc: 81.4815%, Val loss: 0.3548, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 87/100, Train loss: 0.4005, Train acc: 83.0688%, Val loss: 0.3836, Val acc: 77.5000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 88/100, Train loss: 0.3994, Train acc: 82.5397%, Val loss: 0.3641, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 89/100, Train loss: 0.3982, Train acc: 82.0106%, Val loss: 0.3547, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 90/100, Train loss: 0.4009, Train acc: 81.4815%, Val loss: 0.3506, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 91/100, Train loss: 0.4016, Train acc: 82.5397%, Val loss: 0.3698, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 92/100, Train loss: 0.3984, Train acc: 83.0688%, Val loss: 0.3686, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 93/100, Train loss: 0.3977, Train acc: 82.5397%, Val loss: 0.3595, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 94/100, Train loss: 0.3985, Train acc: 83.0688%, Val loss: 0.3600, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 95/100, Train loss: 0.3979, Train acc: 83.0688%, Val loss: 0.3595, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 96/100, Train loss: 0.3977, Train acc: 82.5397%, Val loss: 0.3622, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 97/100, Train loss: 0.3976, Train acc: 82.5397%, Val loss: 0.3618, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 98/100, Train loss: 0.3969, Train acc: 82.5397%, Val loss: 0.3615, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 99/100, Train loss: 0.3965, Train acc: 82.5397%, Val loss: 0.3615, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "Epoch 100/100, Train loss: 0.3975, Train acc: 82.5397%, Val loss: 0.3615, Val acc: 80.0000%, Best Val loss: 0.3249 Best Val acc: 82.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 1.7747, Train acc: 50.7937%, Val loss: 0.7140, Val acc: 52.5000%, Best Val loss: 0.7140 Best Val acc: 52.50%\n",
            "Epoch 2/100, Train loss: 0.8695, Train acc: 49.7354%, Val loss: 0.5344, Val acc: 60.0000%, Best Val loss: 0.5344 Best Val acc: 60.00%\n",
            "Epoch 3/100, Train loss: 0.7966, Train acc: 57.6720%, Val loss: 0.8672, Val acc: 50.0000%, Best Val loss: 0.5344 Best Val acc: 60.00%\n",
            "Epoch 4/100, Train loss: 0.6760, Train acc: 65.6085%, Val loss: 0.4422, Val acc: 60.0000%, Best Val loss: 0.4422 Best Val acc: 60.00%\n",
            "Epoch 5/100, Train loss: 0.6603, Train acc: 64.0212%, Val loss: 0.5635, Val acc: 70.0000%, Best Val loss: 0.4422 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 0.5759, Train acc: 67.1958%, Val loss: 0.4285, Val acc: 75.0000%, Best Val loss: 0.4285 Best Val acc: 75.00%\n",
            "Epoch 7/100, Train loss: 0.5942, Train acc: 69.3122%, Val loss: 0.4595, Val acc: 77.5000%, Best Val loss: 0.4285 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.5663, Train acc: 68.2540%, Val loss: 0.5043, Val acc: 80.0000%, Best Val loss: 0.4285 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.5596, Train acc: 70.3704%, Val loss: 0.4567, Val acc: 75.0000%, Best Val loss: 0.4285 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.5620, Train acc: 72.4868%, Val loss: 0.4653, Val acc: 77.5000%, Best Val loss: 0.4285 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.5899, Train acc: 67.1958%, Val loss: 0.6155, Val acc: 65.0000%, Best Val loss: 0.4285 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6755, Train acc: 60.8466%, Val loss: 0.3886, Val acc: 75.0000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.5694, Train acc: 69.3122%, Val loss: 0.5596, Val acc: 75.0000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.5911, Train acc: 69.8413%, Val loss: 0.4596, Val acc: 77.5000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.5800, Train acc: 68.2540%, Val loss: 0.3993, Val acc: 77.5000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5476, Train acc: 70.8995%, Val loss: 0.5169, Val acc: 80.0000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5503, Train acc: 71.9577%, Val loss: 0.4547, Val acc: 75.0000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5376, Train acc: 73.0159%, Val loss: 0.4013, Val acc: 77.5000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5581, Train acc: 71.4286%, Val loss: 0.7379, Val acc: 60.0000%, Best Val loss: 0.3886 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5792, Train acc: 67.7249%, Val loss: 0.3667, Val acc: 77.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5648, Train acc: 68.7831%, Val loss: 0.4370, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.5220, Train acc: 73.0159%, Val loss: 0.4151, Val acc: 72.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5281, Train acc: 73.0159%, Val loss: 0.3908, Val acc: 77.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.5656, Train acc: 68.7831%, Val loss: 0.5482, Val acc: 67.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5405, Train acc: 70.3704%, Val loss: 0.4001, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5160, Train acc: 74.0741%, Val loss: 0.4292, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.5099, Train acc: 74.0741%, Val loss: 0.3932, Val acc: 77.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.5090, Train acc: 74.0741%, Val loss: 0.4208, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.5077, Train acc: 73.5450%, Val loss: 0.3798, Val acc: 77.5000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5109, Train acc: 71.9577%, Val loss: 0.4151, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5081, Train acc: 73.0159%, Val loss: 0.4160, Val acc: 75.0000%, Best Val loss: 0.3667 Best Val acc: 80.00%\n",
            "Epoch 32/100, Train loss: 0.5170, Train acc: 73.0159%, Val loss: 0.3501, Val acc: 75.0000%, Best Val loss: 0.3501 Best Val acc: 80.00%\n",
            "Epoch 33/100, Train loss: 0.5121, Train acc: 73.0159%, Val loss: 0.4911, Val acc: 77.5000%, Best Val loss: 0.3501 Best Val acc: 80.00%\n",
            "Epoch 34/100, Train loss: 0.4937, Train acc: 76.1905%, Val loss: 0.3471, Val acc: 75.0000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 35/100, Train loss: 0.5110, Train acc: 73.0159%, Val loss: 0.4072, Val acc: 77.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 36/100, Train loss: 0.4923, Train acc: 73.5450%, Val loss: 0.4125, Val acc: 77.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 37/100, Train loss: 0.4885, Train acc: 75.6614%, Val loss: 0.3579, Val acc: 77.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 38/100, Train loss: 0.5520, Train acc: 70.8995%, Val loss: 0.4323, Val acc: 80.0000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 39/100, Train loss: 0.5166, Train acc: 75.1323%, Val loss: 0.4210, Val acc: 77.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 40/100, Train loss: 0.4949, Train acc: 75.6614%, Val loss: 0.3531, Val acc: 77.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 41/100, Train loss: 0.4891, Train acc: 73.5450%, Val loss: 0.5148, Val acc: 67.5000%, Best Val loss: 0.3471 Best Val acc: 80.00%\n",
            "Epoch 42/100, Train loss: 0.5265, Train acc: 72.4868%, Val loss: 0.3416, Val acc: 75.0000%, Best Val loss: 0.3416 Best Val acc: 80.00%\n",
            "Epoch 43/100, Train loss: 0.5529, Train acc: 68.2540%, Val loss: 0.3507, Val acc: 77.5000%, Best Val loss: 0.3416 Best Val acc: 80.00%\n",
            "Epoch 44/100, Train loss: 0.5669, Train acc: 69.8413%, Val loss: 0.5602, Val acc: 67.5000%, Best Val loss: 0.3416 Best Val acc: 80.00%\n",
            "Epoch 45/100, Train loss: 0.5369, Train acc: 73.0159%, Val loss: 0.3363, Val acc: 75.0000%, Best Val loss: 0.3363 Best Val acc: 80.00%\n",
            "Epoch 46/100, Train loss: 0.4718, Train acc: 77.2487%, Val loss: 0.4622, Val acc: 80.0000%, Best Val loss: 0.3363 Best Val acc: 80.00%\n",
            "Epoch 47/100, Train loss: 0.5025, Train acc: 74.0741%, Val loss: 0.3233, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 80.00%\n",
            "Epoch 48/100, Train loss: 0.4979, Train acc: 75.6614%, Val loss: 0.4222, Val acc: 82.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 49/100, Train loss: 0.4732, Train acc: 75.6614%, Val loss: 0.3401, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 50/100, Train loss: 0.4640, Train acc: 76.1905%, Val loss: 0.4137, Val acc: 82.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 51/100, Train loss: 0.4782, Train acc: 74.0741%, Val loss: 0.3851, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 52/100, Train loss: 0.4660, Train acc: 78.8360%, Val loss: 0.3534, Val acc: 80.0000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 53/100, Train loss: 0.4702, Train acc: 77.2487%, Val loss: 0.3946, Val acc: 82.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 54/100, Train loss: 0.4625, Train acc: 77.7778%, Val loss: 0.3782, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 55/100, Train loss: 0.4577, Train acc: 79.3651%, Val loss: 0.3571, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 56/100, Train loss: 0.4592, Train acc: 77.7778%, Val loss: 0.3678, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 57/100, Train loss: 0.4569, Train acc: 78.3069%, Val loss: 0.3509, Val acc: 80.0000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 58/100, Train loss: 0.4608, Train acc: 77.2487%, Val loss: 0.3570, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 59/100, Train loss: 0.4714, Train acc: 78.3069%, Val loss: 0.3642, Val acc: 80.0000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 60/100, Train loss: 0.4731, Train acc: 78.3069%, Val loss: 0.3570, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 61/100, Train loss: 0.4583, Train acc: 77.2487%, Val loss: 0.3481, Val acc: 77.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 62/100, Train loss: 0.4630, Train acc: 76.7196%, Val loss: 0.3672, Val acc: 80.0000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 63/100, Train loss: 0.4683, Train acc: 77.7778%, Val loss: 0.3243, Val acc: 80.0000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 64/100, Train loss: 0.4567, Train acc: 76.7196%, Val loss: 0.4011, Val acc: 82.5000%, Best Val loss: 0.3233 Best Val acc: 82.50%\n",
            "Epoch 65/100, Train loss: 0.4602, Train acc: 77.7778%, Val loss: 0.3197, Val acc: 77.5000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 66/100, Train loss: 0.4542, Train acc: 76.1905%, Val loss: 0.3922, Val acc: 82.5000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 67/100, Train loss: 0.4468, Train acc: 78.8360%, Val loss: 0.3236, Val acc: 80.0000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 68/100, Train loss: 0.4500, Train acc: 78.8360%, Val loss: 0.3613, Val acc: 80.0000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 69/100, Train loss: 0.4433, Train acc: 77.7778%, Val loss: 0.3327, Val acc: 82.5000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 70/100, Train loss: 0.4441, Train acc: 78.8360%, Val loss: 0.3493, Val acc: 82.5000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 71/100, Train loss: 0.4474, Train acc: 78.3069%, Val loss: 0.3654, Val acc: 80.0000%, Best Val loss: 0.3197 Best Val acc: 82.50%\n",
            "Epoch 72/100, Train loss: 0.4578, Train acc: 76.1905%, Val loss: 0.3166, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 73/100, Train loss: 0.4360, Train acc: 79.3651%, Val loss: 0.3812, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 74/100, Train loss: 0.4498, Train acc: 78.3069%, Val loss: 0.3761, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 75/100, Train loss: 0.4443, Train acc: 79.8942%, Val loss: 0.3171, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 76/100, Train loss: 0.4416, Train acc: 80.4233%, Val loss: 0.3520, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 77/100, Train loss: 0.4400, Train acc: 78.3069%, Val loss: 0.3700, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 78/100, Train loss: 0.4400, Train acc: 77.2487%, Val loss: 0.3290, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 79/100, Train loss: 0.4380, Train acc: 79.8942%, Val loss: 0.3401, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 80/100, Train loss: 0.4356, Train acc: 79.3651%, Val loss: 0.3673, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 81/100, Train loss: 0.4366, Train acc: 78.8360%, Val loss: 0.3497, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 82/100, Train loss: 0.4320, Train acc: 79.3651%, Val loss: 0.3287, Val acc: 80.0000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 83/100, Train loss: 0.4341, Train acc: 79.8942%, Val loss: 0.3348, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 84/100, Train loss: 0.4374, Train acc: 79.8942%, Val loss: 0.3459, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 85/100, Train loss: 0.4351, Train acc: 79.3651%, Val loss: 0.3468, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 86/100, Train loss: 0.4325, Train acc: 79.8942%, Val loss: 0.3416, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 87/100, Train loss: 0.4350, Train acc: 79.3651%, Val loss: 0.3518, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 88/100, Train loss: 0.4369, Train acc: 80.4233%, Val loss: 0.3380, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 89/100, Train loss: 0.4328, Train acc: 80.9524%, Val loss: 0.3377, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 90/100, Train loss: 0.4305, Train acc: 80.9524%, Val loss: 0.3435, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 91/100, Train loss: 0.4321, Train acc: 80.4233%, Val loss: 0.3425, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 92/100, Train loss: 0.4339, Train acc: 79.3651%, Val loss: 0.3477, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 93/100, Train loss: 0.4308, Train acc: 79.3651%, Val loss: 0.3454, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 94/100, Train loss: 0.4327, Train acc: 80.4233%, Val loss: 0.3423, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 95/100, Train loss: 0.4316, Train acc: 80.4233%, Val loss: 0.3426, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 96/100, Train loss: 0.4319, Train acc: 80.4233%, Val loss: 0.3439, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 97/100, Train loss: 0.4299, Train acc: 80.4233%, Val loss: 0.3434, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 98/100, Train loss: 0.4326, Train acc: 80.4233%, Val loss: 0.3433, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 99/100, Train loss: 0.4306, Train acc: 80.4233%, Val loss: 0.3433, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "Epoch 100/100, Train loss: 0.4333, Train acc: 80.4233%, Val loss: 0.3433, Val acc: 82.5000%, Best Val loss: 0.3166 Best Val acc: 82.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.8431, Train acc: 44.4444%, Val loss: 0.5753, Val acc: 60.0000%, Best Val loss: 0.5753 Best Val acc: 60.00%\n",
            "Epoch 2/100, Train loss: 0.9784, Train acc: 56.6138%, Val loss: 0.5346, Val acc: 80.0000%, Best Val loss: 0.5346 Best Val acc: 80.00%\n",
            "Epoch 3/100, Train loss: 0.8244, Train acc: 58.2011%, Val loss: 0.6933, Val acc: 62.5000%, Best Val loss: 0.5346 Best Val acc: 80.00%\n",
            "Epoch 4/100, Train loss: 0.6173, Train acc: 70.3704%, Val loss: 0.5845, Val acc: 72.5000%, Best Val loss: 0.5346 Best Val acc: 80.00%\n",
            "Epoch 5/100, Train loss: 0.6267, Train acc: 66.6667%, Val loss: 0.5386, Val acc: 77.5000%, Best Val loss: 0.5346 Best Val acc: 80.00%\n",
            "Epoch 6/100, Train loss: 0.6609, Train acc: 67.1958%, Val loss: 0.3857, Val acc: 75.0000%, Best Val loss: 0.3857 Best Val acc: 80.00%\n",
            "Epoch 7/100, Train loss: 0.6507, Train acc: 65.0794%, Val loss: 0.3723, Val acc: 70.0000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 8/100, Train loss: 0.5498, Train acc: 70.8995%, Val loss: 0.6220, Val acc: 65.0000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 9/100, Train loss: 0.5723, Train acc: 71.9577%, Val loss: 0.6507, Val acc: 62.5000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 10/100, Train loss: 0.5782, Train acc: 70.3704%, Val loss: 0.5422, Val acc: 77.5000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.6219, Train acc: 66.1376%, Val loss: 0.3886, Val acc: 72.5000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6958, Train acc: 64.0212%, Val loss: 0.3808, Val acc: 67.5000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.7121, Train acc: 66.6667%, Val loss: 0.6800, Val acc: 62.5000%, Best Val loss: 0.3723 Best Val acc: 80.00%\n",
            "Epoch 14/100, Train loss: 0.8283, Train acc: 60.3175%, Val loss: 0.3585, Val acc: 75.0000%, Best Val loss: 0.3585 Best Val acc: 80.00%\n",
            "Epoch 15/100, Train loss: 0.6257, Train acc: 70.3704%, Val loss: 0.3553, Val acc: 75.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 16/100, Train loss: 0.5718, Train acc: 70.3704%, Val loss: 0.4289, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 17/100, Train loss: 0.5733, Train acc: 70.3704%, Val loss: 0.4868, Val acc: 80.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 18/100, Train loss: 0.5304, Train acc: 68.7831%, Val loss: 0.4563, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 19/100, Train loss: 0.5667, Train acc: 71.9577%, Val loss: 0.4530, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 20/100, Train loss: 0.5649, Train acc: 68.2540%, Val loss: 0.4489, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 21/100, Train loss: 0.5702, Train acc: 68.7831%, Val loss: 0.4906, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 22/100, Train loss: 0.5425, Train acc: 71.4286%, Val loss: 0.4020, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 23/100, Train loss: 0.5036, Train acc: 71.4286%, Val loss: 0.3793, Val acc: 70.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 24/100, Train loss: 0.6252, Train acc: 67.7249%, Val loss: 0.3728, Val acc: 75.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 25/100, Train loss: 0.5158, Train acc: 73.5450%, Val loss: 0.4602, Val acc: 80.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 26/100, Train loss: 0.5061, Train acc: 75.6614%, Val loss: 0.3740, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 27/100, Train loss: 0.4952, Train acc: 74.0741%, Val loss: 0.4004, Val acc: 77.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 28/100, Train loss: 0.4981, Train acc: 73.5450%, Val loss: 0.3914, Val acc: 75.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 29/100, Train loss: 0.5833, Train acc: 67.7249%, Val loss: 0.3761, Val acc: 75.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 30/100, Train loss: 0.5556, Train acc: 69.8413%, Val loss: 0.3600, Val acc: 72.5000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 31/100, Train loss: 0.5258, Train acc: 71.4286%, Val loss: 0.4151, Val acc: 70.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 32/100, Train loss: 0.6663, Train acc: 66.1376%, Val loss: 0.4257, Val acc: 70.0000%, Best Val loss: 0.3553 Best Val acc: 80.00%\n",
            "Epoch 33/100, Train loss: 0.7516, Train acc: 63.4921%, Val loss: 0.3454, Val acc: 77.5000%, Best Val loss: 0.3454 Best Val acc: 80.00%\n",
            "Epoch 34/100, Train loss: 0.6565, Train acc: 65.6085%, Val loss: 1.3855, Val acc: 45.0000%, Best Val loss: 0.3454 Best Val acc: 80.00%\n",
            "Epoch 35/100, Train loss: 0.6995, Train acc: 71.4286%, Val loss: 0.3193, Val acc: 77.5000%, Best Val loss: 0.3193 Best Val acc: 80.00%\n",
            "Epoch 36/100, Train loss: 0.5284, Train acc: 76.1905%, Val loss: 0.3434, Val acc: 75.0000%, Best Val loss: 0.3193 Best Val acc: 80.00%\n",
            "Epoch 37/100, Train loss: 0.6300, Train acc: 65.6085%, Val loss: 0.3511, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.5306, Train acc: 75.6614%, Val loss: 0.3617, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5353, Train acc: 73.5450%, Val loss: 0.6872, Val acc: 55.0000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5340, Train acc: 71.9577%, Val loss: 0.5604, Val acc: 67.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.4868, Train acc: 74.0741%, Val loss: 0.4784, Val acc: 77.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.4812, Train acc: 74.0741%, Val loss: 0.4313, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.5018, Train acc: 71.9577%, Val loss: 0.3634, Val acc: 75.0000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 44/100, Train loss: 0.4932, Train acc: 72.4868%, Val loss: 0.3461, Val acc: 80.0000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 45/100, Train loss: 0.4775, Train acc: 76.1905%, Val loss: 0.5292, Val acc: 67.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 46/100, Train loss: 0.4736, Train acc: 72.4868%, Val loss: 0.4464, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 82.50%\n",
            "Epoch 47/100, Train loss: 0.4600, Train acc: 78.8360%, Val loss: 0.3935, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 85.00%\n",
            "Epoch 48/100, Train loss: 0.4405, Train acc: 78.8360%, Val loss: 0.3666, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 85.00%\n",
            "Epoch 49/100, Train loss: 0.4510, Train acc: 78.3069%, Val loss: 0.3612, Val acc: 82.5000%, Best Val loss: 0.3193 Best Val acc: 85.00%\n",
            "Epoch 50/100, Train loss: 0.4437, Train acc: 78.8360%, Val loss: 0.3749, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 85.00%\n",
            "Epoch 51/100, Train loss: 0.4446, Train acc: 78.8360%, Val loss: 0.3808, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 52/100, Train loss: 0.4334, Train acc: 79.3651%, Val loss: 0.3433, Val acc: 77.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 53/100, Train loss: 0.4291, Train acc: 80.9524%, Val loss: 0.4100, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 54/100, Train loss: 0.4220, Train acc: 83.0688%, Val loss: 0.3551, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 55/100, Train loss: 0.4245, Train acc: 81.4815%, Val loss: 0.4447, Val acc: 80.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 56/100, Train loss: 0.4426, Train acc: 79.3651%, Val loss: 0.3396, Val acc: 80.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 57/100, Train loss: 0.4494, Train acc: 79.3651%, Val loss: 0.5166, Val acc: 67.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 58/100, Train loss: 0.4112, Train acc: 81.4815%, Val loss: 0.3300, Val acc: 80.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 59/100, Train loss: 0.4516, Train acc: 80.4233%, Val loss: 0.3576, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 60/100, Train loss: 0.4273, Train acc: 79.8942%, Val loss: 0.4606, Val acc: 75.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 61/100, Train loss: 0.4257, Train acc: 79.3651%, Val loss: 0.3441, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 62/100, Train loss: 0.4426, Train acc: 77.7778%, Val loss: 0.3265, Val acc: 80.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 63/100, Train loss: 0.4532, Train acc: 74.6032%, Val loss: 0.3625, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 64/100, Train loss: 0.4201, Train acc: 81.4815%, Val loss: 0.3801, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 65/100, Train loss: 0.4104, Train acc: 82.5397%, Val loss: 0.3556, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 66/100, Train loss: 0.4132, Train acc: 82.0106%, Val loss: 0.4027, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 67/100, Train loss: 0.4186, Train acc: 80.4233%, Val loss: 0.3383, Val acc: 85.0000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 68/100, Train loss: 0.4091, Train acc: 80.4233%, Val loss: 0.3886, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 69/100, Train loss: 0.4043, Train acc: 82.5397%, Val loss: 0.3477, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4022, Train acc: 83.5979%, Val loss: 0.4051, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4111, Train acc: 81.4815%, Val loss: 0.3561, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4042, Train acc: 83.5979%, Val loss: 0.3917, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4129, Train acc: 82.0106%, Val loss: 0.3440, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4043, Train acc: 81.4815%, Val loss: 0.3922, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4145, Train acc: 81.4815%, Val loss: 0.3555, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4063, Train acc: 81.4815%, Val loss: 0.3807, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4019, Train acc: 83.5979%, Val loss: 0.3340, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.3976, Train acc: 83.0688%, Val loss: 0.3887, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4000, Train acc: 83.0688%, Val loss: 0.3514, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4007, Train acc: 84.6561%, Val loss: 0.3614, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.3967, Train acc: 83.5979%, Val loss: 0.3486, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.3984, Train acc: 82.0106%, Val loss: 0.3614, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.3991, Train acc: 84.1270%, Val loss: 0.3882, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.3953, Train acc: 82.5397%, Val loss: 0.3477, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.3956, Train acc: 83.0688%, Val loss: 0.3471, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.3954, Train acc: 83.5979%, Val loss: 0.3577, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.3943, Train acc: 84.6561%, Val loss: 0.3746, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.3956, Train acc: 85.1852%, Val loss: 0.3576, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.3944, Train acc: 84.1270%, Val loss: 0.3547, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.3938, Train acc: 83.5979%, Val loss: 0.3576, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.3924, Train acc: 83.5979%, Val loss: 0.3606, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.3962, Train acc: 83.0688%, Val loss: 0.3572, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.3934, Train acc: 83.5979%, Val loss: 0.3600, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.3960, Train acc: 83.0688%, Val loss: 0.3636, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.3937, Train acc: 83.0688%, Val loss: 0.3582, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.3927, Train acc: 83.5979%, Val loss: 0.3577, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.3954, Train acc: 83.5979%, Val loss: 0.3571, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.3931, Train acc: 83.5979%, Val loss: 0.3572, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.3939, Train acc: 83.5979%, Val loss: 0.3572, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.3941, Train acc: 83.5979%, Val loss: 0.3572, Val acc: 87.5000%, Best Val loss: 0.3193 Best Val acc: 87.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 5.4866, Train acc: 53.9683%, Val loss: 3.8867, Val acc: 37.5000%, Best Val loss: 3.8867 Best Val acc: 37.50%\n",
            "Epoch 2/100, Train loss: 3.6277, Train acc: 49.2063%, Val loss: 1.0463, Val acc: 62.5000%, Best Val loss: 1.0463 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 1.9364, Train acc: 55.0265%, Val loss: 1.0260, Val acc: 65.0000%, Best Val loss: 1.0260 Best Val acc: 65.00%\n",
            "Epoch 4/100, Train loss: 1.8922, Train acc: 55.0265%, Val loss: 0.3970, Val acc: 77.5000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 5/100, Train loss: 1.5011, Train acc: 61.3757%, Val loss: 2.4265, Val acc: 42.5000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 6/100, Train loss: 1.0916, Train acc: 66.1376%, Val loss: 1.1555, Val acc: 60.0000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 7/100, Train loss: 0.9184, Train acc: 65.6085%, Val loss: 0.9397, Val acc: 65.0000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 8/100, Train loss: 0.9702, Train acc: 61.3757%, Val loss: 0.4331, Val acc: 77.5000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 9/100, Train loss: 0.6965, Train acc: 66.6667%, Val loss: 0.4384, Val acc: 77.5000%, Best Val loss: 0.3970 Best Val acc: 77.50%\n",
            "Epoch 10/100, Train loss: 0.6976, Train acc: 65.6085%, Val loss: 0.5404, Val acc: 80.0000%, Best Val loss: 0.3970 Best Val acc: 80.00%\n",
            "Epoch 11/100, Train loss: 0.6546, Train acc: 69.8413%, Val loss: 0.4007, Val acc: 80.0000%, Best Val loss: 0.3970 Best Val acc: 80.00%\n",
            "Epoch 12/100, Train loss: 0.6567, Train acc: 67.1958%, Val loss: 0.3485, Val acc: 75.0000%, Best Val loss: 0.3485 Best Val acc: 80.00%\n",
            "Epoch 13/100, Train loss: 0.7878, Train acc: 64.5503%, Val loss: 0.4154, Val acc: 82.5000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 14/100, Train loss: 0.6462, Train acc: 66.6667%, Val loss: 0.6277, Val acc: 65.0000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 15/100, Train loss: 0.5651, Train acc: 73.5450%, Val loss: 0.4275, Val acc: 75.0000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 16/100, Train loss: 0.6187, Train acc: 64.5503%, Val loss: 0.3972, Val acc: 75.0000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 17/100, Train loss: 0.6207, Train acc: 66.6667%, Val loss: 0.4220, Val acc: 75.0000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 18/100, Train loss: 0.6797, Train acc: 67.7249%, Val loss: 0.5648, Val acc: 65.0000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 19/100, Train loss: 0.8377, Train acc: 63.4921%, Val loss: 0.4298, Val acc: 72.5000%, Best Val loss: 0.3485 Best Val acc: 82.50%\n",
            "Epoch 20/100, Train loss: 0.7850, Train acc: 59.7884%, Val loss: 0.3452, Val acc: 77.5000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 21/100, Train loss: 0.6645, Train acc: 67.1958%, Val loss: 0.5169, Val acc: 82.5000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 22/100, Train loss: 0.6363, Train acc: 67.1958%, Val loss: 0.8484, Val acc: 50.0000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 23/100, Train loss: 0.8017, Train acc: 60.8466%, Val loss: 0.4170, Val acc: 77.5000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 24/100, Train loss: 0.8541, Train acc: 62.4339%, Val loss: 0.3729, Val acc: 77.5000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 25/100, Train loss: 0.8049, Train acc: 65.6085%, Val loss: 0.3907, Val acc: 75.0000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 26/100, Train loss: 0.6019, Train acc: 69.3122%, Val loss: 0.3500, Val acc: 75.0000%, Best Val loss: 0.3452 Best Val acc: 82.50%\n",
            "Epoch 27/100, Train loss: 0.7347, Train acc: 66.6667%, Val loss: 0.3326, Val acc: 77.5000%, Best Val loss: 0.3326 Best Val acc: 82.50%\n",
            "Epoch 28/100, Train loss: 0.6571, Train acc: 68.7831%, Val loss: 0.3336, Val acc: 75.0000%, Best Val loss: 0.3326 Best Val acc: 82.50%\n",
            "Epoch 29/100, Train loss: 0.7063, Train acc: 67.1958%, Val loss: 0.3319, Val acc: 77.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 30/100, Train loss: 0.5848, Train acc: 68.2540%, Val loss: 0.3548, Val acc: 75.0000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 31/100, Train loss: 0.6103, Train acc: 70.3704%, Val loss: 0.3357, Val acc: 75.0000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 32/100, Train loss: 0.5231, Train acc: 71.9577%, Val loss: 0.4820, Val acc: 82.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 33/100, Train loss: 0.7061, Train acc: 66.6667%, Val loss: 0.4515, Val acc: 82.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 34/100, Train loss: 0.5147, Train acc: 72.4868%, Val loss: 0.7663, Val acc: 47.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 35/100, Train loss: 0.6442, Train acc: 68.2540%, Val loss: 0.4396, Val acc: 82.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 36/100, Train loss: 0.5648, Train acc: 69.8413%, Val loss: 0.4495, Val acc: 82.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 37/100, Train loss: 0.5949, Train acc: 71.9577%, Val loss: 0.3557, Val acc: 77.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 38/100, Train loss: 0.6540, Train acc: 70.3704%, Val loss: 0.6933, Val acc: 57.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 39/100, Train loss: 0.5647, Train acc: 69.3122%, Val loss: 0.6272, Val acc: 60.0000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 40/100, Train loss: 0.5926, Train acc: 69.3122%, Val loss: 0.9706, Val acc: 45.0000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 41/100, Train loss: 0.5721, Train acc: 71.4286%, Val loss: 0.8386, Val acc: 47.5000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 42/100, Train loss: 0.5890, Train acc: 70.8995%, Val loss: 0.5877, Val acc: 70.0000%, Best Val loss: 0.3319 Best Val acc: 82.50%\n",
            "Epoch 43/100, Train loss: 0.5749, Train acc: 69.3122%, Val loss: 0.3835, Val acc: 85.0000%, Best Val loss: 0.3319 Best Val acc: 85.00%\n",
            "Epoch 44/100, Train loss: 0.5358, Train acc: 70.8995%, Val loss: 0.3171, Val acc: 80.0000%, Best Val loss: 0.3171 Best Val acc: 85.00%\n",
            "Epoch 45/100, Train loss: 0.4763, Train acc: 74.0741%, Val loss: 0.3943, Val acc: 72.5000%, Best Val loss: 0.3171 Best Val acc: 85.00%\n",
            "Epoch 46/100, Train loss: 0.6070, Train acc: 68.7831%, Val loss: 0.3161, Val acc: 80.0000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 47/100, Train loss: 0.6033, Train acc: 69.3122%, Val loss: 0.9925, Val acc: 45.0000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 48/100, Train loss: 0.5267, Train acc: 75.1323%, Val loss: 0.3220, Val acc: 77.5000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 49/100, Train loss: 0.4680, Train acc: 77.2487%, Val loss: 0.3368, Val acc: 82.5000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 50/100, Train loss: 0.5016, Train acc: 75.1323%, Val loss: 0.4050, Val acc: 85.0000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 51/100, Train loss: 0.4641, Train acc: 75.1323%, Val loss: 0.4487, Val acc: 82.5000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 52/100, Train loss: 0.4718, Train acc: 75.6614%, Val loss: 0.7038, Val acc: 52.5000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 53/100, Train loss: 0.4848, Train acc: 74.6032%, Val loss: 0.3185, Val acc: 80.0000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 54/100, Train loss: 0.5018, Train acc: 71.9577%, Val loss: 0.3283, Val acc: 80.0000%, Best Val loss: 0.3161 Best Val acc: 85.00%\n",
            "Epoch 55/100, Train loss: 0.5211, Train acc: 71.9577%, Val loss: 0.3067, Val acc: 80.0000%, Best Val loss: 0.3067 Best Val acc: 85.00%\n",
            "Epoch 56/100, Train loss: 0.5481, Train acc: 70.8995%, Val loss: 0.6752, Val acc: 57.5000%, Best Val loss: 0.3067 Best Val acc: 85.00%\n",
            "Epoch 57/100, Train loss: 0.4810, Train acc: 75.1323%, Val loss: 0.3424, Val acc: 85.0000%, Best Val loss: 0.3067 Best Val acc: 85.00%\n",
            "Epoch 58/100, Train loss: 0.4508, Train acc: 78.8360%, Val loss: 0.3060, Val acc: 80.0000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 59/100, Train loss: 0.4823, Train acc: 77.2487%, Val loss: 0.4662, Val acc: 75.0000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 60/100, Train loss: 0.4448, Train acc: 77.7778%, Val loss: 0.3508, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 61/100, Train loss: 0.4382, Train acc: 77.7778%, Val loss: 0.3694, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 62/100, Train loss: 0.4498, Train acc: 78.8360%, Val loss: 0.5561, Val acc: 67.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 63/100, Train loss: 0.4759, Train acc: 78.3069%, Val loss: 0.3144, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 64/100, Train loss: 0.4619, Train acc: 74.6032%, Val loss: 0.3062, Val acc: 80.0000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 65/100, Train loss: 0.4560, Train acc: 77.2487%, Val loss: 0.3504, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 66/100, Train loss: 0.4217, Train acc: 80.4233%, Val loss: 0.3566, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 67/100, Train loss: 0.4239, Train acc: 80.9524%, Val loss: 0.3734, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 68/100, Train loss: 0.4197, Train acc: 84.1270%, Val loss: 0.3125, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 85.00%\n",
            "Epoch 69/100, Train loss: 0.4534, Train acc: 74.0741%, Val loss: 0.3213, Val acc: 87.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 70/100, Train loss: 0.4464, Train acc: 79.8942%, Val loss: 0.3365, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 71/100, Train loss: 0.4232, Train acc: 79.8942%, Val loss: 0.3610, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 72/100, Train loss: 0.4207, Train acc: 80.4233%, Val loss: 0.3190, Val acc: 87.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 73/100, Train loss: 0.4485, Train acc: 77.2487%, Val loss: 0.3992, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 74/100, Train loss: 0.4204, Train acc: 79.8942%, Val loss: 0.3321, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 75/100, Train loss: 0.4194, Train acc: 79.8942%, Val loss: 0.3482, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 76/100, Train loss: 0.4131, Train acc: 82.0106%, Val loss: 0.3210, Val acc: 87.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 77/100, Train loss: 0.4157, Train acc: 80.4233%, Val loss: 0.3459, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 78/100, Train loss: 0.4129, Train acc: 80.9524%, Val loss: 0.3636, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 79/100, Train loss: 0.4115, Train acc: 83.0688%, Val loss: 0.3503, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 80/100, Train loss: 0.4108, Train acc: 82.5397%, Val loss: 0.3349, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 81/100, Train loss: 0.4098, Train acc: 83.5979%, Val loss: 0.3489, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 82/100, Train loss: 0.4123, Train acc: 83.0688%, Val loss: 0.3333, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 83/100, Train loss: 0.4083, Train acc: 83.0688%, Val loss: 0.3654, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 84/100, Train loss: 0.4119, Train acc: 84.1270%, Val loss: 0.3459, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 85/100, Train loss: 0.4100, Train acc: 85.1852%, Val loss: 0.3423, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 86/100, Train loss: 0.4070, Train acc: 82.0106%, Val loss: 0.3439, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 87/100, Train loss: 0.4088, Train acc: 82.0106%, Val loss: 0.3314, Val acc: 87.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 88/100, Train loss: 0.4106, Train acc: 82.0106%, Val loss: 0.3615, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 89/100, Train loss: 0.4073, Train acc: 83.5979%, Val loss: 0.3433, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 90/100, Train loss: 0.4118, Train acc: 81.4815%, Val loss: 0.3255, Val acc: 87.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 91/100, Train loss: 0.4086, Train acc: 82.0106%, Val loss: 0.3374, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 92/100, Train loss: 0.4066, Train acc: 83.0688%, Val loss: 0.3603, Val acc: 85.0000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 93/100, Train loss: 0.4084, Train acc: 84.6561%, Val loss: 0.3481, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 94/100, Train loss: 0.4052, Train acc: 84.6561%, Val loss: 0.3472, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 95/100, Train loss: 0.4050, Train acc: 84.6561%, Val loss: 0.3430, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 96/100, Train loss: 0.4065, Train acc: 82.5397%, Val loss: 0.3423, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 97/100, Train loss: 0.4065, Train acc: 82.5397%, Val loss: 0.3426, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 98/100, Train loss: 0.4042, Train acc: 82.5397%, Val loss: 0.3420, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 99/100, Train loss: 0.4063, Train acc: 82.5397%, Val loss: 0.3419, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n",
            "Epoch 100/100, Train loss: 0.4068, Train acc: 82.5397%, Val loss: 0.3419, Val acc: 82.5000%, Best Val loss: 0.3060 Best Val acc: 87.50%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hyperparameters to try\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "hidden_units_list = [64, 128, 256]\n",
        "results = []\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        print(f\"\\nTraining with LR={lr}, Hidden Units={hidden_units}\")\n",
        "\n",
        "        model = Model(hidden_units).cuda()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_acc = -1\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs): # Fixed: Corrected the indentation of this line\n",
        "\n",
        "            # Training\n",
        "            model.train() # Fixed: Set model to training mode\n",
        "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "            for features, labels in train_loader: # Fixed: Iterating over train_loader for training\n",
        "                features, labels = features.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad() # Fixed: Reset gradients\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()  # Fixed: Calculate gradients\n",
        "                optimizer.step() # Fixed: Update weights\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for features, labels in val_loader:\n",
        "                    features, labels = features.cuda(), labels.cuda()\n",
        "                    outputs = model(features)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Checkpoint\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f'model_LR{lr}_HU{hidden_units}.pth')\n",
        "\n",
        "            # Learning rate update\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, '\n",
        "                  f'Train loss: {avg_train_loss:.4f}, Train acc: {train_acc:.4f}%, '\n",
        "                  f'Val loss: {avg_val_loss:.4f}, Val acc: {val_acc:.4f}%, '\n",
        "                  f'Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
        "\n",
        "            # Store performance\n",
        "            train_losses.append(avg_train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        # Save result summary after all epochs\n",
        "        results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Best Val Loss': round(best_val_loss, 4),\n",
        "            'Best Val Acc': round(best_val_acc, 2)\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7984c6e-6652-4160-b572-07d48bc93a3f",
      "metadata": {
        "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
      },
      "source": [
        "#### Visualizing the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
        "outputId": "1796907b-999c-4fb4-ba13-729bedc1b1d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "ax[0].plot(train_accuracies)\n",
        "ax[0].plot(val_accuracies)\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend(['Train', 'Val'])\n",
        "\n",
        "# Plotting training and validation loss\n",
        "ax[1].plot(train_losses)\n",
        "ax[1].plot(val_losses)\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend(['Train', 'Val'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20",
      "metadata": {
        "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
      },
      "source": [
        "## D. Evaluating Your Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "f49735d7-466f-4037-8078-172f03dffd8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f49735d7-466f-4037-8078-172f03dffd8d",
        "outputId": "52bb6789-c110-47aa-d329-083ff04f8b24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   50    0   2       120   219    0        1      158      0      1.6      1   \n",
              "1   58    0   2       120   340    0        1      172      0      0.0      2   \n",
              "2   66    0   3       150   226    0        1      114      0      2.6      0   \n",
              "3   43    1   0       150   247    0        1      171      0      1.5      2   \n",
              "4   69    0   3       140   239    0        1      151      0      1.8      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     2       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   2     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e55452ea-14aa-4cb8-a55c-ea5e1f8a8e07\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e55452ea-14aa-4cb8-a55c-ea5e1f8a8e07')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e55452ea-14aa-4cb8-a55c-ea5e1f8a8e07 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e55452ea-14aa-4cb8-a55c-ea5e1f8a8e07');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69ecf8d0-b743-474a-8998-ba708d2eee81\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69ecf8d0-b743-474a-8998-ba708d2eee81')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69ecf8d0-b743-474a-8998-ba708d2eee81 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 40,\n        \"max\": 71,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          50,\n          58,\n          61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 110,\n        \"max\": 170,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          145,\n          124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 172,\n        \"max\": 417,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          302,\n          282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 109,\n        \"max\": 179,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          137,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9768723406323231,\n        \"min\": 0.0,\n        \"max\": 3.4,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.6,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# read test file\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning for Industrial Application/HW2/heart_dataset_test.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
        "outputId": "fba71044-e090-423f-a69f-373f572f57f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
        "outputId": "77b12e96-c454-410c-b1a5-9db22ddec644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "test_data = test_data.values\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "14d4be20-f64f-421d-8971-e1e47873aef8",
      "metadata": {
        "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
        "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
        "\n",
        "# Create datasets\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
        "outputId": "a07fad27-23ff-4a5d-cc9d-4875d6f1d730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning Rate  Hidden Units  Test Accuracy (%)  Test Loss\n",
            "0         0.0100            64              64.52     0.5805\n",
            "1         0.0100           128              70.97     0.5376\n",
            "2         0.0100           256              77.42     0.4958\n",
            "3         0.0010            64              74.19     0.5378\n",
            "4         0.0010           128              74.19     0.5602\n",
            "5         0.0010           256              64.52     0.6543\n",
            "6         0.0001            64              64.52     0.6024\n",
            "7         0.0001           128              77.42     0.5820\n",
            "8         0.0001           256              70.97     0.5724\n"
          ]
        }
      ],
      "source": [
        "test_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        model = Model(hidden_units).cuda()\n",
        "        model.load_state_dict(torch.load(f'model_LR{lr}_HU{hidden_units}.pth'))\n",
        "        model.eval()\n",
        "\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, labels in test_loader:\n",
        "                features = features.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                predicted = outputs.argmax(-1)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "                test_total += labels.size(0)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "        test_results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Test Accuracy (%)': round(test_acc, 2),\n",
        "            'Test Loss': round(avg_test_loss, 4)\n",
        "        })\n",
        "\n",
        "import pandas as pd\n",
        "df_test_results = pd.DataFrame(test_results)\n",
        "print(df_test_results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}