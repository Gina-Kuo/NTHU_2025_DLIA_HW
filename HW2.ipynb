{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gina-Kuo/NTHU_2025_DLIA_HW/blob/main/lab2_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c",
      "metadata": {
        "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "###### Lab 2\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2025\n",
        "\n",
        "#### 11320IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Lab 2: Predicting Heart Disease with Deep Learning\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
      "metadata": {
        "tags": [],
        "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
        "\n",
        "Throughout this lab, you'll engage with the following key activities:\n",
        "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
        "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
        "- Evaluate the performance of the trained model to understand its accuracy.\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "1. age: Age of the patient in years\n",
        "2. sex: (Male/Female)\n",
        "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
        "4. trestbps: Resting blood pressure\n",
        "5. chol: Serum cholesterol in mg/dl\n",
        "6. fbs: Fasting blood sugar > 120 mg/dl\n",
        "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
        "8. thalach: Maximum heart rate achieved\n",
        "9. exang: Exercise induced angina\n",
        "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
        "11. slope: The slope of the peak exercise ST segment\n",
        "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
        "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
        "14. target: target have disease or not (1=yes, 0=no)\n",
        "\n",
        "### References\n",
        "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad594fc8-4989-40f3-b124-4550fe7df386",
      "metadata": {
        "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
      },
      "source": [
        "## A. Checking and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfITSFq7skol",
        "outputId": "6fef259c-267a-4b99-a608-2c7063cb2b3d"
      },
      "id": "pfITSFq7skol",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
        "outputId": "b67ad4de-2f90-4230-a73f-d51dce888cc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age     sex      cp  trestbps   chol  fbs  restecg  thalach  exang  \\\n",
              "0     41    Male  medium     105.0  198.0    0      1.0    168.0      0   \n",
              "1     65  Female     low     120.0  177.0    0      1.0    140.0      0   \n",
              "2     44  Female  medium     130.0  219.0    0      0.0    188.0      0   \n",
              "3     54  Female    high     125.0  273.0    0      0.0    152.0      0   \n",
              "4     51  Female  severe     125.0  213.0    0      0.0    125.0      1   \n",
              "..   ...     ...     ...       ...    ...  ...      ...      ...    ...   \n",
              "268   40  Female     low     110.0  167.0    0      0.0    114.0      1   \n",
              "269   60  Female     low     117.0  230.0    1      1.0    160.0      1   \n",
              "270   64  Female    high     140.0  335.0    0      1.0    158.0      0   \n",
              "271   43  Female     low     120.0  177.0    0      0.0    120.0      1   \n",
              "272   57  Female     low     150.0  276.0    0      0.0    112.0      1   \n",
              "\n",
              "     oldpeak  slope  ca  thal  target  \n",
              "0        0.0    2.0   1   2.0     1.0  \n",
              "1        0.4    2.0   0   3.0     1.0  \n",
              "2        0.0    2.0   0   2.0     1.0  \n",
              "3        0.5    0.0   1   2.0     1.0  \n",
              "4        1.4    2.0   1   2.0     1.0  \n",
              "..       ...    ...  ..   ...     ...  \n",
              "268      2.0    1.0   0   3.0     0.0  \n",
              "269      1.4    2.0   2   3.0     0.0  \n",
              "270      0.0    2.0   0   2.0     0.0  \n",
              "271      2.5    1.0   0   3.0     0.0  \n",
              "272      0.6    1.0   1   1.0     0.0  \n",
              "\n",
              "[273 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5929b919-05e1-4870-bc9d-072788f4a209\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>Male</td>\n",
              "      <td>medium</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>Female</td>\n",
              "      <td>medium</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>Female</td>\n",
              "      <td>severe</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>273 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5929b919-05e1-4870-bc9d-072788f4a209')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5929b919-05e1-4870-bc9d-072788f4a209 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5929b919-05e1-4870-bc9d-072788f4a209');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1cf0b227-0639-4f6c-99e6-1ec534e91f7c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cf0b227-0639-4f6c-99e6-1ec534e91f7c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1cf0b227-0639-4f6c-99e6-1ec534e91f7c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_625ef7e9-573e-47ff-80ee-b79cedb64ec7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_625ef7e9-573e-47ff-80ee-b79cedb64ec7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 273,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"low\",\n          \"severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.852394760861472,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.44738664891147,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5291071887540655,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.20476792843544,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.184296490376038,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6175201588975051,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6220982126950155,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4986279198706136,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning for Industrial Application/HW2/heart_dataset_train_all.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "34241797-60f0-4818-a44b-f5379948d621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34241797-60f0-4818-a44b-f5379948d621",
        "outputId": "341aa2fe-175d-472d-fb4c-c127c43a9dad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
              "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
        "outputId": "e6493422-315f-4542-dd09-c78bc9fe54e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 273 entries, 0 to 272\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       273 non-null    int64  \n",
            " 1   sex       272 non-null    object \n",
            " 2   cp        272 non-null    object \n",
            " 3   trestbps  272 non-null    float64\n",
            " 4   chol      271 non-null    float64\n",
            " 5   fbs       273 non-null    int64  \n",
            " 6   restecg   272 non-null    float64\n",
            " 7   thalach   272 non-null    float64\n",
            " 8   exang     273 non-null    int64  \n",
            " 9   oldpeak   273 non-null    float64\n",
            " 10  slope     271 non-null    float64\n",
            " 11  ca        273 non-null    int64  \n",
            " 12  thal      272 non-null    float64\n",
            " 13  target    272 non-null    float64\n",
            "dtypes: float64(8), int64(4), object(2)\n",
            "memory usage: 30.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
        "outputId": "d4513270-bdeb-40e9-c87b-32eb118b92ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         1\n",
              "cp          1\n",
              "trestbps    1\n",
              "chol        2\n",
              "fbs         0\n",
              "restecg     1\n",
              "thalach     1\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       2\n",
              "ca          0\n",
              "thal        1\n",
              "target      1\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "# checking for null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932",
      "metadata": {
        "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
        "outputId": "da54457e-93de-49e4-acca-f7cec43266b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
      "metadata": {
        "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a5423288-7875-43e4-c968-b2a2b88b6740"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age sex cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     41   0  1     105.0  198.0    0      1.0    168.0      0      0.0   \n",
              "1     65   1  0     120.0  177.0    0      1.0    140.0      0      0.4   \n",
              "2     44   1  1     130.0  219.0    0      0.0    188.0      0      0.0   \n",
              "3     54   1  2     125.0  273.0    0      0.0    152.0      0      0.5   \n",
              "4     51   1  3     125.0  213.0    0      0.0    125.0      1      1.4   \n",
              "..   ...  .. ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "268   40   1  0     110.0  167.0    0      0.0    114.0      1      2.0   \n",
              "269   60   1  0     117.0  230.0    1      1.0    160.0      1      1.4   \n",
              "270   64   1  2     140.0  335.0    0      1.0    158.0      0      0.0   \n",
              "271   43   1  0     120.0  177.0    0      0.0    120.0      1      2.5   \n",
              "272   57   1  0     150.0  276.0    0      0.0    112.0      1      0.6   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0      2.0   1   2.0     1.0  \n",
              "1      2.0   0   3.0     1.0  \n",
              "2      2.0   0   2.0     1.0  \n",
              "3      0.0   1   2.0     1.0  \n",
              "4      2.0   1   2.0     1.0  \n",
              "..     ...  ..   ...     ...  \n",
              "268    1.0   0   3.0     0.0  \n",
              "269    2.0   2   3.0     0.0  \n",
              "270    2.0   0   2.0     0.0  \n",
              "271    1.0   0   3.0     0.0  \n",
              "272    1.0   1   1.0     0.0  \n",
              "\n",
              "[270 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c16d91ab-02c5-4e2e-bd7f-56f7d00faf0c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c16d91ab-02c5-4e2e-bd7f-56f7d00faf0c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c16d91ab-02c5-4e2e-bd7f-56f7d00faf0c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c16d91ab-02c5-4e2e-bd7f-56f7d00faf0c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5fcec4e9-5ac3-427c-bae7-a8af529ce906\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fcec4e9-5ac3-427c-bae7-a8af529ce906')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5fcec4e9-5ac3-427c-bae7-a8af529ce906 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_38f1841b-bc86-4b14-9625-9463d2f81889\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_38f1841b-bc86-4b14-9625-9463d2f81889 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 270,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.90467515098084,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.5294114300184,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5293141619418645,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.21725300546228,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.188378543758624,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6181877820120636,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6238744511959269,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49894560448305486,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "# Mapping 'sex' descriptions to numbers\n",
        "sex_description = {\n",
        "    'Male': 0,\n",
        "    'Female': 1,\n",
        "}\n",
        "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
        "\n",
        "# Mapping 'cp' (chest pain) descriptions to numbers\n",
        "pain_description = {\n",
        "    'low': 0,\n",
        "    'medium': 1,\n",
        "    'high': 2,\n",
        "    'severe': 3\n",
        "}\n",
        "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "051108c6-7011-4187-9e36-bd2944a019ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "051108c6-7011-4187-9e36-bd2944a019ca",
        "outputId": "74515a0f-906e-4e2f-8620-95e5603fdcf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              age    trestbps        chol         fbs     restecg     thalach  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean    54.385185  131.525926  245.607407    0.151852    0.522222  149.807407   \n",
              "std      9.149713   17.904675   51.529411    0.359544    0.529314   23.217253   \n",
              "min     29.000000   94.000000  126.000000    0.000000    0.000000   71.000000   \n",
              "25%     47.250000  120.000000  210.250000    0.000000    0.000000  134.500000   \n",
              "50%     56.000000  130.000000  240.500000    0.000000    1.000000  152.500000   \n",
              "75%     61.000000  140.000000  274.000000    0.000000    1.000000  167.750000   \n",
              "max     77.000000  200.000000  564.000000    1.000000    2.000000  202.000000   \n",
              "\n",
              "            exang     oldpeak       slope          ca        thal      target  \n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000  \n",
              "mean     0.333333    1.024074    1.400000    0.744444    2.300000    0.544444  \n",
              "std      0.472280    1.188379    0.618188    1.037166    0.623874    0.498946  \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
              "25%      0.000000    0.000000    1.000000    0.000000    2.000000    0.000000  \n",
              "50%      0.000000    0.600000    1.000000    0.000000    2.000000    1.000000  \n",
              "75%      1.000000    1.600000    2.000000    1.000000    3.000000    1.000000  \n",
              "max      1.000000    6.200000    2.000000    4.000000    3.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3cad823-07dd-4eb3-8070-9389f6c06cdd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.385185</td>\n",
              "      <td>131.525926</td>\n",
              "      <td>245.607407</td>\n",
              "      <td>0.151852</td>\n",
              "      <td>0.522222</td>\n",
              "      <td>149.807407</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.024074</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>0.544444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.149713</td>\n",
              "      <td>17.904675</td>\n",
              "      <td>51.529411</td>\n",
              "      <td>0.359544</td>\n",
              "      <td>0.529314</td>\n",
              "      <td>23.217253</td>\n",
              "      <td>0.472280</td>\n",
              "      <td>1.188379</td>\n",
              "      <td>0.618188</td>\n",
              "      <td>1.037166</td>\n",
              "      <td>0.623874</td>\n",
              "      <td>0.498946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.250000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>210.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>134.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>152.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>167.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3cad823-07dd-4eb3-8070-9389f6c06cdd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3cad823-07dd-4eb3-8070-9389f6c06cdd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3cad823-07dd-4eb3-8070-9389f6c06cdd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-faefdddd-85d7-4a1b-b8ac-ef04b77b667c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-faefdddd-85d7-4a1b-b8ac-ef04b77b667c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-faefdddd-85d7-4a1b-b8ac-ef04b77b667c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.26807726653665,\n        \"min\": 9.149713209948986,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          54.385185185185186,\n          56.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.73772068120547,\n        \"min\": 17.90467515098084,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          131.52592592592592,\n          130.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 149.27786697513412,\n        \"min\": 51.5294114300184,\n        \"max\": 564.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          245.6074074074074,\n          240.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.38369701469601,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.15185185185185185,\n          1.0,\n          0.35954367027245654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.2064556868165,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.5222222222222223,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.47345746130112,\n        \"min\": 23.21725300546228,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          149.8074074074074,\n          152.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.31861707775643,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.47227992455486234\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.94427117355892,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          270.0,\n          1.0240740740740741,\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.05680862661214,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          1.4,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.1259484289674,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.7444444444444445,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.81255144269917,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          2.3,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.25610060161931,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5444444444444444,\n          1.0,\n          0.49894560448305486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
        "outputId": "8fb1e92f-ca70-478b-8a71-3e5ae8f8cd99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age       sex        cp  trestbps      chol       fbs  \\\n",
              "age       1.000000 -0.062222 -0.103697  0.261782  0.210520  0.109847   \n",
              "sex      -0.062222  1.000000 -0.040197 -0.055463 -0.166885  0.042384   \n",
              "cp       -0.103697 -0.040197  1.000000  0.035563 -0.063592  0.065869   \n",
              "trestbps  0.261782 -0.055463  0.035563  1.000000  0.128444  0.170606   \n",
              "chol      0.210520 -0.166885 -0.063592  0.128444  1.000000  0.003430   \n",
              "fbs       0.109847  0.042384  0.065869  0.170606  0.003430  1.000000   \n",
              "restecg  -0.124588 -0.069599  0.008389 -0.145195 -0.162687 -0.086165   \n",
              "thalach  -0.412624 -0.058626  0.300307 -0.056631 -0.023753 -0.014297   \n",
              "exang     0.111263  0.124054 -0.428233  0.067116  0.063902  0.029190   \n",
              "oldpeak   0.200243  0.089726 -0.183616  0.184896  0.084355  0.007943   \n",
              "slope    -0.165360 -0.038771  0.135174 -0.126553 -0.031929 -0.056866   \n",
              "ca        0.254462  0.140795 -0.180598  0.093545  0.068647  0.164266   \n",
              "thal      0.077368  0.198493 -0.139765  0.068690  0.121280 -0.004972   \n",
              "target   -0.244798 -0.283776  0.425574 -0.173239 -0.096773 -0.068845   \n",
              "\n",
              "           restecg   thalach     exang   oldpeak     slope        ca  \\\n",
              "age      -0.124588 -0.412624  0.111263  0.200243 -0.165360  0.254462   \n",
              "sex      -0.069599 -0.058626  0.124054  0.089726 -0.038771  0.140795   \n",
              "cp        0.008389  0.300307 -0.428233 -0.183616  0.135174 -0.180598   \n",
              "trestbps -0.145195 -0.056631  0.067116  0.184896 -0.126553  0.093545   \n",
              "chol     -0.162687 -0.023753  0.063902  0.084355 -0.031929  0.068647   \n",
              "fbs      -0.086165 -0.014297  0.029190  0.007943 -0.056866  0.164266   \n",
              "restecg   1.000000  0.025457 -0.089225 -0.047837  0.074982 -0.053946   \n",
              "thalach   0.025457  1.000000 -0.404349 -0.340564  0.370073 -0.205060   \n",
              "exang    -0.089225 -0.404349  1.000000  0.294308 -0.280124  0.106250   \n",
              "oldpeak  -0.047837 -0.340564  0.294308  1.000000 -0.585472  0.223375   \n",
              "slope     0.074982  0.370073 -0.280124 -0.585472  1.000000 -0.083491   \n",
              "ca       -0.053946 -0.205060  0.106250  0.223375 -0.083491  1.000000   \n",
              "thal     -0.003377 -0.078637  0.189253  0.200315 -0.090606  0.136160   \n",
              "target    0.101817  0.432687 -0.457502 -0.443504  0.363983 -0.391031   \n",
              "\n",
              "              thal    target  \n",
              "age       0.077368 -0.244798  \n",
              "sex       0.198493 -0.283776  \n",
              "cp       -0.139765  0.425574  \n",
              "trestbps  0.068690 -0.173239  \n",
              "chol      0.121280 -0.096773  \n",
              "fbs      -0.004972 -0.068845  \n",
              "restecg  -0.003377  0.101817  \n",
              "thalach  -0.078637  0.432687  \n",
              "exang     0.189253 -0.457502  \n",
              "oldpeak   0.200315 -0.443504  \n",
              "slope    -0.090606  0.363983  \n",
              "ca        0.136160 -0.391031  \n",
              "thal      1.000000 -0.311701  \n",
              "target   -0.311701  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8be01f5-c543-4b26-a739-045302c31176\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062222</td>\n",
              "      <td>-0.103697</td>\n",
              "      <td>0.261782</td>\n",
              "      <td>0.210520</td>\n",
              "      <td>0.109847</td>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.412624</td>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.200243</td>\n",
              "      <td>-0.165360</td>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.077368</td>\n",
              "      <td>-0.244798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>-0.062222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.283776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>-0.103697</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.425574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0.261782</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>-0.173239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0.210520</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.096773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0.109847</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.068845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>0.101817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>-0.412624</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.432687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>-0.457502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0.200243</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.443504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>-0.165360</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.363983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>-0.391031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0.077368</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.311701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>-0.244798</td>\n",
              "      <td>-0.283776</td>\n",
              "      <td>0.425574</td>\n",
              "      <td>-0.173239</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.068845</td>\n",
              "      <td>0.101817</td>\n",
              "      <td>0.432687</td>\n",
              "      <td>-0.457502</td>\n",
              "      <td>-0.443504</td>\n",
              "      <td>0.363983</td>\n",
              "      <td>-0.391031</td>\n",
              "      <td>-0.311701</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8be01f5-c543-4b26-a739-045302c31176')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8be01f5-c543-4b26-a739-045302c31176 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8be01f5-c543-4b26-a739-045302c31176');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb2deae0-bf03-4428-a8f1-d322bf5b7a00\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb2deae0-bf03-4428-a8f1-d322bf5b7a00')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb2deae0-bf03-4428-a8f1-d322bf5b7a00 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3336796953563416,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2002432632272073,\n          0.25446220532709146,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2991228194776611,\n        \"min\": -0.28377582305309207,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08972560084685732,\n          0.14079450235695648,\n          -0.062222038735579396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.343237644388663,\n        \"min\": -0.42823328385023884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.18361571896503148,\n          -0.18059763067401924,\n          -0.10369651400029238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28956654914616836,\n        \"min\": -0.1732391623681546,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.18489606509122675,\n          0.09354457752219755,\n          0.2617824169771793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2867254997156597,\n        \"min\": -0.16688487098702884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08435532313293413,\n          0.06864714277304984,\n          0.21052008975387562\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27150092525557695,\n        \"min\": -0.08616493327966852,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.00794318047234304,\n          0.16426559207767857,\n          0.10984693693665605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28984501434053883,\n        \"min\": -0.16268743250369436,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.04783727701622021,\n          -0.05394643396214631,\n          -0.12458764457926447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38230319141316005,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.34056397282727374,\n          -0.20506017090677786,\n          -0.4126237683266394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3727203587084935,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2943081762485106,\n          0.10624980942135133,\n          0.11126311741056943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38472554244127527,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          1.0,\n          0.22337523920060878,\n          0.2002432632272073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3681628540625692,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.5854716356239958,\n          -0.08349138857404798,\n          -0.16535999776800558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3194198076353616,\n        \"min\": -0.3910311347568131,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.22337523920060878,\n          1.0,\n          0.25446220532709146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29843375004744976,\n        \"min\": -0.3117007343731907,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2003145523377015,\n          0.13616037988626117,\n          0.07736766291885036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4251943991536586,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.4435044185350298,\n          -0.3910311347568131,\n          -0.2447981425022257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a",
      "metadata": {
        "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
      },
      "source": [
        "#### Converting the DataFrame to a NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
        "outputId": "8884ab58-3a46-4f81-c1a3-1ea8a4e84bc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np_data = df.values\n",
        "np_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "29b8e189-7f39-435a-8038-39098b147325",
      "metadata": {
        "id": "29b8e189-7f39-435a-8038-39098b147325"
      },
      "outputs": [],
      "source": [
        "split_point = int(np_data.shape[0]*0.7)\n",
        "\n",
        "np.random.shuffle(np_data)\n",
        "\n",
        "x_train = np_data[:split_point, :13]\n",
        "y_train = np_data[:split_point, 13]\n",
        "x_val = np_data[split_point:, :13]\n",
        "y_val = np_data[split_point:, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
        "outputId": "8db8e1ac-6198-48a9-f53a-270515a19c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in train and validation are 189 and 40.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to tensors\n",
        "x_train = torch.from_numpy(np.array(x_train, dtype=np.float32)).float()\n",
        "y_train = torch.from_numpy(np.array(y_train, dtype=np.int64)).long()\n",
        "x_val = torch.from_numpy(np.array(x_val, dtype=np.float32)).float()\n",
        "y_val = torch.from_numpy(np.array(y_val, dtype=np.int64)).long()\n",
        "\n",
        "# Split val into val/test\n",
        "val_split = int(x_val.shape[0] * 0.5)\n",
        "x_test = x_val[val_split:]\n",
        "y_test = y_val[val_split:]\n",
        "x_val = x_val[:val_split]\n",
        "y_val = y_val[:val_split]\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27",
      "metadata": {
        "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
      },
      "source": [
        "## B. Defining Neural Networks\n",
        "\n",
        "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
        "\n",
        "- Neural networks are defined by subclassing `nn.Module`.\n",
        "- The layers of the neural network are initialized in the `__init__` method.\n",
        "- The forward pass operations on input data are defined in the `forward` method.\n",
        "\n",
        "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "77975746-a7a7-4676-9527-57674cd98c0f",
      "metadata": {
        "id": "77975746-a7a7-4676-9527-57674cd98c0f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa",
      "metadata": {
        "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
      },
      "source": [
        "## C. Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
        "outputId": "3f1d4a04-0f69-4525-fb2f-688a06a85d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 16:20:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             32W /   70W |     160MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check your GPU status.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
        "outputId": "fa3b0429-2970-4aa2-d52a-73bd5ff17bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with LR=0.01, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 0.8145, Train acc: 60.3175%, Val loss: 0.8999, Val acc: 37.5000%, Best Val loss: 0.8999 Best Val acc: 37.50%\n",
            "Epoch 2/100, Train loss: 0.6917, Train acc: 60.8466%, Val loss: 0.6987, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 3/100, Train loss: 0.5936, Train acc: 69.3122%, Val loss: 0.7776, Val acc: 47.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 4/100, Train loss: 0.6197, Train acc: 63.4921%, Val loss: 0.7450, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 5/100, Train loss: 0.5972, Train acc: 70.8995%, Val loss: 0.7432, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 6/100, Train loss: 0.5728, Train acc: 68.2540%, Val loss: 0.7412, Val acc: 57.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 7/100, Train loss: 0.5630, Train acc: 70.3704%, Val loss: 0.7185, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 8/100, Train loss: 0.5533, Train acc: 74.0741%, Val loss: 0.7302, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 9/100, Train loss: 0.5488, Train acc: 74.0741%, Val loss: 0.7516, Val acc: 57.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 10/100, Train loss: 0.5642, Train acc: 73.0159%, Val loss: 0.7584, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 11/100, Train loss: 0.5477, Train acc: 69.8413%, Val loss: 0.7749, Val acc: 52.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 12/100, Train loss: 0.5439, Train acc: 73.5450%, Val loss: 0.7299, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 13/100, Train loss: 0.5395, Train acc: 72.4868%, Val loss: 0.7557, Val acc: 52.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 14/100, Train loss: 0.5420, Train acc: 75.1323%, Val loss: 0.7262, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 15/100, Train loss: 0.5493, Train acc: 76.1905%, Val loss: 0.7244, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 16/100, Train loss: 0.5710, Train acc: 69.3122%, Val loss: 0.7435, Val acc: 52.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 17/100, Train loss: 0.5324, Train acc: 74.0741%, Val loss: 0.7328, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 18/100, Train loss: 0.5386, Train acc: 73.0159%, Val loss: 0.7378, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 19/100, Train loss: 0.5379, Train acc: 72.4868%, Val loss: 0.7953, Val acc: 50.0000%, Best Val loss: 0.6987 Best Val acc: 65.00%\n",
            "Epoch 20/100, Train loss: 0.5394, Train acc: 73.0159%, Val loss: 0.7737, Val acc: 70.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 21/100, Train loss: 0.5558, Train acc: 69.3122%, Val loss: 0.8070, Val acc: 52.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 22/100, Train loss: 0.5535, Train acc: 70.3704%, Val loss: 0.7368, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 23/100, Train loss: 0.5134, Train acc: 77.2487%, Val loss: 0.7802, Val acc: 55.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 24/100, Train loss: 0.5111, Train acc: 75.1323%, Val loss: 0.7294, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 25/100, Train loss: 0.5527, Train acc: 68.7831%, Val loss: 0.7427, Val acc: 55.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 26/100, Train loss: 0.5381, Train acc: 71.4286%, Val loss: 0.7242, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 27/100, Train loss: 0.5086, Train acc: 75.6614%, Val loss: 0.7102, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 28/100, Train loss: 0.4976, Train acc: 77.7778%, Val loss: 0.7455, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 29/100, Train loss: 0.5042, Train acc: 76.7196%, Val loss: 0.7288, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 30/100, Train loss: 0.4948, Train acc: 75.6614%, Val loss: 0.7157, Val acc: 60.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 31/100, Train loss: 0.4905, Train acc: 75.6614%, Val loss: 0.7029, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 32/100, Train loss: 0.4899, Train acc: 78.3069%, Val loss: 0.7367, Val acc: 57.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 33/100, Train loss: 0.4883, Train acc: 78.3069%, Val loss: 0.7367, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 34/100, Train loss: 0.5027, Train acc: 75.6614%, Val loss: 0.7561, Val acc: 60.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 35/100, Train loss: 0.4846, Train acc: 76.7196%, Val loss: 0.7180, Val acc: 67.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 36/100, Train loss: 0.4833, Train acc: 76.7196%, Val loss: 0.7155, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 37/100, Train loss: 0.4891, Train acc: 74.6032%, Val loss: 0.7100, Val acc: 60.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 38/100, Train loss: 0.4957, Train acc: 74.6032%, Val loss: 0.7304, Val acc: 67.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 39/100, Train loss: 0.5208, Train acc: 72.4868%, Val loss: 0.7367, Val acc: 70.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 40/100, Train loss: 0.4835, Train acc: 75.1323%, Val loss: 0.7423, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 41/100, Train loss: 0.4911, Train acc: 74.6032%, Val loss: 0.7641, Val acc: 70.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 42/100, Train loss: 0.5092, Train acc: 75.1323%, Val loss: 0.7345, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 43/100, Train loss: 0.4818, Train acc: 75.1323%, Val loss: 0.7171, Val acc: 67.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 44/100, Train loss: 0.4683, Train acc: 74.6032%, Val loss: 0.7107, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 45/100, Train loss: 0.4585, Train acc: 79.3651%, Val loss: 0.7220, Val acc: 67.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 46/100, Train loss: 0.4790, Train acc: 77.7778%, Val loss: 0.7214, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 47/100, Train loss: 0.4543, Train acc: 77.7778%, Val loss: 0.7034, Val acc: 67.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 48/100, Train loss: 0.4571, Train acc: 78.3069%, Val loss: 0.7116, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 49/100, Train loss: 0.4699, Train acc: 75.6614%, Val loss: 0.7239, Val acc: 70.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 50/100, Train loss: 0.4739, Train acc: 75.6614%, Val loss: 0.7217, Val acc: 62.5000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 51/100, Train loss: 0.4571, Train acc: 76.7196%, Val loss: 0.7050, Val acc: 70.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 52/100, Train loss: 0.4821, Train acc: 75.6614%, Val loss: 0.6993, Val acc: 65.0000%, Best Val loss: 0.6987 Best Val acc: 70.00%\n",
            "Epoch 53/100, Train loss: 0.4770, Train acc: 76.1905%, Val loss: 0.6959, Val acc: 70.0000%, Best Val loss: 0.6959 Best Val acc: 70.00%\n",
            "Epoch 54/100, Train loss: 0.4554, Train acc: 77.7778%, Val loss: 0.7045, Val acc: 70.0000%, Best Val loss: 0.6959 Best Val acc: 70.00%\n",
            "Epoch 55/100, Train loss: 0.4779, Train acc: 75.6614%, Val loss: 0.6921, Val acc: 70.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 56/100, Train loss: 0.4804, Train acc: 78.3069%, Val loss: 0.6972, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 57/100, Train loss: 0.4405, Train acc: 76.7196%, Val loss: 0.7084, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 58/100, Train loss: 0.4338, Train acc: 77.7778%, Val loss: 0.7059, Val acc: 70.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 59/100, Train loss: 0.4331, Train acc: 78.3069%, Val loss: 0.6949, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 60/100, Train loss: 0.4564, Train acc: 75.1323%, Val loss: 0.7058, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 61/100, Train loss: 0.4365, Train acc: 79.8942%, Val loss: 0.7150, Val acc: 70.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 62/100, Train loss: 0.4359, Train acc: 74.6032%, Val loss: 0.7021, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 63/100, Train loss: 0.4334, Train acc: 80.4233%, Val loss: 0.6968, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 64/100, Train loss: 0.4251, Train acc: 78.3069%, Val loss: 0.7081, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 65/100, Train loss: 0.4235, Train acc: 78.8360%, Val loss: 0.7039, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 66/100, Train loss: 0.4266, Train acc: 77.2487%, Val loss: 0.7104, Val acc: 67.5000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 67/100, Train loss: 0.4261, Train acc: 79.3651%, Val loss: 0.7106, Val acc: 65.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 68/100, Train loss: 0.4169, Train acc: 79.3651%, Val loss: 0.6967, Val acc: 70.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 69/100, Train loss: 0.4301, Train acc: 77.2487%, Val loss: 0.6927, Val acc: 70.0000%, Best Val loss: 0.6921 Best Val acc: 70.00%\n",
            "Epoch 70/100, Train loss: 0.4222, Train acc: 79.8942%, Val loss: 0.6881, Val acc: 72.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 71/100, Train loss: 0.4195, Train acc: 79.3651%, Val loss: 0.6959, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 72/100, Train loss: 0.4172, Train acc: 78.3069%, Val loss: 0.6986, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 73/100, Train loss: 0.4159, Train acc: 79.3651%, Val loss: 0.7008, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 74/100, Train loss: 0.4177, Train acc: 80.4233%, Val loss: 0.7012, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 75/100, Train loss: 0.4223, Train acc: 78.3069%, Val loss: 0.6974, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 76/100, Train loss: 0.4117, Train acc: 79.3651%, Val loss: 0.6980, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 77/100, Train loss: 0.4159, Train acc: 79.3651%, Val loss: 0.6960, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 78/100, Train loss: 0.4098, Train acc: 78.8360%, Val loss: 0.7016, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 79/100, Train loss: 0.4140, Train acc: 80.4233%, Val loss: 0.7034, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 80/100, Train loss: 0.4156, Train acc: 79.3651%, Val loss: 0.6997, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 81/100, Train loss: 0.4106, Train acc: 79.3651%, Val loss: 0.6980, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 82/100, Train loss: 0.4096, Train acc: 79.3651%, Val loss: 0.6970, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 83/100, Train loss: 0.4112, Train acc: 79.8942%, Val loss: 0.6978, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 84/100, Train loss: 0.4116, Train acc: 80.4233%, Val loss: 0.6968, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 85/100, Train loss: 0.4083, Train acc: 79.3651%, Val loss: 0.6938, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 86/100, Train loss: 0.4095, Train acc: 79.3651%, Val loss: 0.6945, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 87/100, Train loss: 0.4078, Train acc: 79.8942%, Val loss: 0.6952, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 88/100, Train loss: 0.4088, Train acc: 79.3651%, Val loss: 0.6960, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 89/100, Train loss: 0.4109, Train acc: 79.8942%, Val loss: 0.6962, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 90/100, Train loss: 0.4082, Train acc: 79.3651%, Val loss: 0.6953, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 91/100, Train loss: 0.4097, Train acc: 79.8942%, Val loss: 0.6947, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 92/100, Train loss: 0.4089, Train acc: 79.8942%, Val loss: 0.6954, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 93/100, Train loss: 0.4103, Train acc: 79.3651%, Val loss: 0.6958, Val acc: 67.5000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 94/100, Train loss: 0.4090, Train acc: 79.3651%, Val loss: 0.6961, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 95/100, Train loss: 0.4063, Train acc: 79.3651%, Val loss: 0.6963, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 96/100, Train loss: 0.4086, Train acc: 79.3651%, Val loss: 0.6961, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 97/100, Train loss: 0.4080, Train acc: 79.3651%, Val loss: 0.6961, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 98/100, Train loss: 0.4071, Train acc: 79.3651%, Val loss: 0.6960, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 99/100, Train loss: 0.4073, Train acc: 79.3651%, Val loss: 0.6960, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "Epoch 100/100, Train loss: 0.4082, Train acc: 79.3651%, Val loss: 0.6960, Val acc: 70.0000%, Best Val loss: 0.6881 Best Val acc: 72.50%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 1.4022, Train acc: 58.2011%, Val loss: 1.1374, Val acc: 70.0000%, Best Val loss: 1.1374 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 0.7359, Train acc: 65.6085%, Val loss: 1.1088, Val acc: 72.5000%, Best Val loss: 1.1088 Best Val acc: 72.50%\n",
            "Epoch 3/100, Train loss: 0.8307, Train acc: 60.8466%, Val loss: 0.7889, Val acc: 65.0000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 4/100, Train loss: 0.7226, Train acc: 65.0794%, Val loss: 1.0626, Val acc: 72.5000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 5/100, Train loss: 0.8404, Train acc: 58.2011%, Val loss: 0.8402, Val acc: 62.5000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 0.7302, Train acc: 65.0794%, Val loss: 1.2445, Val acc: 47.5000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 7/100, Train loss: 0.7106, Train acc: 62.4339%, Val loss: 0.9288, Val acc: 50.0000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 8/100, Train loss: 0.6436, Train acc: 66.6667%, Val loss: 0.9447, Val acc: 65.0000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 9/100, Train loss: 0.5797, Train acc: 67.7249%, Val loss: 0.8233, Val acc: 65.0000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 10/100, Train loss: 0.5586, Train acc: 74.0741%, Val loss: 0.8875, Val acc: 52.5000%, Best Val loss: 0.7889 Best Val acc: 72.50%\n",
            "Epoch 11/100, Train loss: 0.5439, Train acc: 76.1905%, Val loss: 0.7845, Val acc: 67.5000%, Best Val loss: 0.7845 Best Val acc: 72.50%\n",
            "Epoch 12/100, Train loss: 0.5246, Train acc: 74.0741%, Val loss: 0.8253, Val acc: 52.5000%, Best Val loss: 0.7845 Best Val acc: 72.50%\n",
            "Epoch 13/100, Train loss: 0.5341, Train acc: 74.6032%, Val loss: 0.7824, Val acc: 55.0000%, Best Val loss: 0.7824 Best Val acc: 72.50%\n",
            "Epoch 14/100, Train loss: 0.5343, Train acc: 75.1323%, Val loss: 0.7965, Val acc: 57.5000%, Best Val loss: 0.7824 Best Val acc: 72.50%\n",
            "Epoch 15/100, Train loss: 0.5325, Train acc: 74.0741%, Val loss: 0.8081, Val acc: 60.0000%, Best Val loss: 0.7824 Best Val acc: 72.50%\n",
            "Epoch 16/100, Train loss: 0.5286, Train acc: 72.4868%, Val loss: 0.7774, Val acc: 62.5000%, Best Val loss: 0.7774 Best Val acc: 72.50%\n",
            "Epoch 17/100, Train loss: 0.5093, Train acc: 76.1905%, Val loss: 0.8566, Val acc: 52.5000%, Best Val loss: 0.7774 Best Val acc: 72.50%\n",
            "Epoch 18/100, Train loss: 0.5187, Train acc: 74.0741%, Val loss: 0.8123, Val acc: 55.0000%, Best Val loss: 0.7774 Best Val acc: 72.50%\n",
            "Epoch 19/100, Train loss: 0.5464, Train acc: 72.4868%, Val loss: 0.9892, Val acc: 52.5000%, Best Val loss: 0.7774 Best Val acc: 72.50%\n",
            "Epoch 20/100, Train loss: 0.5526, Train acc: 71.9577%, Val loss: 0.7497, Val acc: 65.0000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 21/100, Train loss: 0.4943, Train acc: 77.2487%, Val loss: 0.8355, Val acc: 52.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 22/100, Train loss: 0.5218, Train acc: 74.6032%, Val loss: 0.8249, Val acc: 62.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 23/100, Train loss: 0.5675, Train acc: 73.0159%, Val loss: 0.8732, Val acc: 52.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 24/100, Train loss: 0.5558, Train acc: 71.4286%, Val loss: 0.7586, Val acc: 60.0000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 25/100, Train loss: 0.5362, Train acc: 73.5450%, Val loss: 0.7745, Val acc: 57.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 26/100, Train loss: 0.5232, Train acc: 75.1323%, Val loss: 0.7865, Val acc: 67.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 27/100, Train loss: 0.5450, Train acc: 71.4286%, Val loss: 0.9525, Val acc: 65.0000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 28/100, Train loss: 0.6962, Train acc: 63.4921%, Val loss: 1.1111, Val acc: 65.0000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 29/100, Train loss: 0.6826, Train acc: 67.1958%, Val loss: 0.9354, Val acc: 62.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 30/100, Train loss: 0.5937, Train acc: 72.4868%, Val loss: 0.8862, Val acc: 57.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 31/100, Train loss: 0.5206, Train acc: 73.5450%, Val loss: 0.8381, Val acc: 52.5000%, Best Val loss: 0.7497 Best Val acc: 72.50%\n",
            "Epoch 32/100, Train loss: 0.4930, Train acc: 74.0741%, Val loss: 0.7122, Val acc: 62.5000%, Best Val loss: 0.7122 Best Val acc: 72.50%\n",
            "Epoch 33/100, Train loss: 0.5025, Train acc: 74.0741%, Val loss: 0.8430, Val acc: 65.0000%, Best Val loss: 0.7122 Best Val acc: 72.50%\n",
            "Epoch 34/100, Train loss: 0.5828, Train acc: 65.0794%, Val loss: 0.7432, Val acc: 72.5000%, Best Val loss: 0.7122 Best Val acc: 72.50%\n",
            "Epoch 35/100, Train loss: 0.4862, Train acc: 75.6614%, Val loss: 0.7594, Val acc: 60.0000%, Best Val loss: 0.7122 Best Val acc: 72.50%\n",
            "Epoch 36/100, Train loss: 0.4642, Train acc: 77.2487%, Val loss: 0.7471, Val acc: 70.0000%, Best Val loss: 0.7122 Best Val acc: 72.50%\n",
            "Epoch 37/100, Train loss: 0.4698, Train acc: 74.0741%, Val loss: 0.7025, Val acc: 70.0000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 38/100, Train loss: 0.4656, Train acc: 78.3069%, Val loss: 0.7082, Val acc: 67.5000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 39/100, Train loss: 0.4850, Train acc: 76.7196%, Val loss: 0.7053, Val acc: 67.5000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 40/100, Train loss: 0.5001, Train acc: 74.6032%, Val loss: 0.7843, Val acc: 55.0000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 41/100, Train loss: 0.5064, Train acc: 75.1323%, Val loss: 0.7652, Val acc: 60.0000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 42/100, Train loss: 0.4653, Train acc: 77.7778%, Val loss: 0.7057, Val acc: 67.5000%, Best Val loss: 0.7025 Best Val acc: 72.50%\n",
            "Epoch 43/100, Train loss: 0.4464, Train acc: 78.3069%, Val loss: 0.6994, Val acc: 67.5000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 44/100, Train loss: 0.4487, Train acc: 77.7778%, Val loss: 0.7139, Val acc: 65.0000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 45/100, Train loss: 0.4555, Train acc: 78.8360%, Val loss: 0.7261, Val acc: 70.0000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 46/100, Train loss: 0.4497, Train acc: 77.2487%, Val loss: 0.7310, Val acc: 67.5000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 47/100, Train loss: 0.5105, Train acc: 76.7196%, Val loss: 0.9002, Val acc: 50.0000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 48/100, Train loss: 0.5041, Train acc: 75.1323%, Val loss: 0.7718, Val acc: 70.0000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 49/100, Train loss: 0.5017, Train acc: 76.1905%, Val loss: 0.7866, Val acc: 70.0000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 50/100, Train loss: 0.4786, Train acc: 78.3069%, Val loss: 0.7858, Val acc: 57.5000%, Best Val loss: 0.6994 Best Val acc: 72.50%\n",
            "Epoch 51/100, Train loss: 0.4384, Train acc: 80.9524%, Val loss: 0.6914, Val acc: 70.0000%, Best Val loss: 0.6914 Best Val acc: 72.50%\n",
            "Epoch 52/100, Train loss: 0.4296, Train acc: 78.8360%, Val loss: 0.6854, Val acc: 72.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 53/100, Train loss: 0.4489, Train acc: 77.7778%, Val loss: 0.7076, Val acc: 62.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 54/100, Train loss: 0.4222, Train acc: 79.8942%, Val loss: 0.6990, Val acc: 67.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 55/100, Train loss: 0.4136, Train acc: 79.8942%, Val loss: 0.6892, Val acc: 67.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 56/100, Train loss: 0.4241, Train acc: 78.8360%, Val loss: 0.6915, Val acc: 72.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 57/100, Train loss: 0.4254, Train acc: 81.4815%, Val loss: 0.7601, Val acc: 57.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 58/100, Train loss: 0.4848, Train acc: 75.6614%, Val loss: 0.7728, Val acc: 65.0000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 59/100, Train loss: 0.4450, Train acc: 78.8360%, Val loss: 0.7054, Val acc: 62.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 60/100, Train loss: 0.4104, Train acc: 80.4233%, Val loss: 0.7150, Val acc: 70.0000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 61/100, Train loss: 0.4364, Train acc: 77.7778%, Val loss: 0.6861, Val acc: 72.5000%, Best Val loss: 0.6854 Best Val acc: 72.50%\n",
            "Epoch 62/100, Train loss: 0.4277, Train acc: 79.3651%, Val loss: 0.6803, Val acc: 65.0000%, Best Val loss: 0.6803 Best Val acc: 72.50%\n",
            "Epoch 63/100, Train loss: 0.4142, Train acc: 82.0106%, Val loss: 0.6880, Val acc: 75.0000%, Best Val loss: 0.6803 Best Val acc: 75.00%\n",
            "Epoch 64/100, Train loss: 0.4037, Train acc: 80.4233%, Val loss: 0.6836, Val acc: 70.0000%, Best Val loss: 0.6803 Best Val acc: 75.00%\n",
            "Epoch 65/100, Train loss: 0.4010, Train acc: 80.9524%, Val loss: 0.6913, Val acc: 70.0000%, Best Val loss: 0.6803 Best Val acc: 75.00%\n",
            "Epoch 66/100, Train loss: 0.4030, Train acc: 83.5979%, Val loss: 0.6903, Val acc: 65.0000%, Best Val loss: 0.6803 Best Val acc: 75.00%\n",
            "Epoch 67/100, Train loss: 0.4087, Train acc: 79.8942%, Val loss: 0.6738, Val acc: 72.5000%, Best Val loss: 0.6738 Best Val acc: 75.00%\n",
            "Epoch 68/100, Train loss: 0.4061, Train acc: 82.0106%, Val loss: 0.6677, Val acc: 70.0000%, Best Val loss: 0.6677 Best Val acc: 75.00%\n",
            "Epoch 69/100, Train loss: 0.4145, Train acc: 81.4815%, Val loss: 0.6665, Val acc: 70.0000%, Best Val loss: 0.6665 Best Val acc: 75.00%\n",
            "Epoch 70/100, Train loss: 0.3959, Train acc: 81.4815%, Val loss: 0.6637, Val acc: 75.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 71/100, Train loss: 0.4044, Train acc: 80.4233%, Val loss: 0.6664, Val acc: 70.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 72/100, Train loss: 0.4035, Train acc: 80.4233%, Val loss: 0.6820, Val acc: 72.5000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 73/100, Train loss: 0.4160, Train acc: 80.9524%, Val loss: 0.6683, Val acc: 70.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 74/100, Train loss: 0.4216, Train acc: 79.8942%, Val loss: 0.6660, Val acc: 75.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 75/100, Train loss: 0.4012, Train acc: 83.5979%, Val loss: 0.6772, Val acc: 75.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 76/100, Train loss: 0.4204, Train acc: 79.8942%, Val loss: 0.6757, Val acc: 67.5000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 77/100, Train loss: 0.4066, Train acc: 80.4233%, Val loss: 0.6896, Val acc: 70.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 78/100, Train loss: 0.4011, Train acc: 85.1852%, Val loss: 0.6654, Val acc: 70.0000%, Best Val loss: 0.6637 Best Val acc: 75.00%\n",
            "Epoch 79/100, Train loss: 0.3897, Train acc: 82.5397%, Val loss: 0.6607, Val acc: 75.0000%, Best Val loss: 0.6607 Best Val acc: 75.00%\n",
            "Epoch 80/100, Train loss: 0.4005, Train acc: 83.0688%, Val loss: 0.6706, Val acc: 72.5000%, Best Val loss: 0.6607 Best Val acc: 75.00%\n",
            "Epoch 81/100, Train loss: 0.3843, Train acc: 83.5979%, Val loss: 0.6655, Val acc: 70.0000%, Best Val loss: 0.6607 Best Val acc: 75.00%\n",
            "Epoch 82/100, Train loss: 0.3907, Train acc: 82.0106%, Val loss: 0.6582, Val acc: 72.5000%, Best Val loss: 0.6582 Best Val acc: 75.00%\n",
            "Epoch 83/100, Train loss: 0.3852, Train acc: 81.4815%, Val loss: 0.6604, Val acc: 72.5000%, Best Val loss: 0.6582 Best Val acc: 75.00%\n",
            "Epoch 84/100, Train loss: 0.3907, Train acc: 82.5397%, Val loss: 0.6555, Val acc: 75.0000%, Best Val loss: 0.6555 Best Val acc: 75.00%\n",
            "Epoch 85/100, Train loss: 0.3867, Train acc: 83.0688%, Val loss: 0.6541, Val acc: 72.5000%, Best Val loss: 0.6541 Best Val acc: 75.00%\n",
            "Epoch 86/100, Train loss: 0.3936, Train acc: 80.9524%, Val loss: 0.6532, Val acc: 72.5000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 87/100, Train loss: 0.3989, Train acc: 81.4815%, Val loss: 0.6625, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 88/100, Train loss: 0.3859, Train acc: 83.0688%, Val loss: 0.6540, Val acc: 72.5000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 89/100, Train loss: 0.3854, Train acc: 80.4233%, Val loss: 0.6538, Val acc: 72.5000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 90/100, Train loss: 0.3849, Train acc: 80.9524%, Val loss: 0.6536, Val acc: 72.5000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 91/100, Train loss: 0.3858, Train acc: 82.0106%, Val loss: 0.6552, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 92/100, Train loss: 0.3848, Train acc: 82.5397%, Val loss: 0.6547, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 93/100, Train loss: 0.3831, Train acc: 83.0688%, Val loss: 0.6544, Val acc: 72.5000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 94/100, Train loss: 0.3851, Train acc: 83.0688%, Val loss: 0.6546, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 95/100, Train loss: 0.3827, Train acc: 83.0688%, Val loss: 0.6544, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 96/100, Train loss: 0.3852, Train acc: 83.0688%, Val loss: 0.6542, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 97/100, Train loss: 0.3837, Train acc: 83.0688%, Val loss: 0.6543, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 98/100, Train loss: 0.3829, Train acc: 83.0688%, Val loss: 0.6543, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 99/100, Train loss: 0.3824, Train acc: 83.0688%, Val loss: 0.6542, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "Epoch 100/100, Train loss: 0.3836, Train acc: 83.0688%, Val loss: 0.6542, Val acc: 75.0000%, Best Val loss: 0.6532 Best Val acc: 75.00%\n",
            "\n",
            "Training with LR=0.01, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 4.8984, Train acc: 49.2063%, Val loss: 2.7767, Val acc: 70.0000%, Best Val loss: 2.7767 Best Val acc: 70.00%\n",
            "Epoch 2/100, Train loss: 1.8957, Train acc: 62.9630%, Val loss: 2.8309, Val acc: 70.0000%, Best Val loss: 2.7767 Best Val acc: 70.00%\n",
            "Epoch 3/100, Train loss: 0.9956, Train acc: 73.0159%, Val loss: 1.8778, Val acc: 57.5000%, Best Val loss: 1.8778 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 1.1109, Train acc: 67.1958%, Val loss: 1.5736, Val acc: 62.5000%, Best Val loss: 1.5736 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.8549, Train acc: 69.3122%, Val loss: 1.0696, Val acc: 55.0000%, Best Val loss: 1.0696 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 1.0670, Train acc: 63.4921%, Val loss: 1.0925, Val acc: 52.5000%, Best Val loss: 1.0696 Best Val acc: 70.00%\n",
            "Epoch 7/100, Train loss: 0.8826, Train acc: 67.7249%, Val loss: 0.9454, Val acc: 55.0000%, Best Val loss: 0.9454 Best Val acc: 70.00%\n",
            "Epoch 8/100, Train loss: 0.8455, Train acc: 66.1376%, Val loss: 1.2523, Val acc: 55.0000%, Best Val loss: 0.9454 Best Val acc: 70.00%\n",
            "Epoch 9/100, Train loss: 0.7199, Train acc: 70.3704%, Val loss: 1.4507, Val acc: 47.5000%, Best Val loss: 0.9454 Best Val acc: 70.00%\n",
            "Epoch 10/100, Train loss: 0.7503, Train acc: 69.3122%, Val loss: 1.0797, Val acc: 52.5000%, Best Val loss: 0.9454 Best Val acc: 70.00%\n",
            "Epoch 11/100, Train loss: 0.6727, Train acc: 65.6085%, Val loss: 0.9834, Val acc: 72.5000%, Best Val loss: 0.9454 Best Val acc: 72.50%\n",
            "Epoch 12/100, Train loss: 0.7026, Train acc: 67.1958%, Val loss: 0.7282, Val acc: 57.5000%, Best Val loss: 0.7282 Best Val acc: 72.50%\n",
            "Epoch 13/100, Train loss: 0.6565, Train acc: 66.1376%, Val loss: 1.2912, Val acc: 40.0000%, Best Val loss: 0.7282 Best Val acc: 72.50%\n",
            "Epoch 14/100, Train loss: 0.6359, Train acc: 70.3704%, Val loss: 0.7789, Val acc: 60.0000%, Best Val loss: 0.7282 Best Val acc: 72.50%\n",
            "Epoch 15/100, Train loss: 0.5947, Train acc: 74.0741%, Val loss: 0.9396, Val acc: 70.0000%, Best Val loss: 0.7282 Best Val acc: 72.50%\n",
            "Epoch 16/100, Train loss: 0.6276, Train acc: 70.3704%, Val loss: 0.6532, Val acc: 67.5000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 17/100, Train loss: 0.5523, Train acc: 74.0741%, Val loss: 0.9211, Val acc: 50.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 18/100, Train loss: 0.8343, Train acc: 62.4339%, Val loss: 0.9426, Val acc: 55.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 19/100, Train loss: 0.8008, Train acc: 64.0212%, Val loss: 0.7865, Val acc: 65.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 20/100, Train loss: 0.7869, Train acc: 65.6085%, Val loss: 1.3370, Val acc: 65.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 21/100, Train loss: 0.9323, Train acc: 62.9630%, Val loss: 1.4648, Val acc: 65.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 22/100, Train loss: 0.8972, Train acc: 64.5503%, Val loss: 1.5011, Val acc: 65.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 23/100, Train loss: 0.7472, Train acc: 67.1958%, Val loss: 1.0789, Val acc: 70.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 24/100, Train loss: 0.6642, Train acc: 71.9577%, Val loss: 1.1931, Val acc: 65.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 25/100, Train loss: 0.8127, Train acc: 66.6667%, Val loss: 1.1760, Val acc: 67.5000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 26/100, Train loss: 0.5338, Train acc: 74.6032%, Val loss: 0.6561, Val acc: 70.0000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 27/100, Train loss: 0.4618, Train acc: 75.6614%, Val loss: 0.7130, Val acc: 67.5000%, Best Val loss: 0.6532 Best Val acc: 72.50%\n",
            "Epoch 28/100, Train loss: 0.4733, Train acc: 75.1323%, Val loss: 0.6266, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 29/100, Train loss: 0.4202, Train acc: 82.5397%, Val loss: 0.6934, Val acc: 70.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 30/100, Train loss: 0.5494, Train acc: 70.8995%, Val loss: 0.7199, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 31/100, Train loss: 0.5646, Train acc: 68.7831%, Val loss: 1.1306, Val acc: 42.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 32/100, Train loss: 0.8582, Train acc: 63.4921%, Val loss: 0.9079, Val acc: 55.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 33/100, Train loss: 0.6317, Train acc: 67.7249%, Val loss: 1.0523, Val acc: 52.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 34/100, Train loss: 0.5445, Train acc: 74.0741%, Val loss: 0.8416, Val acc: 65.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 35/100, Train loss: 0.4461, Train acc: 80.9524%, Val loss: 0.8621, Val acc: 57.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 36/100, Train loss: 0.5085, Train acc: 77.2487%, Val loss: 0.9789, Val acc: 45.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 37/100, Train loss: 0.5032, Train acc: 75.1323%, Val loss: 0.6930, Val acc: 67.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 38/100, Train loss: 0.4252, Train acc: 76.7196%, Val loss: 0.6294, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 39/100, Train loss: 0.4027, Train acc: 79.8942%, Val loss: 0.6439, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 40/100, Train loss: 0.3951, Train acc: 83.0688%, Val loss: 0.6934, Val acc: 70.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 41/100, Train loss: 0.4255, Train acc: 77.2487%, Val loss: 0.7029, Val acc: 70.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 42/100, Train loss: 0.4607, Train acc: 75.6614%, Val loss: 0.7578, Val acc: 62.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 43/100, Train loss: 0.4750, Train acc: 77.2487%, Val loss: 0.6726, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 44/100, Train loss: 0.5115, Train acc: 73.5450%, Val loss: 0.7180, Val acc: 67.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 45/100, Train loss: 0.5782, Train acc: 73.5450%, Val loss: 1.2039, Val acc: 45.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 46/100, Train loss: 0.6696, Train acc: 67.1958%, Val loss: 0.8040, Val acc: 60.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 47/100, Train loss: 0.4607, Train acc: 75.6614%, Val loss: 0.7492, Val acc: 67.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 48/100, Train loss: 0.3999, Train acc: 79.3651%, Val loss: 0.6290, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 49/100, Train loss: 0.3776, Train acc: 82.5397%, Val loss: 0.6354, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 50/100, Train loss: 0.3745, Train acc: 85.1852%, Val loss: 0.6575, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 51/100, Train loss: 0.3934, Train acc: 80.9524%, Val loss: 0.6328, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 52/100, Train loss: 0.3660, Train acc: 85.1852%, Val loss: 0.6388, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 53/100, Train loss: 0.3801, Train acc: 82.5397%, Val loss: 0.6680, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 54/100, Train loss: 0.3725, Train acc: 81.4815%, Val loss: 0.6272, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 55/100, Train loss: 0.3747, Train acc: 82.5397%, Val loss: 0.6709, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 56/100, Train loss: 0.4367, Train acc: 79.8942%, Val loss: 0.7013, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 57/100, Train loss: 0.4083, Train acc: 80.4233%, Val loss: 0.7861, Val acc: 70.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 58/100, Train loss: 0.4475, Train acc: 78.3069%, Val loss: 0.6559, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 59/100, Train loss: 0.4222, Train acc: 79.3651%, Val loss: 0.6794, Val acc: 70.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 60/100, Train loss: 0.3550, Train acc: 85.1852%, Val loss: 0.6494, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 61/100, Train loss: 0.3531, Train acc: 85.7143%, Val loss: 0.6398, Val acc: 72.5000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 62/100, Train loss: 0.3596, Train acc: 84.6561%, Val loss: 0.6383, Val acc: 75.0000%, Best Val loss: 0.6266 Best Val acc: 75.00%\n",
            "Epoch 63/100, Train loss: 0.3582, Train acc: 85.7143%, Val loss: 0.6238, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 64/100, Train loss: 0.3597, Train acc: 83.5979%, Val loss: 0.6324, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 65/100, Train loss: 0.3573, Train acc: 82.5397%, Val loss: 0.6656, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 66/100, Train loss: 0.3671, Train acc: 84.1270%, Val loss: 0.6766, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 67/100, Train loss: 0.3972, Train acc: 79.3651%, Val loss: 0.6531, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 68/100, Train loss: 0.3797, Train acc: 80.9524%, Val loss: 0.6333, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 69/100, Train loss: 0.3671, Train acc: 84.6561%, Val loss: 0.6361, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 70/100, Train loss: 0.3560, Train acc: 82.0106%, Val loss: 0.6318, Val acc: 70.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 71/100, Train loss: 0.3497, Train acc: 84.1270%, Val loss: 0.6561, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 72/100, Train loss: 0.3617, Train acc: 82.0106%, Val loss: 0.6371, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 73/100, Train loss: 0.3590, Train acc: 84.1270%, Val loss: 0.6393, Val acc: 70.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 74/100, Train loss: 0.3548, Train acc: 81.4815%, Val loss: 0.6498, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 75/100, Train loss: 0.3646, Train acc: 84.6561%, Val loss: 0.6476, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 76/100, Train loss: 0.3701, Train acc: 82.5397%, Val loss: 0.6870, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 77/100, Train loss: 0.3723, Train acc: 82.0106%, Val loss: 0.6646, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 78/100, Train loss: 0.3572, Train acc: 83.0688%, Val loss: 0.6569, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 79/100, Train loss: 0.3521, Train acc: 87.3016%, Val loss: 0.6362, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 80/100, Train loss: 0.3461, Train acc: 83.5979%, Val loss: 0.6538, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 81/100, Train loss: 0.3447, Train acc: 87.3016%, Val loss: 0.6421, Val acc: 72.5000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 82/100, Train loss: 0.3596, Train acc: 82.5397%, Val loss: 0.6401, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 83/100, Train loss: 0.3592, Train acc: 81.4815%, Val loss: 0.6335, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 84/100, Train loss: 0.3535, Train acc: 83.0688%, Val loss: 0.6369, Val acc: 70.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 85/100, Train loss: 0.3546, Train acc: 86.2434%, Val loss: 0.6458, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 86/100, Train loss: 0.3454, Train acc: 84.6561%, Val loss: 0.6337, Val acc: 70.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 87/100, Train loss: 0.3454, Train acc: 85.1852%, Val loss: 0.6304, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 75.00%\n",
            "Epoch 88/100, Train loss: 0.3392, Train acc: 87.3016%, Val loss: 0.6332, Val acc: 77.5000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 89/100, Train loss: 0.3405, Train acc: 86.7725%, Val loss: 0.6356, Val acc: 77.5000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 90/100, Train loss: 0.3425, Train acc: 86.7725%, Val loss: 0.6303, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 91/100, Train loss: 0.3414, Train acc: 86.7725%, Val loss: 0.6311, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 92/100, Train loss: 0.3388, Train acc: 88.3598%, Val loss: 0.6323, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 93/100, Train loss: 0.3373, Train acc: 87.8307%, Val loss: 0.6313, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 94/100, Train loss: 0.3368, Train acc: 87.8307%, Val loss: 0.6302, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 95/100, Train loss: 0.3374, Train acc: 87.8307%, Val loss: 0.6297, Val acc: 77.5000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 96/100, Train loss: 0.3382, Train acc: 88.3598%, Val loss: 0.6303, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 97/100, Train loss: 0.3382, Train acc: 88.3598%, Val loss: 0.6301, Val acc: 77.5000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 98/100, Train loss: 0.3365, Train acc: 87.8307%, Val loss: 0.6304, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 99/100, Train loss: 0.3365, Train acc: 87.8307%, Val loss: 0.6304, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "Epoch 100/100, Train loss: 0.3365, Train acc: 87.8307%, Val loss: 0.6304, Val acc: 75.0000%, Best Val loss: 0.6238 Best Val acc: 77.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 3.1717, Train acc: 47.6190%, Val loss: 2.2344, Val acc: 35.0000%, Best Val loss: 2.2344 Best Val acc: 35.00%\n",
            "Epoch 2/100, Train loss: 1.3669, Train acc: 54.4974%, Val loss: 1.3420, Val acc: 65.0000%, Best Val loss: 1.3420 Best Val acc: 65.00%\n",
            "Epoch 3/100, Train loss: 0.8935, Train acc: 51.8519%, Val loss: 1.1927, Val acc: 35.0000%, Best Val loss: 1.1927 Best Val acc: 65.00%\n",
            "Epoch 4/100, Train loss: 0.8095, Train acc: 53.9683%, Val loss: 0.8301, Val acc: 70.0000%, Best Val loss: 0.8301 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.6169, Train acc: 64.0212%, Val loss: 0.9267, Val acc: 45.0000%, Best Val loss: 0.8301 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 0.6092, Train acc: 66.6667%, Val loss: 0.7644, Val acc: 70.0000%, Best Val loss: 0.7644 Best Val acc: 70.00%\n",
            "Epoch 7/100, Train loss: 0.5490, Train acc: 70.8995%, Val loss: 0.8289, Val acc: 50.0000%, Best Val loss: 0.7644 Best Val acc: 70.00%\n",
            "Epoch 8/100, Train loss: 0.6205, Train acc: 64.0212%, Val loss: 0.7013, Val acc: 65.0000%, Best Val loss: 0.7013 Best Val acc: 70.00%\n",
            "Epoch 9/100, Train loss: 0.5405, Train acc: 75.1323%, Val loss: 0.6978, Val acc: 62.5000%, Best Val loss: 0.6978 Best Val acc: 70.00%\n",
            "Epoch 10/100, Train loss: 0.5387, Train acc: 73.0159%, Val loss: 0.6878, Val acc: 65.0000%, Best Val loss: 0.6878 Best Val acc: 70.00%\n",
            "Epoch 11/100, Train loss: 0.5271, Train acc: 76.7196%, Val loss: 0.6833, Val acc: 67.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 12/100, Train loss: 0.5245, Train acc: 74.6032%, Val loss: 0.7031, Val acc: 57.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 13/100, Train loss: 0.5284, Train acc: 74.6032%, Val loss: 0.6845, Val acc: 67.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 14/100, Train loss: 0.5091, Train acc: 75.1323%, Val loss: 0.6986, Val acc: 60.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 15/100, Train loss: 0.5133, Train acc: 76.7196%, Val loss: 0.7036, Val acc: 60.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 16/100, Train loss: 0.5056, Train acc: 77.2487%, Val loss: 0.6925, Val acc: 67.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 17/100, Train loss: 0.5230, Train acc: 74.6032%, Val loss: 0.6835, Val acc: 67.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 18/100, Train loss: 0.5118, Train acc: 76.7196%, Val loss: 0.7196, Val acc: 60.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 19/100, Train loss: 0.5104, Train acc: 73.5450%, Val loss: 0.6907, Val acc: 65.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 20/100, Train loss: 0.4921, Train acc: 77.2487%, Val loss: 0.7243, Val acc: 60.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 21/100, Train loss: 0.5208, Train acc: 74.0741%, Val loss: 0.6987, Val acc: 60.0000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 22/100, Train loss: 0.5046, Train acc: 74.6032%, Val loss: 0.6910, Val acc: 67.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 23/100, Train loss: 0.4922, Train acc: 78.8360%, Val loss: 0.7393, Val acc: 62.5000%, Best Val loss: 0.6833 Best Val acc: 70.00%\n",
            "Epoch 24/100, Train loss: 0.5251, Train acc: 75.6614%, Val loss: 0.6780, Val acc: 65.0000%, Best Val loss: 0.6780 Best Val acc: 70.00%\n",
            "Epoch 25/100, Train loss: 0.5176, Train acc: 75.6614%, Val loss: 0.6531, Val acc: 70.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 26/100, Train loss: 0.5111, Train acc: 72.4868%, Val loss: 0.6866, Val acc: 60.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 27/100, Train loss: 0.4817, Train acc: 75.6614%, Val loss: 0.6566, Val acc: 67.5000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 28/100, Train loss: 0.4823, Train acc: 78.3069%, Val loss: 0.6794, Val acc: 62.5000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 29/100, Train loss: 0.4819, Train acc: 77.7778%, Val loss: 0.6603, Val acc: 67.5000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 30/100, Train loss: 0.4868, Train acc: 75.6614%, Val loss: 0.6653, Val acc: 70.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 31/100, Train loss: 0.5067, Train acc: 73.5450%, Val loss: 0.6693, Val acc: 65.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 32/100, Train loss: 0.4730, Train acc: 75.6614%, Val loss: 0.6585, Val acc: 65.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 33/100, Train loss: 0.4761, Train acc: 76.1905%, Val loss: 0.6767, Val acc: 60.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 34/100, Train loss: 0.4680, Train acc: 78.8360%, Val loss: 0.6683, Val acc: 70.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 35/100, Train loss: 0.4746, Train acc: 77.2487%, Val loss: 0.6593, Val acc: 65.0000%, Best Val loss: 0.6531 Best Val acc: 70.00%\n",
            "Epoch 36/100, Train loss: 0.4656, Train acc: 78.3069%, Val loss: 0.6525, Val acc: 70.0000%, Best Val loss: 0.6525 Best Val acc: 70.00%\n",
            "Epoch 37/100, Train loss: 0.4698, Train acc: 78.8360%, Val loss: 0.6749, Val acc: 62.5000%, Best Val loss: 0.6525 Best Val acc: 70.00%\n",
            "Epoch 38/100, Train loss: 0.4812, Train acc: 75.1323%, Val loss: 0.6727, Val acc: 67.5000%, Best Val loss: 0.6525 Best Val acc: 70.00%\n",
            "Epoch 39/100, Train loss: 0.5085, Train acc: 70.8995%, Val loss: 0.6449, Val acc: 70.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 40/100, Train loss: 0.4861, Train acc: 77.7778%, Val loss: 0.7062, Val acc: 70.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 41/100, Train loss: 0.4994, Train acc: 75.6614%, Val loss: 0.6597, Val acc: 70.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 42/100, Train loss: 0.4545, Train acc: 78.3069%, Val loss: 0.7149, Val acc: 70.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 43/100, Train loss: 0.4675, Train acc: 77.2487%, Val loss: 0.6598, Val acc: 65.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 44/100, Train loss: 0.4738, Train acc: 75.1323%, Val loss: 0.6551, Val acc: 62.5000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 45/100, Train loss: 0.4551, Train acc: 77.7778%, Val loss: 0.6711, Val acc: 65.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 46/100, Train loss: 0.4731, Train acc: 78.8360%, Val loss: 0.6528, Val acc: 65.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 47/100, Train loss: 0.4573, Train acc: 75.1323%, Val loss: 0.6462, Val acc: 70.0000%, Best Val loss: 0.6449 Best Val acc: 70.00%\n",
            "Epoch 48/100, Train loss: 0.4488, Train acc: 78.8360%, Val loss: 0.6415, Val acc: 70.0000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 49/100, Train loss: 0.4563, Train acc: 77.7778%, Val loss: 0.6497, Val acc: 67.5000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 50/100, Train loss: 0.4496, Train acc: 76.7196%, Val loss: 0.6453, Val acc: 70.0000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 51/100, Train loss: 0.4545, Train acc: 78.8360%, Val loss: 0.6488, Val acc: 67.5000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 52/100, Train loss: 0.4370, Train acc: 77.2487%, Val loss: 0.6439, Val acc: 70.0000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 53/100, Train loss: 0.4429, Train acc: 78.3069%, Val loss: 0.6445, Val acc: 67.5000%, Best Val loss: 0.6415 Best Val acc: 70.00%\n",
            "Epoch 54/100, Train loss: 0.4545, Train acc: 77.2487%, Val loss: 0.6389, Val acc: 67.5000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 55/100, Train loss: 0.4485, Train acc: 80.4233%, Val loss: 0.6475, Val acc: 67.5000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 56/100, Train loss: 0.4541, Train acc: 76.7196%, Val loss: 0.6507, Val acc: 65.0000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 57/100, Train loss: 0.4559, Train acc: 78.3069%, Val loss: 0.6545, Val acc: 67.5000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 58/100, Train loss: 0.4586, Train acc: 78.3069%, Val loss: 0.6543, Val acc: 65.0000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 59/100, Train loss: 0.4312, Train acc: 78.8360%, Val loss: 0.6426, Val acc: 67.5000%, Best Val loss: 0.6389 Best Val acc: 70.00%\n",
            "Epoch 60/100, Train loss: 0.4492, Train acc: 77.2487%, Val loss: 0.6331, Val acc: 67.5000%, Best Val loss: 0.6331 Best Val acc: 70.00%\n",
            "Epoch 61/100, Train loss: 0.4354, Train acc: 78.3069%, Val loss: 0.6312, Val acc: 72.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 62/100, Train loss: 0.4275, Train acc: 78.8360%, Val loss: 0.6402, Val acc: 72.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 63/100, Train loss: 0.4315, Train acc: 77.7778%, Val loss: 0.6349, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 64/100, Train loss: 0.4337, Train acc: 77.7778%, Val loss: 0.6392, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 65/100, Train loss: 0.4296, Train acc: 79.8942%, Val loss: 0.6378, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 66/100, Train loss: 0.4298, Train acc: 77.2487%, Val loss: 0.6361, Val acc: 72.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 67/100, Train loss: 0.4278, Train acc: 79.3651%, Val loss: 0.6345, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 68/100, Train loss: 0.4297, Train acc: 79.8942%, Val loss: 0.6336, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 69/100, Train loss: 0.4333, Train acc: 77.2487%, Val loss: 0.6359, Val acc: 72.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 70/100, Train loss: 0.4267, Train acc: 79.3651%, Val loss: 0.6391, Val acc: 72.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 71/100, Train loss: 0.4317, Train acc: 79.8942%, Val loss: 0.6324, Val acc: 67.5000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 72/100, Train loss: 0.4390, Train acc: 77.2487%, Val loss: 0.6361, Val acc: 70.0000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 73/100, Train loss: 0.4408, Train acc: 78.3069%, Val loss: 0.6453, Val acc: 70.0000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 74/100, Train loss: 0.4269, Train acc: 82.0106%, Val loss: 0.6313, Val acc: 70.0000%, Best Val loss: 0.6312 Best Val acc: 72.50%\n",
            "Epoch 75/100, Train loss: 0.4300, Train acc: 78.8360%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 76/100, Train loss: 0.4239, Train acc: 79.3651%, Val loss: 0.6340, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 77/100, Train loss: 0.4234, Train acc: 78.8360%, Val loss: 0.6324, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 78/100, Train loss: 0.4243, Train acc: 78.8360%, Val loss: 0.6318, Val acc: 70.0000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 79/100, Train loss: 0.4254, Train acc: 79.3651%, Val loss: 0.6348, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 80/100, Train loss: 0.4266, Train acc: 78.8360%, Val loss: 0.6326, Val acc: 72.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 81/100, Train loss: 0.4220, Train acc: 79.3651%, Val loss: 0.6329, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 82/100, Train loss: 0.4272, Train acc: 78.8360%, Val loss: 0.6352, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 83/100, Train loss: 0.4211, Train acc: 79.8942%, Val loss: 0.6311, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 84/100, Train loss: 0.4251, Train acc: 79.8942%, Val loss: 0.6325, Val acc: 72.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 85/100, Train loss: 0.4233, Train acc: 79.8942%, Val loss: 0.6315, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 86/100, Train loss: 0.4198, Train acc: 79.8942%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 87/100, Train loss: 0.4224, Train acc: 78.8360%, Val loss: 0.6312, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 88/100, Train loss: 0.4213, Train acc: 79.8942%, Val loss: 0.6309, Val acc: 67.5000%, Best Val loss: 0.6307 Best Val acc: 72.50%\n",
            "Epoch 89/100, Train loss: 0.4231, Train acc: 79.3651%, Val loss: 0.6306, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 90/100, Train loss: 0.4191, Train acc: 79.3651%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 91/100, Train loss: 0.4195, Train acc: 79.3651%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 92/100, Train loss: 0.4212, Train acc: 79.3651%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 93/100, Train loss: 0.4192, Train acc: 78.8360%, Val loss: 0.6306, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 94/100, Train loss: 0.4204, Train acc: 79.8942%, Val loss: 0.6307, Val acc: 67.5000%, Best Val loss: 0.6306 Best Val acc: 72.50%\n",
            "Epoch 95/100, Train loss: 0.4198, Train acc: 79.8942%, Val loss: 0.6305, Val acc: 67.5000%, Best Val loss: 0.6305 Best Val acc: 72.50%\n",
            "Epoch 96/100, Train loss: 0.4233, Train acc: 79.8942%, Val loss: 0.6305, Val acc: 67.5000%, Best Val loss: 0.6305 Best Val acc: 72.50%\n",
            "Epoch 97/100, Train loss: 0.4202, Train acc: 79.3651%, Val loss: 0.6304, Val acc: 67.5000%, Best Val loss: 0.6304 Best Val acc: 72.50%\n",
            "Epoch 98/100, Train loss: 0.4180, Train acc: 79.3651%, Val loss: 0.6305, Val acc: 67.5000%, Best Val loss: 0.6304 Best Val acc: 72.50%\n",
            "Epoch 99/100, Train loss: 0.4191, Train acc: 79.3651%, Val loss: 0.6305, Val acc: 67.5000%, Best Val loss: 0.6304 Best Val acc: 72.50%\n",
            "Epoch 100/100, Train loss: 0.4200, Train acc: 79.3651%, Val loss: 0.6305, Val acc: 67.5000%, Best Val loss: 0.6304 Best Val acc: 72.50%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.0770, Train acc: 53.4392%, Val loss: 0.8954, Val acc: 72.5000%, Best Val loss: 0.8954 Best Val acc: 72.50%\n",
            "Epoch 2/100, Train loss: 0.8416, Train acc: 61.3757%, Val loss: 1.1998, Val acc: 72.5000%, Best Val loss: 0.8954 Best Val acc: 72.50%\n",
            "Epoch 3/100, Train loss: 0.7788, Train acc: 63.4921%, Val loss: 1.0615, Val acc: 50.0000%, Best Val loss: 0.8954 Best Val acc: 72.50%\n",
            "Epoch 4/100, Train loss: 0.6753, Train acc: 65.6085%, Val loss: 1.2872, Val acc: 45.0000%, Best Val loss: 0.8954 Best Val acc: 72.50%\n",
            "Epoch 5/100, Train loss: 0.8153, Train acc: 60.3175%, Val loss: 1.0018, Val acc: 60.0000%, Best Val loss: 0.8954 Best Val acc: 72.50%\n",
            "Epoch 6/100, Train loss: 0.5725, Train acc: 70.8995%, Val loss: 0.8829, Val acc: 62.5000%, Best Val loss: 0.8829 Best Val acc: 72.50%\n",
            "Epoch 7/100, Train loss: 0.5629, Train acc: 71.9577%, Val loss: 0.8726, Val acc: 57.5000%, Best Val loss: 0.8726 Best Val acc: 72.50%\n",
            "Epoch 8/100, Train loss: 0.5484, Train acc: 71.9577%, Val loss: 0.8346, Val acc: 57.5000%, Best Val loss: 0.8346 Best Val acc: 72.50%\n",
            "Epoch 9/100, Train loss: 0.5307, Train acc: 76.1905%, Val loss: 0.8170, Val acc: 62.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 10/100, Train loss: 0.5481, Train acc: 69.8413%, Val loss: 0.9347, Val acc: 45.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 11/100, Train loss: 0.5286, Train acc: 72.4868%, Val loss: 0.8573, Val acc: 62.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 12/100, Train loss: 0.5510, Train acc: 69.8413%, Val loss: 0.8627, Val acc: 60.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 13/100, Train loss: 0.5022, Train acc: 76.7196%, Val loss: 0.9159, Val acc: 55.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 14/100, Train loss: 0.5711, Train acc: 74.0741%, Val loss: 0.8806, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 15/100, Train loss: 0.5041, Train acc: 71.9577%, Val loss: 0.8714, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 16/100, Train loss: 0.4972, Train acc: 75.1323%, Val loss: 0.8722, Val acc: 57.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 17/100, Train loss: 0.5017, Train acc: 76.1905%, Val loss: 0.8891, Val acc: 62.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 18/100, Train loss: 0.5547, Train acc: 72.4868%, Val loss: 0.9928, Val acc: 50.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 19/100, Train loss: 0.4967, Train acc: 73.0159%, Val loss: 0.8653, Val acc: 62.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 20/100, Train loss: 0.4844, Train acc: 75.6614%, Val loss: 0.8806, Val acc: 62.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 21/100, Train loss: 0.5175, Train acc: 73.0159%, Val loss: 0.8736, Val acc: 67.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 22/100, Train loss: 0.5076, Train acc: 69.3122%, Val loss: 1.0458, Val acc: 50.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 23/100, Train loss: 0.6153, Train acc: 64.0212%, Val loss: 1.0775, Val acc: 52.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 24/100, Train loss: 0.5490, Train acc: 70.8995%, Val loss: 1.0014, Val acc: 57.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 25/100, Train loss: 0.5228, Train acc: 73.0159%, Val loss: 0.8942, Val acc: 60.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 26/100, Train loss: 0.4862, Train acc: 74.0741%, Val loss: 0.8973, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 27/100, Train loss: 0.4661, Train acc: 76.1905%, Val loss: 0.8476, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 28/100, Train loss: 0.4628, Train acc: 77.7778%, Val loss: 0.8263, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 29/100, Train loss: 0.4721, Train acc: 78.3069%, Val loss: 0.8288, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 30/100, Train loss: 0.4774, Train acc: 76.1905%, Val loss: 0.8314, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 31/100, Train loss: 0.4580, Train acc: 75.6614%, Val loss: 0.9286, Val acc: 57.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 32/100, Train loss: 0.4701, Train acc: 77.2487%, Val loss: 0.8378, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 33/100, Train loss: 0.4607, Train acc: 76.1905%, Val loss: 0.8339, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 34/100, Train loss: 0.4435, Train acc: 77.7778%, Val loss: 0.8418, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 35/100, Train loss: 0.4579, Train acc: 78.3069%, Val loss: 0.8637, Val acc: 65.0000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 36/100, Train loss: 0.4431, Train acc: 78.3069%, Val loss: 0.8395, Val acc: 67.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 37/100, Train loss: 0.4457, Train acc: 80.4233%, Val loss: 0.8616, Val acc: 67.5000%, Best Val loss: 0.8170 Best Val acc: 72.50%\n",
            "Epoch 38/100, Train loss: 0.4822, Train acc: 76.1905%, Val loss: 0.8158, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 39/100, Train loss: 0.4364, Train acc: 79.3651%, Val loss: 0.8465, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 40/100, Train loss: 0.4533, Train acc: 78.3069%, Val loss: 0.8417, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 41/100, Train loss: 0.5547, Train acc: 72.4868%, Val loss: 0.8387, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 42/100, Train loss: 0.4452, Train acc: 79.3651%, Val loss: 0.8345, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 43/100, Train loss: 0.4902, Train acc: 73.5450%, Val loss: 0.8263, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 44/100, Train loss: 0.4855, Train acc: 77.2487%, Val loss: 1.0061, Val acc: 50.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 45/100, Train loss: 0.5052, Train acc: 73.5450%, Val loss: 0.8439, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 46/100, Train loss: 0.4685, Train acc: 76.1905%, Val loss: 0.8337, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 47/100, Train loss: 0.4194, Train acc: 80.9524%, Val loss: 0.8486, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 48/100, Train loss: 0.4244, Train acc: 78.8360%, Val loss: 0.9128, Val acc: 57.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 49/100, Train loss: 0.4352, Train acc: 77.2487%, Val loss: 0.8516, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 50/100, Train loss: 0.4983, Train acc: 68.7831%, Val loss: 0.9798, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 51/100, Train loss: 0.4865, Train acc: 76.1905%, Val loss: 0.8954, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 52/100, Train loss: 0.4939, Train acc: 75.6614%, Val loss: 0.9934, Val acc: 55.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 53/100, Train loss: 0.5297, Train acc: 68.7831%, Val loss: 1.0257, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 54/100, Train loss: 0.4556, Train acc: 78.8360%, Val loss: 0.9040, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 55/100, Train loss: 0.4226, Train acc: 77.2487%, Val loss: 0.8690, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 56/100, Train loss: 0.4080, Train acc: 83.5979%, Val loss: 0.8527, Val acc: 65.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 57/100, Train loss: 0.4246, Train acc: 76.7196%, Val loss: 0.8304, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 58/100, Train loss: 0.4014, Train acc: 83.0688%, Val loss: 0.8358, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 59/100, Train loss: 0.4060, Train acc: 77.7778%, Val loss: 0.8592, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 60/100, Train loss: 0.4041, Train acc: 79.8942%, Val loss: 0.8431, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 61/100, Train loss: 0.4022, Train acc: 80.4233%, Val loss: 0.8353, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 62/100, Train loss: 0.4132, Train acc: 80.4233%, Val loss: 0.8778, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 63/100, Train loss: 0.4216, Train acc: 78.8360%, Val loss: 0.8355, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 64/100, Train loss: 0.4130, Train acc: 80.4233%, Val loss: 0.8402, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 65/100, Train loss: 0.4150, Train acc: 78.3069%, Val loss: 0.8212, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 66/100, Train loss: 0.4005, Train acc: 82.0106%, Val loss: 0.8243, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 67/100, Train loss: 0.3977, Train acc: 80.9524%, Val loss: 0.8280, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 68/100, Train loss: 0.3914, Train acc: 83.5979%, Val loss: 0.8268, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 69/100, Train loss: 0.3978, Train acc: 80.4233%, Val loss: 0.8318, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 70/100, Train loss: 0.4159, Train acc: 81.4815%, Val loss: 0.8380, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 71/100, Train loss: 0.3965, Train acc: 82.0106%, Val loss: 0.8269, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 72/100, Train loss: 0.3918, Train acc: 81.4815%, Val loss: 0.8271, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 73/100, Train loss: 0.3924, Train acc: 83.0688%, Val loss: 0.8219, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 74/100, Train loss: 0.3899, Train acc: 82.5397%, Val loss: 0.8257, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 75/100, Train loss: 0.3965, Train acc: 83.0688%, Val loss: 0.8263, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 72.50%\n",
            "Epoch 76/100, Train loss: 0.3879, Train acc: 83.5979%, Val loss: 0.8370, Val acc: 75.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 77/100, Train loss: 0.3938, Train acc: 83.0688%, Val loss: 0.8262, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 78/100, Train loss: 0.3840, Train acc: 85.1852%, Val loss: 0.8261, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 79/100, Train loss: 0.3862, Train acc: 82.0106%, Val loss: 0.8281, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 80/100, Train loss: 0.3931, Train acc: 82.0106%, Val loss: 0.8287, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 81/100, Train loss: 0.3796, Train acc: 84.6561%, Val loss: 0.8289, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 82/100, Train loss: 0.3920, Train acc: 79.8942%, Val loss: 0.8247, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 83/100, Train loss: 0.3795, Train acc: 85.1852%, Val loss: 0.8269, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 84/100, Train loss: 0.3837, Train acc: 84.1270%, Val loss: 0.8246, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 85/100, Train loss: 0.3816, Train acc: 82.5397%, Val loss: 0.8306, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 86/100, Train loss: 0.3810, Train acc: 83.0688%, Val loss: 0.8256, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 87/100, Train loss: 0.3790, Train acc: 84.6561%, Val loss: 0.8252, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 88/100, Train loss: 0.3819, Train acc: 85.1852%, Val loss: 0.8258, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 89/100, Train loss: 0.3800, Train acc: 84.6561%, Val loss: 0.8253, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 90/100, Train loss: 0.3791, Train acc: 84.1270%, Val loss: 0.8252, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 91/100, Train loss: 0.3792, Train acc: 84.6561%, Val loss: 0.8248, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 92/100, Train loss: 0.3804, Train acc: 84.1270%, Val loss: 0.8250, Val acc: 70.0000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 93/100, Train loss: 0.3802, Train acc: 84.1270%, Val loss: 0.8249, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 94/100, Train loss: 0.3794, Train acc: 84.1270%, Val loss: 0.8243, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 95/100, Train loss: 0.3778, Train acc: 85.1852%, Val loss: 0.8245, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 96/100, Train loss: 0.3785, Train acc: 85.1852%, Val loss: 0.8246, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 97/100, Train loss: 0.3768, Train acc: 85.1852%, Val loss: 0.8245, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 98/100, Train loss: 0.3769, Train acc: 85.1852%, Val loss: 0.8245, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 99/100, Train loss: 0.3776, Train acc: 85.1852%, Val loss: 0.8245, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "Epoch 100/100, Train loss: 0.3776, Train acc: 85.1852%, Val loss: 0.8245, Val acc: 67.5000%, Best Val loss: 0.8158 Best Val acc: 75.00%\n",
            "\n",
            "Training with LR=0.001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 8.8826, Train acc: 46.0317%, Val loss: 8.1466, Val acc: 35.0000%, Best Val loss: 8.1466 Best Val acc: 35.00%\n",
            "Epoch 2/100, Train loss: 2.9936, Train acc: 54.4974%, Val loss: 1.5578, Val acc: 55.0000%, Best Val loss: 1.5578 Best Val acc: 55.00%\n",
            "Epoch 3/100, Train loss: 1.0803, Train acc: 62.9630%, Val loss: 1.5631, Val acc: 57.5000%, Best Val loss: 1.5578 Best Val acc: 57.50%\n",
            "Epoch 4/100, Train loss: 0.7023, Train acc: 71.9577%, Val loss: 1.1514, Val acc: 60.0000%, Best Val loss: 1.1514 Best Val acc: 60.00%\n",
            "Epoch 5/100, Train loss: 0.6369, Train acc: 70.3704%, Val loss: 0.8971, Val acc: 62.5000%, Best Val loss: 0.8971 Best Val acc: 62.50%\n",
            "Epoch 6/100, Train loss: 0.8109, Train acc: 66.6667%, Val loss: 0.7961, Val acc: 67.5000%, Best Val loss: 0.7961 Best Val acc: 67.50%\n",
            "Epoch 7/100, Train loss: 0.7637, Train acc: 68.2540%, Val loss: 0.8977, Val acc: 60.0000%, Best Val loss: 0.7961 Best Val acc: 67.50%\n",
            "Epoch 8/100, Train loss: 0.5748, Train acc: 72.4868%, Val loss: 0.9373, Val acc: 60.0000%, Best Val loss: 0.7961 Best Val acc: 67.50%\n",
            "Epoch 9/100, Train loss: 0.5880, Train acc: 70.8995%, Val loss: 0.9586, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 10/100, Train loss: 0.7253, Train acc: 61.3757%, Val loss: 0.9002, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 11/100, Train loss: 0.6241, Train acc: 67.7249%, Val loss: 0.8900, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 12/100, Train loss: 0.6803, Train acc: 69.8413%, Val loss: 0.9268, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 13/100, Train loss: 0.6864, Train acc: 66.1376%, Val loss: 0.9770, Val acc: 52.5000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 14/100, Train loss: 0.5985, Train acc: 69.3122%, Val loss: 0.8830, Val acc: 60.0000%, Best Val loss: 0.7961 Best Val acc: 70.00%\n",
            "Epoch 15/100, Train loss: 0.5174, Train acc: 74.6032%, Val loss: 1.0265, Val acc: 72.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 16/100, Train loss: 0.5935, Train acc: 70.3704%, Val loss: 0.8196, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 17/100, Train loss: 0.5683, Train acc: 69.8413%, Val loss: 0.9841, Val acc: 57.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 18/100, Train loss: 0.5540, Train acc: 73.0159%, Val loss: 0.8583, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 19/100, Train loss: 0.5798, Train acc: 67.7249%, Val loss: 0.9216, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 20/100, Train loss: 0.5175, Train acc: 70.8995%, Val loss: 0.8112, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 21/100, Train loss: 0.5174, Train acc: 73.0159%, Val loss: 0.8578, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 22/100, Train loss: 0.5919, Train acc: 68.7831%, Val loss: 1.1855, Val acc: 47.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 23/100, Train loss: 0.6272, Train acc: 67.1958%, Val loss: 1.2915, Val acc: 47.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 24/100, Train loss: 0.6934, Train acc: 67.1958%, Val loss: 1.1627, Val acc: 50.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 25/100, Train loss: 0.7898, Train acc: 67.1958%, Val loss: 1.0057, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 26/100, Train loss: 0.6883, Train acc: 70.8995%, Val loss: 0.9768, Val acc: 60.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 27/100, Train loss: 0.5169, Train acc: 71.9577%, Val loss: 0.9644, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 28/100, Train loss: 0.6043, Train acc: 72.4868%, Val loss: 1.0006, Val acc: 57.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 29/100, Train loss: 0.6876, Train acc: 65.6085%, Val loss: 0.8698, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 30/100, Train loss: 0.5400, Train acc: 74.0741%, Val loss: 0.8949, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 31/100, Train loss: 0.4789, Train acc: 73.0159%, Val loss: 0.9364, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 32/100, Train loss: 0.4894, Train acc: 74.6032%, Val loss: 0.8357, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 33/100, Train loss: 0.4910, Train acc: 75.6614%, Val loss: 0.8409, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 34/100, Train loss: 0.4692, Train acc: 77.7778%, Val loss: 0.8214, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 35/100, Train loss: 0.5607, Train acc: 75.6614%, Val loss: 0.8248, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 36/100, Train loss: 0.6353, Train acc: 72.4868%, Val loss: 1.1186, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 37/100, Train loss: 0.6645, Train acc: 69.3122%, Val loss: 1.4679, Val acc: 62.5000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 38/100, Train loss: 0.7242, Train acc: 67.7249%, Val loss: 1.1419, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 39/100, Train loss: 0.6281, Train acc: 69.8413%, Val loss: 0.9833, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 40/100, Train loss: 0.5319, Train acc: 74.0741%, Val loss: 0.9153, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 41/100, Train loss: 0.4834, Train acc: 73.0159%, Val loss: 0.8213, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 42/100, Train loss: 0.4226, Train acc: 78.3069%, Val loss: 0.8453, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 43/100, Train loss: 0.4182, Train acc: 83.5979%, Val loss: 0.7966, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 72.50%\n",
            "Epoch 44/100, Train loss: 0.4478, Train acc: 77.2487%, Val loss: 0.8025, Val acc: 75.0000%, Best Val loss: 0.7961 Best Val acc: 75.00%\n",
            "Epoch 45/100, Train loss: 0.4318, Train acc: 77.7778%, Val loss: 0.8593, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 75.00%\n",
            "Epoch 46/100, Train loss: 0.4471, Train acc: 77.2487%, Val loss: 1.0453, Val acc: 65.0000%, Best Val loss: 0.7961 Best Val acc: 75.00%\n",
            "Epoch 47/100, Train loss: 0.4460, Train acc: 78.3069%, Val loss: 0.8250, Val acc: 70.0000%, Best Val loss: 0.7961 Best Val acc: 75.00%\n",
            "Epoch 48/100, Train loss: 0.3978, Train acc: 81.4815%, Val loss: 0.7970, Val acc: 77.5000%, Best Val loss: 0.7961 Best Val acc: 77.50%\n",
            "Epoch 49/100, Train loss: 0.4318, Train acc: 77.2487%, Val loss: 0.7898, Val acc: 72.5000%, Best Val loss: 0.7898 Best Val acc: 77.50%\n",
            "Epoch 50/100, Train loss: 0.4059, Train acc: 79.8942%, Val loss: 0.8682, Val acc: 67.5000%, Best Val loss: 0.7898 Best Val acc: 77.50%\n",
            "Epoch 51/100, Train loss: 0.4630, Train acc: 77.7778%, Val loss: 0.8005, Val acc: 65.0000%, Best Val loss: 0.7898 Best Val acc: 77.50%\n",
            "Epoch 52/100, Train loss: 0.3888, Train acc: 82.0106%, Val loss: 0.7963, Val acc: 77.5000%, Best Val loss: 0.7898 Best Val acc: 77.50%\n",
            "Epoch 53/100, Train loss: 0.3922, Train acc: 80.4233%, Val loss: 0.7845, Val acc: 75.0000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 54/100, Train loss: 0.3870, Train acc: 82.5397%, Val loss: 0.7962, Val acc: 77.5000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 55/100, Train loss: 0.3836, Train acc: 82.5397%, Val loss: 0.8325, Val acc: 67.5000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 56/100, Train loss: 0.3964, Train acc: 78.3069%, Val loss: 0.7962, Val acc: 70.0000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 57/100, Train loss: 0.3812, Train acc: 84.1270%, Val loss: 0.7930, Val acc: 75.0000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 58/100, Train loss: 0.3902, Train acc: 82.5397%, Val loss: 0.7901, Val acc: 77.5000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 59/100, Train loss: 0.3968, Train acc: 78.8360%, Val loss: 0.8616, Val acc: 70.0000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 60/100, Train loss: 0.4199, Train acc: 77.7778%, Val loss: 0.8116, Val acc: 70.0000%, Best Val loss: 0.7845 Best Val acc: 77.50%\n",
            "Epoch 61/100, Train loss: 0.4120, Train acc: 83.0688%, Val loss: 0.7781, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 62/100, Train loss: 0.3766, Train acc: 79.8942%, Val loss: 0.7855, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 63/100, Train loss: 0.3709, Train acc: 82.5397%, Val loss: 0.9216, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 64/100, Train loss: 0.4527, Train acc: 77.2487%, Val loss: 0.8307, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 65/100, Train loss: 0.4454, Train acc: 79.8942%, Val loss: 0.8494, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 66/100, Train loss: 0.4300, Train acc: 77.2487%, Val loss: 0.9953, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 67/100, Train loss: 0.4949, Train acc: 74.0741%, Val loss: 0.8922, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 68/100, Train loss: 0.3923, Train acc: 83.5979%, Val loss: 0.8438, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 69/100, Train loss: 0.3966, Train acc: 83.0688%, Val loss: 0.8035, Val acc: 75.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 70/100, Train loss: 0.4199, Train acc: 80.9524%, Val loss: 0.8214, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 71/100, Train loss: 0.4544, Train acc: 75.1323%, Val loss: 0.9142, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 72/100, Train loss: 0.4091, Train acc: 79.8942%, Val loss: 0.8218, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 73/100, Train loss: 0.3894, Train acc: 80.9524%, Val loss: 0.8469, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 74/100, Train loss: 0.4411, Train acc: 76.1905%, Val loss: 0.8041, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 75/100, Train loss: 0.3898, Train acc: 84.1270%, Val loss: 0.8126, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 76/100, Train loss: 0.3863, Train acc: 80.9524%, Val loss: 0.7948, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 77/100, Train loss: 0.4260, Train acc: 78.3069%, Val loss: 0.7998, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 78/100, Train loss: 0.4107, Train acc: 80.4233%, Val loss: 0.7987, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 79/100, Train loss: 0.3729, Train acc: 82.5397%, Val loss: 0.8586, Val acc: 67.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 80/100, Train loss: 0.3764, Train acc: 81.4815%, Val loss: 0.8249, Val acc: 70.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 81/100, Train loss: 0.3613, Train acc: 84.1270%, Val loss: 0.8178, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 82/100, Train loss: 0.3757, Train acc: 81.4815%, Val loss: 0.8027, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 83/100, Train loss: 0.3564, Train acc: 83.0688%, Val loss: 0.7969, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 84/100, Train loss: 0.3591, Train acc: 82.5397%, Val loss: 0.7957, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 85/100, Train loss: 0.3701, Train acc: 83.0688%, Val loss: 0.8114, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 86/100, Train loss: 0.3585, Train acc: 83.5979%, Val loss: 0.7957, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 87/100, Train loss: 0.3611, Train acc: 83.0688%, Val loss: 0.7951, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 88/100, Train loss: 0.3557, Train acc: 85.1852%, Val loss: 0.8007, Val acc: 75.0000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 89/100, Train loss: 0.3564, Train acc: 85.1852%, Val loss: 0.7916, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 90/100, Train loss: 0.3550, Train acc: 84.6561%, Val loss: 0.7918, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 91/100, Train loss: 0.3546, Train acc: 85.1852%, Val loss: 0.7948, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 92/100, Train loss: 0.3560, Train acc: 85.7143%, Val loss: 0.7946, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 93/100, Train loss: 0.3530, Train acc: 85.1852%, Val loss: 0.7933, Val acc: 77.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 94/100, Train loss: 0.3535, Train acc: 84.6561%, Val loss: 0.7917, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 95/100, Train loss: 0.3517, Train acc: 85.1852%, Val loss: 0.7911, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 96/100, Train loss: 0.3524, Train acc: 85.1852%, Val loss: 0.7913, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 97/100, Train loss: 0.3532, Train acc: 85.1852%, Val loss: 0.7919, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 98/100, Train loss: 0.3520, Train acc: 85.1852%, Val loss: 0.7923, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 99/100, Train loss: 0.3538, Train acc: 84.6561%, Val loss: 0.7923, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "Epoch 100/100, Train loss: 0.3532, Train acc: 85.1852%, Val loss: 0.7923, Val acc: 72.5000%, Best Val loss: 0.7781 Best Val acc: 77.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=64\n",
            "Epoch 1/100, Train loss: 1.5213, Train acc: 51.8519%, Val loss: 0.8061, Val acc: 52.5000%, Best Val loss: 0.8061 Best Val acc: 52.50%\n",
            "Epoch 2/100, Train loss: 0.7993, Train acc: 57.1429%, Val loss: 0.8566, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 62.50%\n",
            "Epoch 3/100, Train loss: 0.6178, Train acc: 67.7249%, Val loss: 0.9971, Val acc: 55.0000%, Best Val loss: 0.8061 Best Val acc: 62.50%\n",
            "Epoch 4/100, Train loss: 0.5848, Train acc: 67.1958%, Val loss: 0.8755, Val acc: 65.0000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 5/100, Train loss: 0.5949, Train acc: 70.3704%, Val loss: 0.8076, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 6/100, Train loss: 0.5470, Train acc: 72.4868%, Val loss: 0.8114, Val acc: 60.0000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 7/100, Train loss: 0.5932, Train acc: 68.2540%, Val loss: 0.9371, Val acc: 55.0000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 8/100, Train loss: 0.6169, Train acc: 65.0794%, Val loss: 0.8329, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 9/100, Train loss: 0.5460, Train acc: 76.1905%, Val loss: 0.8267, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 10/100, Train loss: 0.5603, Train acc: 68.7831%, Val loss: 0.9401, Val acc: 52.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 11/100, Train loss: 0.6103, Train acc: 68.2540%, Val loss: 0.8348, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 12/100, Train loss: 0.5450, Train acc: 77.7778%, Val loss: 0.8528, Val acc: 62.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 13/100, Train loss: 0.5258, Train acc: 72.4868%, Val loss: 0.9062, Val acc: 52.5000%, Best Val loss: 0.8061 Best Val acc: 65.00%\n",
            "Epoch 14/100, Train loss: 0.5405, Train acc: 73.5450%, Val loss: 0.7876, Val acc: 62.5000%, Best Val loss: 0.7876 Best Val acc: 65.00%\n",
            "Epoch 15/100, Train loss: 0.5301, Train acc: 74.0741%, Val loss: 0.7875, Val acc: 67.5000%, Best Val loss: 0.7875 Best Val acc: 67.50%\n",
            "Epoch 16/100, Train loss: 0.5575, Train acc: 71.4286%, Val loss: 0.7696, Val acc: 67.5000%, Best Val loss: 0.7696 Best Val acc: 67.50%\n",
            "Epoch 17/100, Train loss: 0.5469, Train acc: 71.4286%, Val loss: 0.7811, Val acc: 62.5000%, Best Val loss: 0.7696 Best Val acc: 67.50%\n",
            "Epoch 18/100, Train loss: 0.4992, Train acc: 76.1905%, Val loss: 0.7754, Val acc: 62.5000%, Best Val loss: 0.7696 Best Val acc: 67.50%\n",
            "Epoch 19/100, Train loss: 0.5174, Train acc: 73.5450%, Val loss: 0.7937, Val acc: 57.5000%, Best Val loss: 0.7696 Best Val acc: 67.50%\n",
            "Epoch 20/100, Train loss: 0.4873, Train acc: 75.1323%, Val loss: 0.7691, Val acc: 62.5000%, Best Val loss: 0.7691 Best Val acc: 67.50%\n",
            "Epoch 21/100, Train loss: 0.5202, Train acc: 70.3704%, Val loss: 0.7685, Val acc: 65.0000%, Best Val loss: 0.7685 Best Val acc: 67.50%\n",
            "Epoch 22/100, Train loss: 0.5170, Train acc: 75.1323%, Val loss: 0.8790, Val acc: 70.0000%, Best Val loss: 0.7685 Best Val acc: 70.00%\n",
            "Epoch 23/100, Train loss: 0.5413, Train acc: 71.9577%, Val loss: 0.7718, Val acc: 57.5000%, Best Val loss: 0.7685 Best Val acc: 70.00%\n",
            "Epoch 24/100, Train loss: 0.4828, Train acc: 75.6614%, Val loss: 0.7740, Val acc: 60.0000%, Best Val loss: 0.7685 Best Val acc: 70.00%\n",
            "Epoch 25/100, Train loss: 0.4960, Train acc: 78.3069%, Val loss: 0.8284, Val acc: 60.0000%, Best Val loss: 0.7685 Best Val acc: 70.00%\n",
            "Epoch 26/100, Train loss: 0.5374, Train acc: 73.0159%, Val loss: 0.7515, Val acc: 62.5000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 27/100, Train loss: 0.5102, Train acc: 75.1323%, Val loss: 0.7961, Val acc: 60.0000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 28/100, Train loss: 0.5595, Train acc: 66.6667%, Val loss: 0.7976, Val acc: 67.5000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 29/100, Train loss: 0.5557, Train acc: 71.4286%, Val loss: 0.8892, Val acc: 70.0000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 30/100, Train loss: 0.5730, Train acc: 70.3704%, Val loss: 0.7533, Val acc: 62.5000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 31/100, Train loss: 0.4959, Train acc: 74.6032%, Val loss: 0.8292, Val acc: 62.5000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 32/100, Train loss: 0.4984, Train acc: 77.7778%, Val loss: 0.7552, Val acc: 62.5000%, Best Val loss: 0.7515 Best Val acc: 70.00%\n",
            "Epoch 33/100, Train loss: 0.4566, Train acc: 77.2487%, Val loss: 0.7487, Val acc: 60.0000%, Best Val loss: 0.7487 Best Val acc: 70.00%\n",
            "Epoch 34/100, Train loss: 0.4685, Train acc: 77.7778%, Val loss: 0.7762, Val acc: 70.0000%, Best Val loss: 0.7487 Best Val acc: 70.00%\n",
            "Epoch 35/100, Train loss: 0.4774, Train acc: 76.1905%, Val loss: 0.7367, Val acc: 65.0000%, Best Val loss: 0.7367 Best Val acc: 70.00%\n",
            "Epoch 36/100, Train loss: 0.4596, Train acc: 75.6614%, Val loss: 0.8222, Val acc: 60.0000%, Best Val loss: 0.7367 Best Val acc: 70.00%\n",
            "Epoch 37/100, Train loss: 0.4507, Train acc: 81.4815%, Val loss: 0.7864, Val acc: 70.0000%, Best Val loss: 0.7367 Best Val acc: 70.00%\n",
            "Epoch 38/100, Train loss: 0.4603, Train acc: 81.4815%, Val loss: 0.7643, Val acc: 67.5000%, Best Val loss: 0.7367 Best Val acc: 70.00%\n",
            "Epoch 39/100, Train loss: 0.4567, Train acc: 78.3069%, Val loss: 0.7535, Val acc: 70.0000%, Best Val loss: 0.7367 Best Val acc: 70.00%\n",
            "Epoch 40/100, Train loss: 0.4699, Train acc: 77.7778%, Val loss: 0.7320, Val acc: 60.0000%, Best Val loss: 0.7320 Best Val acc: 70.00%\n",
            "Epoch 41/100, Train loss: 0.4308, Train acc: 79.3651%, Val loss: 0.7329, Val acc: 70.0000%, Best Val loss: 0.7320 Best Val acc: 70.00%\n",
            "Epoch 42/100, Train loss: 0.4492, Train acc: 80.9524%, Val loss: 0.7711, Val acc: 60.0000%, Best Val loss: 0.7320 Best Val acc: 70.00%\n",
            "Epoch 43/100, Train loss: 0.4727, Train acc: 76.7196%, Val loss: 0.7090, Val acc: 67.5000%, Best Val loss: 0.7090 Best Val acc: 70.00%\n",
            "Epoch 44/100, Train loss: 0.4655, Train acc: 75.6614%, Val loss: 0.7911, Val acc: 70.0000%, Best Val loss: 0.7090 Best Val acc: 70.00%\n",
            "Epoch 45/100, Train loss: 0.5015, Train acc: 76.7196%, Val loss: 0.7811, Val acc: 67.5000%, Best Val loss: 0.7090 Best Val acc: 70.00%\n",
            "Epoch 46/100, Train loss: 0.5269, Train acc: 73.5450%, Val loss: 0.7324, Val acc: 67.5000%, Best Val loss: 0.7090 Best Val acc: 70.00%\n",
            "Epoch 47/100, Train loss: 0.4834, Train acc: 73.5450%, Val loss: 0.6947, Val acc: 67.5000%, Best Val loss: 0.6947 Best Val acc: 70.00%\n",
            "Epoch 48/100, Train loss: 0.4489, Train acc: 80.4233%, Val loss: 0.7423, Val acc: 72.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 49/100, Train loss: 0.4545, Train acc: 76.7196%, Val loss: 0.7923, Val acc: 67.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 50/100, Train loss: 0.4275, Train acc: 78.8360%, Val loss: 0.7713, Val acc: 72.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 51/100, Train loss: 0.4186, Train acc: 79.8942%, Val loss: 0.7292, Val acc: 70.0000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 52/100, Train loss: 0.4262, Train acc: 80.9524%, Val loss: 0.7037, Val acc: 67.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 53/100, Train loss: 0.4245, Train acc: 78.8360%, Val loss: 0.6947, Val acc: 67.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 54/100, Train loss: 0.4157, Train acc: 80.9524%, Val loss: 0.7197, Val acc: 70.0000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 55/100, Train loss: 0.4205, Train acc: 79.8942%, Val loss: 0.7064, Val acc: 62.5000%, Best Val loss: 0.6947 Best Val acc: 72.50%\n",
            "Epoch 56/100, Train loss: 0.4093, Train acc: 80.9524%, Val loss: 0.7056, Val acc: 75.0000%, Best Val loss: 0.6947 Best Val acc: 75.00%\n",
            "Epoch 57/100, Train loss: 0.4323, Train acc: 77.7778%, Val loss: 0.6811, Val acc: 67.5000%, Best Val loss: 0.6811 Best Val acc: 75.00%\n",
            "Epoch 58/100, Train loss: 0.4219, Train acc: 82.0106%, Val loss: 0.6865, Val acc: 77.5000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 59/100, Train loss: 0.4244, Train acc: 80.4233%, Val loss: 0.6879, Val acc: 70.0000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 60/100, Train loss: 0.4152, Train acc: 80.4233%, Val loss: 0.6883, Val acc: 65.0000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 61/100, Train loss: 0.4089, Train acc: 81.4815%, Val loss: 0.7034, Val acc: 65.0000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 62/100, Train loss: 0.4177, Train acc: 82.5397%, Val loss: 0.7094, Val acc: 65.0000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 63/100, Train loss: 0.4044, Train acc: 80.9524%, Val loss: 0.6871, Val acc: 67.5000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 64/100, Train loss: 0.4001, Train acc: 85.1852%, Val loss: 0.6929, Val acc: 75.0000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 65/100, Train loss: 0.4032, Train acc: 81.4815%, Val loss: 0.6825, Val acc: 67.5000%, Best Val loss: 0.6811 Best Val acc: 77.50%\n",
            "Epoch 66/100, Train loss: 0.3995, Train acc: 83.0688%, Val loss: 0.6758, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 67/100, Train loss: 0.4052, Train acc: 83.0688%, Val loss: 0.6878, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 68/100, Train loss: 0.4044, Train acc: 81.4815%, Val loss: 0.6963, Val acc: 67.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 69/100, Train loss: 0.3953, Train acc: 83.5979%, Val loss: 0.6850, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 70/100, Train loss: 0.4010, Train acc: 82.5397%, Val loss: 0.6873, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 71/100, Train loss: 0.3946, Train acc: 84.1270%, Val loss: 0.6849, Val acc: 67.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 72/100, Train loss: 0.3966, Train acc: 82.5397%, Val loss: 0.6908, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 73/100, Train loss: 0.3920, Train acc: 82.5397%, Val loss: 0.6863, Val acc: 65.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 74/100, Train loss: 0.3943, Train acc: 82.5397%, Val loss: 0.6840, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 75/100, Train loss: 0.3912, Train acc: 82.5397%, Val loss: 0.6842, Val acc: 75.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 76/100, Train loss: 0.3916, Train acc: 82.5397%, Val loss: 0.6818, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 77/100, Train loss: 0.3978, Train acc: 82.5397%, Val loss: 0.6842, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 78/100, Train loss: 0.4078, Train acc: 82.5397%, Val loss: 0.6892, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 79/100, Train loss: 0.4022, Train acc: 80.4233%, Val loss: 0.6978, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 80/100, Train loss: 0.3893, Train acc: 83.0688%, Val loss: 0.6834, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 81/100, Train loss: 0.3945, Train acc: 82.5397%, Val loss: 0.6882, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 82/100, Train loss: 0.3873, Train acc: 83.5979%, Val loss: 0.6865, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 83/100, Train loss: 0.3943, Train acc: 83.0688%, Val loss: 0.7002, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 84/100, Train loss: 0.3907, Train acc: 82.5397%, Val loss: 0.6827, Val acc: 67.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 85/100, Train loss: 0.3944, Train acc: 82.5397%, Val loss: 0.6838, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 86/100, Train loss: 0.3885, Train acc: 84.1270%, Val loss: 0.6861, Val acc: 75.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 87/100, Train loss: 0.3879, Train acc: 83.5979%, Val loss: 0.6881, Val acc: 77.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 88/100, Train loss: 0.3881, Train acc: 82.5397%, Val loss: 0.6839, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 89/100, Train loss: 0.3875, Train acc: 84.6561%, Val loss: 0.6845, Val acc: 67.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 90/100, Train loss: 0.3844, Train acc: 83.5979%, Val loss: 0.6847, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 91/100, Train loss: 0.3854, Train acc: 84.6561%, Val loss: 0.6848, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 92/100, Train loss: 0.3855, Train acc: 83.0688%, Val loss: 0.6854, Val acc: 75.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 93/100, Train loss: 0.3856, Train acc: 83.5979%, Val loss: 0.6852, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 94/100, Train loss: 0.3868, Train acc: 83.5979%, Val loss: 0.6848, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 95/100, Train loss: 0.3846, Train acc: 84.6561%, Val loss: 0.6843, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 96/100, Train loss: 0.3843, Train acc: 84.6561%, Val loss: 0.6842, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 97/100, Train loss: 0.3834, Train acc: 84.1270%, Val loss: 0.6841, Val acc: 72.5000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 98/100, Train loss: 0.3850, Train acc: 84.6561%, Val loss: 0.6841, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 99/100, Train loss: 0.3855, Train acc: 84.6561%, Val loss: 0.6841, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "Epoch 100/100, Train loss: 0.3861, Train acc: 84.6561%, Val loss: 0.6841, Val acc: 70.0000%, Best Val loss: 0.6758 Best Val acc: 77.50%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=128\n",
            "Epoch 1/100, Train loss: 2.7195, Train acc: 53.4392%, Val loss: 2.4662, Val acc: 37.5000%, Best Val loss: 2.4662 Best Val acc: 37.50%\n",
            "Epoch 2/100, Train loss: 1.5958, Train acc: 49.7354%, Val loss: 0.9696, Val acc: 40.0000%, Best Val loss: 0.9696 Best Val acc: 40.00%\n",
            "Epoch 3/100, Train loss: 0.9489, Train acc: 55.0265%, Val loss: 0.9422, Val acc: 70.0000%, Best Val loss: 0.9422 Best Val acc: 70.00%\n",
            "Epoch 4/100, Train loss: 0.8828, Train acc: 59.2593%, Val loss: 1.0891, Val acc: 67.5000%, Best Val loss: 0.9422 Best Val acc: 70.00%\n",
            "Epoch 5/100, Train loss: 0.7753, Train acc: 67.1958%, Val loss: 1.1526, Val acc: 47.5000%, Best Val loss: 0.9422 Best Val acc: 70.00%\n",
            "Epoch 6/100, Train loss: 0.6597, Train acc: 68.2540%, Val loss: 0.9617, Val acc: 65.0000%, Best Val loss: 0.9422 Best Val acc: 70.00%\n",
            "Epoch 7/100, Train loss: 0.6367, Train acc: 69.8413%, Val loss: 0.8285, Val acc: 57.5000%, Best Val loss: 0.8285 Best Val acc: 70.00%\n",
            "Epoch 8/100, Train loss: 0.5774, Train acc: 73.0159%, Val loss: 0.7743, Val acc: 55.0000%, Best Val loss: 0.7743 Best Val acc: 70.00%\n",
            "Epoch 9/100, Train loss: 0.5797, Train acc: 70.3704%, Val loss: 0.7039, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 10/100, Train loss: 0.5832, Train acc: 68.2540%, Val loss: 0.7619, Val acc: 65.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 11/100, Train loss: 0.5412, Train acc: 69.8413%, Val loss: 0.7591, Val acc: 60.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 12/100, Train loss: 0.5352, Train acc: 72.4868%, Val loss: 0.7522, Val acc: 57.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 13/100, Train loss: 0.5278, Train acc: 75.1323%, Val loss: 0.7792, Val acc: 65.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 14/100, Train loss: 0.5689, Train acc: 72.4868%, Val loss: 0.7871, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 15/100, Train loss: 0.6279, Train acc: 66.1376%, Val loss: 0.7636, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 16/100, Train loss: 0.6520, Train acc: 64.0212%, Val loss: 1.0297, Val acc: 52.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 17/100, Train loss: 0.5781, Train acc: 68.7831%, Val loss: 0.9049, Val acc: 50.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 18/100, Train loss: 0.5566, Train acc: 73.0159%, Val loss: 0.8269, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 19/100, Train loss: 0.5479, Train acc: 74.6032%, Val loss: 0.9042, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 20/100, Train loss: 0.6090, Train acc: 71.9577%, Val loss: 0.8822, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 21/100, Train loss: 0.6030, Train acc: 68.7831%, Val loss: 0.7541, Val acc: 57.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 22/100, Train loss: 0.4899, Train acc: 75.6614%, Val loss: 0.7943, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 23/100, Train loss: 0.5162, Train acc: 76.7196%, Val loss: 0.7637, Val acc: 57.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 24/100, Train loss: 0.4762, Train acc: 76.1905%, Val loss: 0.7725, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 25/100, Train loss: 0.4876, Train acc: 74.0741%, Val loss: 0.7547, Val acc: 67.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 26/100, Train loss: 0.4936, Train acc: 79.8942%, Val loss: 0.7541, Val acc: 57.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 27/100, Train loss: 0.5024, Train acc: 74.6032%, Val loss: 0.7839, Val acc: 60.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 28/100, Train loss: 0.5030, Train acc: 74.0741%, Val loss: 0.7635, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 29/100, Train loss: 0.5987, Train acc: 69.8413%, Val loss: 1.1349, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 30/100, Train loss: 0.6200, Train acc: 70.3704%, Val loss: 0.9025, Val acc: 60.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 31/100, Train loss: 0.5762, Train acc: 71.4286%, Val loss: 1.1712, Val acc: 47.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 32/100, Train loss: 0.7151, Train acc: 67.1958%, Val loss: 0.9040, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 33/100, Train loss: 0.5232, Train acc: 73.5450%, Val loss: 0.8064, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 34/100, Train loss: 0.5427, Train acc: 73.0159%, Val loss: 1.1963, Val acc: 47.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 35/100, Train loss: 0.6210, Train acc: 67.7249%, Val loss: 0.7633, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 36/100, Train loss: 0.5318, Train acc: 73.5450%, Val loss: 0.8624, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 37/100, Train loss: 0.5139, Train acc: 73.0159%, Val loss: 0.8023, Val acc: 60.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 38/100, Train loss: 0.4824, Train acc: 78.8360%, Val loss: 0.8314, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 39/100, Train loss: 0.4695, Train acc: 76.7196%, Val loss: 0.7651, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 40/100, Train loss: 0.4493, Train acc: 79.8942%, Val loss: 0.8958, Val acc: 55.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 41/100, Train loss: 0.5315, Train acc: 72.4868%, Val loss: 0.8520, Val acc: 70.0000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 42/100, Train loss: 0.4955, Train acc: 78.3069%, Val loss: 0.7567, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 43/100, Train loss: 0.4685, Train acc: 75.1323%, Val loss: 0.7405, Val acc: 62.5000%, Best Val loss: 0.7039 Best Val acc: 70.00%\n",
            "Epoch 44/100, Train loss: 0.4406, Train acc: 78.8360%, Val loss: 0.7024, Val acc: 70.0000%, Best Val loss: 0.7024 Best Val acc: 70.00%\n",
            "Epoch 45/100, Train loss: 0.4427, Train acc: 76.7196%, Val loss: 0.7269, Val acc: 62.5000%, Best Val loss: 0.7024 Best Val acc: 70.00%\n",
            "Epoch 46/100, Train loss: 0.4384, Train acc: 79.3651%, Val loss: 0.7597, Val acc: 67.5000%, Best Val loss: 0.7024 Best Val acc: 70.00%\n",
            "Epoch 47/100, Train loss: 0.4393, Train acc: 78.3069%, Val loss: 0.7432, Val acc: 72.5000%, Best Val loss: 0.7024 Best Val acc: 72.50%\n",
            "Epoch 48/100, Train loss: 0.4291, Train acc: 82.0106%, Val loss: 0.7713, Val acc: 65.0000%, Best Val loss: 0.7024 Best Val acc: 72.50%\n",
            "Epoch 49/100, Train loss: 0.4619, Train acc: 77.7778%, Val loss: 0.7030, Val acc: 67.5000%, Best Val loss: 0.7024 Best Val acc: 72.50%\n",
            "Epoch 50/100, Train loss: 0.4645, Train acc: 77.2487%, Val loss: 0.7730, Val acc: 67.5000%, Best Val loss: 0.7024 Best Val acc: 72.50%\n",
            "Epoch 51/100, Train loss: 0.4607, Train acc: 74.6032%, Val loss: 0.7028, Val acc: 67.5000%, Best Val loss: 0.7024 Best Val acc: 72.50%\n",
            "Epoch 52/100, Train loss: 0.4206, Train acc: 78.8360%, Val loss: 0.6831, Val acc: 72.5000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 53/100, Train loss: 0.4173, Train acc: 79.8942%, Val loss: 0.7217, Val acc: 65.0000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 54/100, Train loss: 0.4343, Train acc: 80.4233%, Val loss: 0.7182, Val acc: 70.0000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 55/100, Train loss: 0.4470, Train acc: 76.1905%, Val loss: 0.6854, Val acc: 65.0000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 56/100, Train loss: 0.4264, Train acc: 82.0106%, Val loss: 0.7078, Val acc: 65.0000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 57/100, Train loss: 0.4498, Train acc: 74.6032%, Val loss: 0.7378, Val acc: 72.5000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 58/100, Train loss: 0.4332, Train acc: 78.8360%, Val loss: 0.7007, Val acc: 67.5000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 59/100, Train loss: 0.4075, Train acc: 79.8942%, Val loss: 0.6889, Val acc: 72.5000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 60/100, Train loss: 0.4147, Train acc: 80.9524%, Val loss: 0.7084, Val acc: 65.0000%, Best Val loss: 0.6831 Best Val acc: 72.50%\n",
            "Epoch 61/100, Train loss: 0.4206, Train acc: 80.4233%, Val loss: 0.6995, Val acc: 75.0000%, Best Val loss: 0.6831 Best Val acc: 75.00%\n",
            "Epoch 62/100, Train loss: 0.4187, Train acc: 82.0106%, Val loss: 0.6868, Val acc: 75.0000%, Best Val loss: 0.6831 Best Val acc: 75.00%\n",
            "Epoch 63/100, Train loss: 0.4109, Train acc: 80.4233%, Val loss: 0.6770, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 64/100, Train loss: 0.4102, Train acc: 81.4815%, Val loss: 0.6875, Val acc: 72.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 65/100, Train loss: 0.4076, Train acc: 79.8942%, Val loss: 0.6897, Val acc: 70.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 66/100, Train loss: 0.4075, Train acc: 82.5397%, Val loss: 0.6842, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 67/100, Train loss: 0.4230, Train acc: 80.9524%, Val loss: 0.6873, Val acc: 65.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 68/100, Train loss: 0.4207, Train acc: 79.3651%, Val loss: 0.6776, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 69/100, Train loss: 0.4063, Train acc: 82.0106%, Val loss: 0.6807, Val acc: 67.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 70/100, Train loss: 0.4142, Train acc: 81.4815%, Val loss: 0.6825, Val acc: 67.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 71/100, Train loss: 0.3948, Train acc: 82.5397%, Val loss: 0.6871, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 72/100, Train loss: 0.4024, Train acc: 82.0106%, Val loss: 0.6865, Val acc: 72.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 73/100, Train loss: 0.4019, Train acc: 81.4815%, Val loss: 0.6887, Val acc: 67.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 74/100, Train loss: 0.4109, Train acc: 82.0106%, Val loss: 0.6835, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 75/100, Train loss: 0.4099, Train acc: 82.5397%, Val loss: 0.6907, Val acc: 65.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 76/100, Train loss: 0.4067, Train acc: 82.0106%, Val loss: 0.6878, Val acc: 75.0000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 77/100, Train loss: 0.4027, Train acc: 80.9524%, Val loss: 0.6787, Val acc: 67.5000%, Best Val loss: 0.6770 Best Val acc: 75.00%\n",
            "Epoch 78/100, Train loss: 0.3974, Train acc: 82.5397%, Val loss: 0.6744, Val acc: 75.0000%, Best Val loss: 0.6744 Best Val acc: 75.00%\n",
            "Epoch 79/100, Train loss: 0.3991, Train acc: 80.9524%, Val loss: 0.6734, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 80/100, Train loss: 0.3961, Train acc: 82.5397%, Val loss: 0.6751, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 81/100, Train loss: 0.4007, Train acc: 81.4815%, Val loss: 0.6745, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 82/100, Train loss: 0.3940, Train acc: 82.5397%, Val loss: 0.6782, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 83/100, Train loss: 0.3943, Train acc: 82.0106%, Val loss: 0.6776, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 84/100, Train loss: 0.3937, Train acc: 80.9524%, Val loss: 0.6785, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 85/100, Train loss: 0.3949, Train acc: 82.0106%, Val loss: 0.6799, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 86/100, Train loss: 0.3941, Train acc: 82.5397%, Val loss: 0.6761, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 87/100, Train loss: 0.3927, Train acc: 81.4815%, Val loss: 0.6750, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 88/100, Train loss: 0.3942, Train acc: 82.5397%, Val loss: 0.6753, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 89/100, Train loss: 0.3972, Train acc: 80.9524%, Val loss: 0.6742, Val acc: 70.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 90/100, Train loss: 0.3926, Train acc: 81.4815%, Val loss: 0.6734, Val acc: 72.5000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 91/100, Train loss: 0.3950, Train acc: 80.9524%, Val loss: 0.6744, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 92/100, Train loss: 0.3910, Train acc: 82.0106%, Val loss: 0.6736, Val acc: 75.0000%, Best Val loss: 0.6734 Best Val acc: 75.00%\n",
            "Epoch 93/100, Train loss: 0.3913, Train acc: 82.5397%, Val loss: 0.6733, Val acc: 75.0000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 94/100, Train loss: 0.3916, Train acc: 82.0106%, Val loss: 0.6733, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 95/100, Train loss: 0.3939, Train acc: 82.5397%, Val loss: 0.6737, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 96/100, Train loss: 0.3905, Train acc: 82.5397%, Val loss: 0.6738, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 97/100, Train loss: 0.3912, Train acc: 82.0106%, Val loss: 0.6737, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 98/100, Train loss: 0.3916, Train acc: 82.5397%, Val loss: 0.6737, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 99/100, Train loss: 0.3903, Train acc: 82.5397%, Val loss: 0.6737, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "Epoch 100/100, Train loss: 0.3906, Train acc: 82.5397%, Val loss: 0.6737, Val acc: 72.5000%, Best Val loss: 0.6733 Best Val acc: 75.00%\n",
            "\n",
            "Training with LR=0.0001, Hidden Units=256\n",
            "Epoch 1/100, Train loss: 8.0713, Train acc: 47.0899%, Val loss: 7.1400, Val acc: 65.0000%, Best Val loss: 7.1400 Best Val acc: 65.00%\n",
            "Epoch 2/100, Train loss: 4.7597, Train acc: 49.2063%, Val loss: 3.8741, Val acc: 35.0000%, Best Val loss: 3.8741 Best Val acc: 65.00%\n",
            "Epoch 3/100, Train loss: 3.7481, Train acc: 47.0899%, Val loss: 2.0191, Val acc: 67.5000%, Best Val loss: 2.0191 Best Val acc: 67.50%\n",
            "Epoch 4/100, Train loss: 1.9947, Train acc: 56.6138%, Val loss: 1.8150, Val acc: 50.0000%, Best Val loss: 1.8150 Best Val acc: 67.50%\n",
            "Epoch 5/100, Train loss: 2.0744, Train acc: 59.7884%, Val loss: 1.8953, Val acc: 47.5000%, Best Val loss: 1.8150 Best Val acc: 67.50%\n",
            "Epoch 6/100, Train loss: 1.4589, Train acc: 60.3175%, Val loss: 1.8446, Val acc: 67.5000%, Best Val loss: 1.8150 Best Val acc: 67.50%\n",
            "Epoch 7/100, Train loss: 0.9817, Train acc: 66.1376%, Val loss: 2.0854, Val acc: 42.5000%, Best Val loss: 1.8150 Best Val acc: 67.50%\n",
            "Epoch 8/100, Train loss: 0.9851, Train acc: 59.2593%, Val loss: 1.3517, Val acc: 70.0000%, Best Val loss: 1.3517 Best Val acc: 70.00%\n",
            "Epoch 9/100, Train loss: 0.9021, Train acc: 66.6667%, Val loss: 0.9647, Val acc: 65.0000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 10/100, Train loss: 0.6879, Train acc: 68.7831%, Val loss: 1.3165, Val acc: 45.0000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 11/100, Train loss: 0.8184, Train acc: 64.0212%, Val loss: 0.9996, Val acc: 55.0000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 12/100, Train loss: 0.7171, Train acc: 64.0212%, Val loss: 0.9867, Val acc: 62.5000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 13/100, Train loss: 0.5903, Train acc: 69.8413%, Val loss: 1.0477, Val acc: 62.5000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 14/100, Train loss: 0.6006, Train acc: 68.7831%, Val loss: 1.3380, Val acc: 42.5000%, Best Val loss: 0.9647 Best Val acc: 70.00%\n",
            "Epoch 15/100, Train loss: 0.6318, Train acc: 68.2540%, Val loss: 0.9172, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 16/100, Train loss: 0.5690, Train acc: 72.4868%, Val loss: 0.9209, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 17/100, Train loss: 0.5446, Train acc: 71.4286%, Val loss: 1.0479, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 18/100, Train loss: 0.6416, Train acc: 64.5503%, Val loss: 0.9273, Val acc: 60.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 19/100, Train loss: 0.6994, Train acc: 65.0794%, Val loss: 1.0822, Val acc: 55.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 20/100, Train loss: 0.5532, Train acc: 73.5450%, Val loss: 0.9749, Val acc: 52.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 21/100, Train loss: 0.5360, Train acc: 71.4286%, Val loss: 0.9980, Val acc: 55.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 22/100, Train loss: 0.5453, Train acc: 71.4286%, Val loss: 0.9567, Val acc: 60.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 23/100, Train loss: 0.5305, Train acc: 73.0159%, Val loss: 0.9538, Val acc: 52.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 24/100, Train loss: 0.5675, Train acc: 71.4286%, Val loss: 0.9438, Val acc: 55.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 25/100, Train loss: 0.6136, Train acc: 66.1376%, Val loss: 0.9206, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 26/100, Train loss: 0.4891, Train acc: 77.7778%, Val loss: 0.9471, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 27/100, Train loss: 0.5605, Train acc: 71.9577%, Val loss: 0.9677, Val acc: 60.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 28/100, Train loss: 0.6486, Train acc: 66.1376%, Val loss: 1.8055, Val acc: 40.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 29/100, Train loss: 0.7743, Train acc: 66.1376%, Val loss: 1.0706, Val acc: 60.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 30/100, Train loss: 0.7932, Train acc: 65.0794%, Val loss: 1.5106, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 31/100, Train loss: 0.7528, Train acc: 65.6085%, Val loss: 1.4289, Val acc: 45.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 32/100, Train loss: 0.7156, Train acc: 66.6667%, Val loss: 1.2125, Val acc: 52.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 33/100, Train loss: 0.5918, Train acc: 70.3704%, Val loss: 1.1023, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 34/100, Train loss: 0.5354, Train acc: 73.0159%, Val loss: 0.9996, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 35/100, Train loss: 0.5002, Train acc: 73.0159%, Val loss: 1.0035, Val acc: 57.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 36/100, Train loss: 0.4956, Train acc: 74.6032%, Val loss: 0.9399, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 37/100, Train loss: 0.4959, Train acc: 75.1323%, Val loss: 0.9241, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 38/100, Train loss: 0.5019, Train acc: 74.0741%, Val loss: 1.0473, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 39/100, Train loss: 0.5434, Train acc: 70.3704%, Val loss: 0.9565, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 40/100, Train loss: 0.4957, Train acc: 74.0741%, Val loss: 1.1828, Val acc: 50.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 41/100, Train loss: 0.5512, Train acc: 70.8995%, Val loss: 0.9649, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 42/100, Train loss: 0.4790, Train acc: 75.1323%, Val loss: 0.9395, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 43/100, Train loss: 0.5206, Train acc: 72.4868%, Val loss: 0.9337, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 44/100, Train loss: 0.4756, Train acc: 76.7196%, Val loss: 1.0409, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 45/100, Train loss: 0.5228, Train acc: 71.9577%, Val loss: 0.9680, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 46/100, Train loss: 0.4970, Train acc: 74.0741%, Val loss: 0.9778, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 47/100, Train loss: 0.4458, Train acc: 78.8360%, Val loss: 1.0083, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 48/100, Train loss: 0.4674, Train acc: 74.0741%, Val loss: 0.9564, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 49/100, Train loss: 0.4456, Train acc: 78.3069%, Val loss: 0.9582, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 50/100, Train loss: 0.4643, Train acc: 78.3069%, Val loss: 0.9508, Val acc: 62.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 51/100, Train loss: 0.4761, Train acc: 77.2487%, Val loss: 1.0084, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 52/100, Train loss: 0.4505, Train acc: 78.8360%, Val loss: 0.9493, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 53/100, Train loss: 0.4442, Train acc: 78.8360%, Val loss: 0.9443, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 54/100, Train loss: 0.4655, Train acc: 75.6614%, Val loss: 1.0859, Val acc: 52.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 55/100, Train loss: 0.5467, Train acc: 67.7249%, Val loss: 0.9403, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 56/100, Train loss: 0.4740, Train acc: 75.6614%, Val loss: 0.9512, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 57/100, Train loss: 0.4436, Train acc: 79.3651%, Val loss: 0.9473, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 58/100, Train loss: 0.4616, Train acc: 75.6614%, Val loss: 1.0068, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 59/100, Train loss: 0.4379, Train acc: 75.1323%, Val loss: 0.9655, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 60/100, Train loss: 0.4338, Train acc: 79.3651%, Val loss: 0.9854, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 61/100, Train loss: 0.4516, Train acc: 77.2487%, Val loss: 0.9565, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 62/100, Train loss: 0.4377, Train acc: 76.7196%, Val loss: 0.9491, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 63/100, Train loss: 0.4728, Train acc: 76.7196%, Val loss: 0.9871, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 64/100, Train loss: 0.4706, Train acc: 76.1905%, Val loss: 0.9763, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 65/100, Train loss: 0.4587, Train acc: 79.3651%, Val loss: 0.9835, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 66/100, Train loss: 0.4285, Train acc: 79.3651%, Val loss: 0.9613, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 67/100, Train loss: 0.4179, Train acc: 80.4233%, Val loss: 0.9849, Val acc: 65.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 68/100, Train loss: 0.4387, Train acc: 78.3069%, Val loss: 0.9521, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 69/100, Train loss: 0.4188, Train acc: 82.0106%, Val loss: 0.9534, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 70/100, Train loss: 0.4211, Train acc: 81.4815%, Val loss: 0.9493, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 71/100, Train loss: 0.4093, Train acc: 80.9524%, Val loss: 0.9718, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 70.00%\n",
            "Epoch 72/100, Train loss: 0.4326, Train acc: 78.8360%, Val loss: 0.9493, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 73/100, Train loss: 0.4086, Train acc: 83.0688%, Val loss: 0.9642, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 74/100, Train loss: 0.4162, Train acc: 80.4233%, Val loss: 0.9552, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 75/100, Train loss: 0.4113, Train acc: 81.4815%, Val loss: 0.9504, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 76/100, Train loss: 0.4082, Train acc: 82.5397%, Val loss: 0.9512, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 77/100, Train loss: 0.4141, Train acc: 81.4815%, Val loss: 0.9475, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 78/100, Train loss: 0.4077, Train acc: 80.4233%, Val loss: 0.9533, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 79/100, Train loss: 0.4118, Train acc: 82.0106%, Val loss: 0.9554, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 80/100, Train loss: 0.4098, Train acc: 82.0106%, Val loss: 0.9529, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 81/100, Train loss: 0.4136, Train acc: 82.5397%, Val loss: 0.9527, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 82/100, Train loss: 0.4118, Train acc: 82.0106%, Val loss: 0.9625, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 83/100, Train loss: 0.4093, Train acc: 80.4233%, Val loss: 0.9531, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 84/100, Train loss: 0.4091, Train acc: 82.5397%, Val loss: 0.9549, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 85/100, Train loss: 0.4089, Train acc: 80.9524%, Val loss: 0.9590, Val acc: 67.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 86/100, Train loss: 0.4059, Train acc: 80.9524%, Val loss: 0.9566, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 87/100, Train loss: 0.4052, Train acc: 82.5397%, Val loss: 0.9552, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 88/100, Train loss: 0.4079, Train acc: 82.0106%, Val loss: 0.9560, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 89/100, Train loss: 0.4098, Train acc: 82.5397%, Val loss: 0.9587, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 90/100, Train loss: 0.4034, Train acc: 81.4815%, Val loss: 0.9552, Val acc: 70.0000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 91/100, Train loss: 0.4035, Train acc: 82.5397%, Val loss: 0.9548, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 92/100, Train loss: 0.4057, Train acc: 82.5397%, Val loss: 0.9550, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 93/100, Train loss: 0.4047, Train acc: 82.5397%, Val loss: 0.9546, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 94/100, Train loss: 0.4036, Train acc: 82.5397%, Val loss: 0.9546, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 95/100, Train loss: 0.4032, Train acc: 83.0688%, Val loss: 0.9552, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 96/100, Train loss: 0.4028, Train acc: 83.0688%, Val loss: 0.9550, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 97/100, Train loss: 0.4038, Train acc: 83.0688%, Val loss: 0.9551, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 98/100, Train loss: 0.4044, Train acc: 83.0688%, Val loss: 0.9552, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 99/100, Train loss: 0.4042, Train acc: 83.0688%, Val loss: 0.9552, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n",
            "Epoch 100/100, Train loss: 0.4034, Train acc: 83.0688%, Val loss: 0.9552, Val acc: 72.5000%, Best Val loss: 0.9172 Best Val acc: 72.50%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hyperparameters to try\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "hidden_units_list = [64, 128, 256]\n",
        "results = []\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        print(f\"\\nTraining with LR={lr}, Hidden Units={hidden_units}\")\n",
        "\n",
        "        model = Model(hidden_units).cuda()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_acc = -1\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs): # Fixed: Corrected the indentation of this line\n",
        "\n",
        "            # Training\n",
        "            model.train() # Fixed: Set model to training mode\n",
        "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "            for features, labels in train_loader: # Fixed: Iterating over train_loader for training\n",
        "                features, labels = features.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad() # Fixed: Reset gradients\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()  # Fixed: Calculate gradients\n",
        "                optimizer.step() # Fixed: Update weights\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for features, labels in val_loader:\n",
        "                    features, labels = features.cuda(), labels.cuda()\n",
        "                    outputs = model(features)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Checkpoint\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f'model_LR{lr}_HU{hidden_units}.pth')\n",
        "\n",
        "            # Learning rate update\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, '\n",
        "                  f'Train loss: {avg_train_loss:.4f}, Train acc: {train_acc:.4f}%, '\n",
        "                  f'Val loss: {avg_val_loss:.4f}, Val acc: {val_acc:.4f}%, '\n",
        "                  f'Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
        "\n",
        "            # Store performance\n",
        "            train_losses.append(avg_train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "        # Save result summary after all epochs\n",
        "        results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Best Val Loss': round(best_val_loss, 4),\n",
        "            'Best Val Acc': round(best_val_acc, 2)\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7984c6e-6652-4160-b572-07d48bc93a3f",
      "metadata": {
        "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
      },
      "source": [
        "#### Visualizing the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
        "outputId": "33a7d1ae-388d-48e6-ad5e-c87f216d4cfb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "ax[0].plot(train_accuracies)\n",
        "ax[0].plot(val_accuracies)\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend(['Train', 'Val'])\n",
        "\n",
        "# Plotting training and validation loss\n",
        "ax[1].plot(train_losses)\n",
        "ax[1].plot(val_losses)\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend(['Train', 'Val'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20",
      "metadata": {
        "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
      },
      "source": [
        "## D. Evaluating Your Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "f49735d7-466f-4037-8078-172f03dffd8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f49735d7-466f-4037-8078-172f03dffd8d",
        "outputId": "72ba331e-cc70-42a9-e983-775e77b406f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   50    0   2       120   219    0        1      158      0      1.6      1   \n",
              "1   58    0   2       120   340    0        1      172      0      0.0      2   \n",
              "2   66    0   3       150   226    0        1      114      0      2.6      0   \n",
              "3   43    1   0       150   247    0        1      171      0      1.5      2   \n",
              "4   69    0   3       140   239    0        1      151      0      1.8      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     2       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   2     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0616e586-b520-4bc6-abe4-6c6f8ac4e4e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0616e586-b520-4bc6-abe4-6c6f8ac4e4e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0616e586-b520-4bc6-abe4-6c6f8ac4e4e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0616e586-b520-4bc6-abe4-6c6f8ac4e4e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-22019ead-db92-493b-8d9f-33364a373b98\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22019ead-db92-493b-8d9f-33364a373b98')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-22019ead-db92-493b-8d9f-33364a373b98 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 40,\n        \"max\": 71,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          50,\n          58,\n          61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 110,\n        \"max\": 170,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          145,\n          124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 172,\n        \"max\": 417,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          302,\n          282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 109,\n        \"max\": 179,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          137,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9768723406323231,\n        \"min\": 0.0,\n        \"max\": 3.4,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.6,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "# read test file\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning for Industrial Application/HW2/heart_dataset_test.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
        "outputId": "b4e72b77-8666-4943-b834-bdc5c44bea84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
        "outputId": "2ab139b3-d547-454c-e865-d5ba50ad8e77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "test_data = test_data.values\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "14d4be20-f64f-421d-8971-e1e47873aef8",
      "metadata": {
        "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
        "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
        "\n",
        "# Create datasets\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
        "outputId": "e25d427e-4175-4996-ed84-06a576b0ac2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning Rate  Hidden Units  Test Accuracy (%)  Test Loss\n",
            "0         0.0100            64              64.52     0.6236\n",
            "1         0.0100           128              74.19     0.5820\n",
            "2         0.0100           256              77.42     0.5607\n",
            "3         0.0010            64              67.74     0.6013\n",
            "4         0.0010           128              74.19     0.6320\n",
            "5         0.0010           256              77.42     0.6939\n",
            "6         0.0001            64              70.97     0.6312\n",
            "7         0.0001           128              67.74     0.6253\n",
            "8         0.0001           256              64.52     0.7258\n"
          ]
        }
      ],
      "source": [
        "test_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        model = Model(hidden_units).cuda()\n",
        "        model.load_state_dict(torch.load(f'model_LR{lr}_HU{hidden_units}.pth'))\n",
        "        model.eval()\n",
        "\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, labels in test_loader:\n",
        "                features = features.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                predicted = outputs.argmax(-1)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "                test_total += labels.size(0)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "        test_results.append({\n",
        "            'Learning Rate': lr,\n",
        "            'Hidden Units': hidden_units,\n",
        "            'Test Accuracy (%)': round(test_acc, 2),\n",
        "            'Test Loss': round(avg_test_loss, 4)\n",
        "        })\n",
        "\n",
        "import pandas as pd\n",
        "df_test_results = pd.DataFrame(test_results)\n",
        "print(df_test_results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
